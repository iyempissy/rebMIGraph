{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership Leakage Graph TSTS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tGWysPQ8aHfM",
        "outputId": "1b5a22c47-a54c-4946-ac5c-ee5c8faed549"
      },
      "source": [
        "# This is for the TSTS setting\n",
        "# Install required packages.\n",
        "!pip install -q torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git\n",
        "\n",
        "'''\n",
        "# This is using different subgraphs for both train and test. Thus TSTS\n",
        "'''\n",
        "\n",
        "import os.path as osp\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid, Reddit, Flickr\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, SGConv, GATConv\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import itertools\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import NeighborSampler\n",
        "\n",
        "from torch_geometric.utils import subgraph\n",
        "from torch_geometric.data import Data\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, roc_curve, roc_auc_score, f1_score\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(2)  # change this cos sometimes port 0 is full\n",
        "\n",
        "num_of_runs = 2#11  # this runs the program 10 times\n",
        "\n",
        "for which_run in range(1, num_of_runs):\n",
        "    random_data = os.urandom(4)\n",
        "\n",
        "    rand_state = int.from_bytes(random_data, byteorder=\"big\") #10, #3469326556, 959554842, 1521097589 #\n",
        "    print(\"rand_state\", rand_state)\n",
        "    torch.manual_seed(rand_state)\n",
        "    random.seed(rand_state)\n",
        "    np.random.seed(seed=rand_state)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # we run each data_type e.g cora against all model type\n",
        "\n",
        "    '''\n",
        "    Set parameters here ===================================================================================?????????\n",
        "    '''\n",
        "\n",
        "    model_type = \"GCN\"  # GCN, GAT, SAGE, SGC\n",
        "    data_type = \"Cora\"  # CiteSeer, Cora, PubMed, Flickr, Reddit\n",
        "\n",
        "    mode = \"TSTS\"  # train on subgraph, test on subgraph\n",
        "\n",
        "    save_shadow_OutTrain = \"posteriorsShadowOut_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_shadow_InTrain = \"posteriorsShadowTrain_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_target_OutTrain = \"posteriorsTargetOut_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_target_InTrain = \"posteriorsTargetTrain_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "\n",
        "    save_target_InTrain_nodes_neigbors = \"nodesNeigborsTargetTrain_\" + mode + \"_\" + data_type + \"_\" + model_type + \".npy\"\n",
        "    save_target_OutTrain_nodes_neigbors = \"nodesNeigborsTargetOut_\" + mode + \"_\" + data_type + \"_\" + model_type + \".npy\"\n",
        "\n",
        "    result_file = open(\"resultfile_\" + mode + \"_\" + model_type + \".txt\", \"a\")\n",
        "\n",
        "    '''\n",
        "    ######################################## Data ##############################################\n",
        "    '''\n",
        "    if data_type == \"Reddit\":\n",
        "        ###################################### Reddit ##################################\n",
        "\n",
        "        # path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Reddit')\n",
        "        path = \"./data/Reddit\"\n",
        "        dataset = Reddit(path)\n",
        "        # data = dataset[0]\n",
        "        # print(\"len(dataset)\", len(dataset)) # Reddit dataset consists of 1 graph\n",
        "        # print(\"data\", data) # Data(edge_index=[2, 114615892], test_mask=[232965], train_mask=[232965], val_mask=[232965], x=[232965, 602], y=[232965])\n",
        "        # print(\"Total Num of nodes in dataset\", data.num_nodes) # 232965\n",
        "        # print(\"Total Num of edges in dataset\", data.num_edges) # 114615892\n",
        "        # print(\"Total Num of node features in dataset\", data.num_node_features) # 602\n",
        "        # print(\"Total Num of features in dataset\", dataset.num_features) # same as node features # 602\n",
        "        # print(\"Num classes\", dataset.num_classes) #41\n",
        "\n",
        "        # Reduced this cos it's taking too long to create subgraph\n",
        "        num_train_Train_per_class = 500  # 1000\n",
        "        num_train_Shadow_per_class = 500  # 1000\n",
        "        num_test_Target = 20500  # 41000\n",
        "        num_test_Shadow = 20500  # 41000\n",
        "\n",
        "    elif data_type == \"Flickr\":\n",
        "\n",
        "        ###################################### Flickr ##################################\n",
        "\n",
        "        path = \"./data/Flickr\"\n",
        "        dataset = Flickr(path)\n",
        "        data = dataset[0]\n",
        "        # print(\"len(dataset)\", len(dataset))  # Flikr dataset consists of 1 graph\n",
        "        # print(\"data\",\n",
        "        #       data)  # Data(edge_index=[2, 899756], test_mask=[89250], train_mask=[89250], val_mask=[89250], x=[89250, 500], y=[89250])\n",
        "        # print(\"Total Num of nodes in dataset\", data.num_nodes)  # 89250\n",
        "        # print(\"Total Num of edges in dataset\", data.num_edges)  # 899756\n",
        "        # print(\"Total Num of node features in dataset\", data.num_node_features)  # 500\n",
        "        # print(\"Total Num of features in dataset\", dataset.num_features)  # same as node features # 500\n",
        "        # print(\"Num classes\", dataset.num_classes)  # 7\n",
        "\n",
        "        num_train_Train_per_class = 1500  # cos min of all classes only have 3k nodes\n",
        "        num_train_Shadow_per_class = 1500\n",
        "        num_test_Target = 10500\n",
        "        num_test_Shadow = 10500\n",
        "\n",
        "    elif data_type == \"Cora\":\n",
        "\n",
        "        ###################################### Cora ##################################\n",
        "\n",
        "        dataset = Planetoid(root='./data/Cora', name='Cora', split=\"random\")  # set test to 1320 to match train\n",
        "        num_train_Train_per_class = 90  # 180\n",
        "        num_train_Shadow_per_class = 90\n",
        "        num_test_Target = 630\n",
        "        num_test_Shadow = 630\n",
        "\n",
        "    elif data_type == \"CiteSeer\":\n",
        "\n",
        "        ###################################### CiteSeer ##################################\n",
        "\n",
        "        dataset = Planetoid(root='./data/CiteSeer', name='CiteSeer', split=\"random\")\n",
        "        num_train_Train_per_class = 100\n",
        "        num_train_Shadow_per_class = 100\n",
        "        num_test_Target = 600\n",
        "        num_test_Shadow = 600\n",
        "\n",
        "\n",
        "    elif data_type == \"PubMed\":\n",
        "\n",
        "        ###################################### Reddit ##################################\n",
        "\n",
        "        dataset = Planetoid(root='./data/PubMed', name='PubMed', split=\"random\")\n",
        "\n",
        "        num_train_Train_per_class = 1500\n",
        "        num_train_Shadow_per_class = 1500\n",
        "        num_test_Target = 4500\n",
        "        num_test_Shadow = 4500\n",
        "\n",
        "    else:\n",
        "        print(\"Error: No data specified\")\n",
        "\n",
        "    data = dataset[0]\n",
        "    len_y = data.y.size(0)\n",
        "    print(\"data\", data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    ############################################# Target and Shadow Models ###############################################\n",
        "    '''\n",
        "\n",
        "\n",
        "    class TargetModel(torch.nn.Module):\n",
        "        def __init__(self, dataset):\n",
        "            super(TargetModel, self).__init__()\n",
        "\n",
        "            if model_type == \"GCN\":\n",
        "                # GCN\n",
        "                self.conv1 = GCNConv(dataset.num_node_features, 256)\n",
        "                self.conv2 = GCNConv(256, dataset.num_classes)\n",
        "            elif model_type == \"SAGE\":\n",
        "                # GraphSage\n",
        "                # self.conv1 = SAGEConv(dataset.num_node_features, 256)\n",
        "                # self.conv2 = SAGEConv(256, dataset.num_classes)\n",
        "\n",
        "                self.num_layers = 2\n",
        "\n",
        "                self.convs = torch.nn.ModuleList()\n",
        "                self.convs.append(SAGEConv(dataset.num_node_features, 256))\n",
        "                self.convs.append(SAGEConv(256, dataset.num_classes))\n",
        "\n",
        "            elif model_type == \"SGC\":\n",
        "                # SGC\n",
        "                self.conv1 = SGConv(dataset.num_node_features, 256, K=2, cached=False)\n",
        "                self.conv2 = SGConv(256, dataset.num_classes, K=2, cached=False)\n",
        "\n",
        "            elif model_type == \"GAT\":\n",
        "                # GAT\n",
        "                self.conv1 = GATConv(dataset.num_features, 8, heads=8, dropout=0.1)\n",
        "                # On the Pubmed dataset, use heads=8 in conv2.\n",
        "                if data_type == \"PubMed\":\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False)\n",
        "                    # self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False, dropout=0.1)\n",
        "                else:\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=False)\n",
        "            else:\n",
        "                print(\"Error: No model selected\")\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "            # print(\"xxxxxxx\", x.size())\n",
        "\n",
        "            if model_type == \"SAGE\":\n",
        "                all_node_and_neigbors = []  # {} # lookup table dictionary #changed to list cos it;s not possible to slice esp for TSTS\n",
        "                all_nodes = []\n",
        "\n",
        "                # the edge_index here is quite different (it is a list cos we will be passing train_loader). edge index is different based on batch data.\n",
        "                # Note edge_index here is a bipartite graph. meaning all the edges retured are connected\n",
        "                for i, (edge_ind, _, size) in enumerate(edge_index):\n",
        "                    # print(\"iiiiiiiiiiiiiiii\", i)\n",
        "                    # print(\"edge_ind\", edge_ind)\n",
        "\n",
        "                    edges_raw = edge_ind.cpu().numpy()\n",
        "                    # print(\"edges_raw\", edges_raw)\n",
        "                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                    G = nx.Graph()\n",
        "                    G.add_nodes_from(list(range(\n",
        "                        num_test_Target)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class\n",
        "                    G.add_edges_from(edges)\n",
        "\n",
        "                    # print(\"x.size(0) forward\", x.size(0))\n",
        "                    # print(\"x.size forward\", x.size())\n",
        "                    # getting the neigbors of a particular node.\n",
        "                    for n in range(0, x.size(\n",
        "                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                        all_nodes.append(n)  # get all nodes\n",
        "                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                        # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "                    # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "                    x_target = x[:size[1]]  # Target nodes are always placed first.\n",
        "                    x = self.convs[i]((x, x_target), edge_ind)\n",
        "                    if i != self.num_layers - 1:\n",
        "                        x = F.relu(x)\n",
        "                        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "                # print(\"Final all nodes and neighbors\", all_node_and_neigbors)\n",
        "                return x.log_softmax(dim=-1), all_node_and_neigbors\n",
        "\n",
        "            else:\n",
        "                # torch.set_printoptions(threshold=10000)\n",
        "                # print(\"x0\", x[0])\n",
        "                # print(\"x1\", x[1])\n",
        "                # print(\"x\", x.shape)\n",
        "                # print(\"edge_index\", edge_index.shape)\n",
        "\n",
        "                # Begin edit data for passing node n it's neghbprs:\n",
        "                edges_raw = edge_index.cpu().numpy()\n",
        "                # print(\"edges_raw\", edges_raw)\n",
        "                edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                G = nx.Graph()\n",
        "                G.add_nodes_from(list(range(\n",
        "                    num_test_Target)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class\n",
        "                G.add_edges_from(edges)\n",
        "\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                # print(\"x.size(0) shadow forward\", x.size(0))\n",
        "                # getting the neigbors of a particular node.\n",
        "                for n in range(0, x.size(\n",
        "                        0)):  # x.size(0) caters for when u input the full graph for for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                    all_nodes.append(n)  # get all nodes\n",
        "                    # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                    all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                    # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "                # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "                x, edge_index = x, edge_index\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                # x = F.dropout(x, p=0.5, training=self.training)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "                x = self.conv2(x, edge_index)\n",
        "                # x = F.relu(x)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "                return F.log_softmax(x, dim=1), all_node_and_neigbors\n",
        "\n",
        "\n",
        "\n",
        "    class ShadowModel(torch.nn.Module):\n",
        "        def __init__(self, dataset):\n",
        "            super(ShadowModel, self).__init__()\n",
        "\n",
        "            if model_type == \"GCN\":\n",
        "                # GCN\n",
        "                self.conv1 = GCNConv(dataset.num_node_features, 256)\n",
        "                self.conv2 = GCNConv(256, dataset.num_classes)\n",
        "            elif model_type == \"SAGE\":\n",
        "                # GraphSage\n",
        "                # self.conv1 = SAGEConv(dataset.num_node_features, 256)\n",
        "                # self.conv2 = SAGEConv(256, dataset.num_classes)\n",
        "\n",
        "                self.num_layers = 2\n",
        "\n",
        "                self.convs = torch.nn.ModuleList()\n",
        "                self.convs.append(SAGEConv(dataset.num_node_features, 256))\n",
        "                self.convs.append(SAGEConv(256, dataset.num_classes))\n",
        "\n",
        "            elif model_type == \"SGC\":\n",
        "                # SGC\n",
        "                self.conv1 = SGConv(dataset.num_node_features, 256, K=2, cached=False)\n",
        "                self.conv2 = SGConv(256, dataset.num_classes, K=2, cached=False)\n",
        "\n",
        "            elif model_type == \"GAT\":\n",
        "                # GAT\n",
        "                self.conv1 = GATConv(dataset.num_features, 8, heads=8, dropout=0.1)\n",
        "                # On the Pubmed dataset, use heads=8 in conv2.\n",
        "                if data_type == \"PubMed\":\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False)\n",
        "                    # self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False, dropout=0.1)\n",
        "                else:\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=False)\n",
        "            else:\n",
        "                print(\"Error: No model selected\")\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "            # print(\"xxxxxxx\", x.size())\n",
        "            if model_type == \"SAGE\":\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                # the edge_index here is quite different (it is a list cos we will be passing train_loader). edge index is different based on batch data.\n",
        "                # Note edge_index here is a bipartite graph. meaning all the edges retured are connected\n",
        "                for i, (edge_ind, _, size) in enumerate(edge_index):\n",
        "                    # print(\"iiiiiiiiiiiiiiii\", i)\n",
        "                    edges_raw = edge_ind.cpu().numpy()\n",
        "                    # print(\"edges_raw\", edges_raw)\n",
        "                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                    G = nx.Graph()\n",
        "                    G.add_nodes_from(list(range(\n",
        "                        num_test_Target)))  # this is set to num_test_Shadow cos that's the max no of nodes since num_test_Shadow = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Shadow\n",
        "                    G.add_edges_from(edges)\n",
        "\n",
        "                    # print(\"x.size(0) shadow forward\", x.size(0))\n",
        "                    # print(\"x.size\", x.size())\n",
        "                    # getting the neigbors of a particular node\n",
        "                    for n in range(0, x.size(\n",
        "                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                        all_nodes.append(n)  # get all nodes\n",
        "                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                        # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "                    # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "                    x_shadow = x[:size[1]]  # Shadow nodes are always placed first.\n",
        "                    x = self.convs[i]((x, x_shadow), edge_ind)\n",
        "                    if i != self.num_layers - 1:\n",
        "                        x = F.relu(x)\n",
        "                        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "                # print(\"Final all nodes and neighbors\", all_node_and_neigbors)\n",
        "                return x.log_softmax(dim=-1), all_node_and_neigbors\n",
        "\n",
        "            else:\n",
        "                edges_raw = edge_index.cpu().numpy()\n",
        "                # print(\"edges_raw\", edges_raw)\n",
        "                edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                G = nx.Graph()\n",
        "                G.add_nodes_from(list(range(\n",
        "                    num_test_Target)))  # this is set to num_test_Shadow cos that's the max no of nodes since num_test_Shadow = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Shadow\n",
        "                G.add_edges_from(edges)\n",
        "\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                # print(\"x.size(0)\", x.size(0))\n",
        "                # getting the neigbors of a particular node\n",
        "                for n in range(0, x.size(\n",
        "                        0)):  # x.size(0) caters for when u input the full graph for for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                    all_nodes.append(n)  # get all nodes\n",
        "                    # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                    all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                    # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "                # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "\n",
        "\n",
        "                x, edge_index = x, edge_index\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                # x = F.dropout(x, p=0.5, training=self.training)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "                x = self.conv2(x, edge_index)\n",
        "                # x = F.relu(x)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "                return F.log_softmax(x, dim=1), all_node_and_neigbors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    ##################################### Data Processing Inductive Split ######################################\n",
        "    '''\n",
        "\n",
        "\n",
        "    def get_inductive_spilt(data, num_classes, num_train_Train_per_class, num_train_Shadow_per_class, num_test_Target,\n",
        "                            num_test_Shadow):\n",
        "        # -----------------------------------------------------------------------\n",
        "        # target_train, target_out\n",
        "        # shadow_train, shadow_out\n",
        "        '''\n",
        "        Randomly choose 'num_train_Train_per_class' and 'num_train_Shadow_per_class' per classes for training Target and shadow models respectively\n",
        "        Random choose 'num_test_Target' and 'num_test_Shadow' for testing (out data) Target and shadow models respectively\n",
        "\n",
        "        '''\n",
        "\n",
        "        # convert all label to list\n",
        "        label_idx = data.y.numpy().tolist()\n",
        "        print(\"label_idx\", len(label_idx))\n",
        "        target_train_idx = []\n",
        "        shadow_train_idx = []\n",
        "        # for i in range(num_classes):\n",
        "        #     c = [x for x in range(len(label_idx)) if label_idx[x] == i]\n",
        "        #     print(\"c\", len(c)) #the min is 180 which is 7th class\n",
        "        #     sample = random.sample(range(c),num_train_Train_per_class)\n",
        "        #     target_train_idx.extend(sample)\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            idx = (data.y == c).nonzero().view(-1)\n",
        "            sample_train_idx = idx[torch.randperm(idx.size(0))]\n",
        "            sample_target_train_idx = sample_train_idx[:num_train_Train_per_class]\n",
        "            target_train_idx.extend(sample_target_train_idx)\n",
        "\n",
        "            print(\"idx.size(0)\", idx.size(0))  # this is the total number of data in each class\n",
        "\n",
        "            # Ensure they don't over lap\n",
        "            # sample_shadow_train_idx = idx[torch.randperm(idx.size(0))[num_train_Train_per_class:num_train_Train_per_class + num_train_Shadow_per_class]]\n",
        "            sample_shadow_train_idx = sample_train_idx[\n",
        "                                      num_train_Train_per_class:num_train_Train_per_class + num_train_Shadow_per_class]\n",
        "            shadow_train_idx.extend(sample_shadow_train_idx)\n",
        "\n",
        "        print(\"shadow_train_idx\", len(shadow_train_idx))\n",
        "        print(\"Target_train_idx\", len(target_train_idx))\n",
        "\n",
        "        others = [x for x in range(len(label_idx)) if x not in set(target_train_idx) and x not in set(shadow_train_idx)]\n",
        "        # print(\"others\",others)\n",
        "        print(\"done others\")\n",
        "        target_test_idx = random.sample(others, num_test_Target)\n",
        "        print(\"done target test\")\n",
        "        shadow_test = [x for x in others if x not in set(target_test_idx)]\n",
        "        shadow_test_idx = random.sample(shadow_test, num_test_Shadow)\n",
        "        print(\"done shadow test\")\n",
        "\n",
        "        print(\"target_test_idx\", len(target_test_idx))\n",
        "        print(\"shadow_test_idx\", len(shadow_test_idx))\n",
        "\n",
        "        # ----------set values for mask--------------------------------\n",
        "        # Also changed this so as to conform with the shape of others. No, this uncommented one is better\n",
        "        # We don't need this cos we have already created a subgraph outta it\n",
        "\n",
        "        target_train_mask = torch.ones(len(target_train_idx), dtype=torch.uint8)\n",
        "        shadow_train_mask = torch.ones(len(shadow_train_idx), dtype=torch.uint8)\n",
        "\n",
        "        # target_train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "        # for i in target_train_idx:\n",
        "        #     target_train_mask[i] = 1\n",
        "\n",
        "        # ---test-mask---\n",
        "        target_test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "        for i in target_test_idx:\n",
        "            target_test_mask[i] = 1\n",
        "        # ---val-mask-----\n",
        "        shadow_test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "        for i in shadow_test_idx:\n",
        "            shadow_test_mask[i] = 1\n",
        "\n",
        "        '''\n",
        "        get all nodes and corresponding edge_index information\n",
        "        '''\n",
        "        # This is for creating subgraphs\n",
        "\n",
        "        # For target\n",
        "\n",
        "        # train\n",
        "        target_x_inductive = data.x[target_train_idx]\n",
        "        target_y_inductive = data.y[target_train_idx]\n",
        "        target_edge_index_inductive, _ = subgraph(target_train_idx, data.edge_index)\n",
        "\n",
        "        # test\n",
        "        target_x_test_inductive = data.x[target_test_idx]\n",
        "        target_y_test_inductive = data.y[target_test_idx]\n",
        "        target_test_edge_index_inductive, _ = subgraph(target_test_idx, data.edge_index)\n",
        "\n",
        "        # For shadow\n",
        "        shadow_x_inductive = data.x[shadow_train_idx]\n",
        "        shadow_y_inductive = data.y[shadow_train_idx]\n",
        "        shadow_edge_index_inductive, _ = subgraph(shadow_train_idx, data.edge_index)\n",
        "\n",
        "        # test\n",
        "        shadow_x_test_inductive = data.x[shadow_test_idx]\n",
        "        shadow_y_test_inductive = data.y[shadow_test_idx]\n",
        "        shadow_test_edge_index_inductive, _ = subgraph(shadow_test_idx, data.edge_index)\n",
        "\n",
        "        '''\n",
        "        in this part use a vertex_map to get a correct target_edge_index_inductive\n",
        "        '''\n",
        "        # ---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*\n",
        "        '''\n",
        "        get new edge_index information, because some nodes were removed from orginal nodes set. so there\n",
        "        are some edge_index that will disappear. If we dont do that, it will cause error: out of index 193\n",
        "        '''\n",
        "\n",
        "        target_vertex_map = {}\n",
        "        ind = -1\n",
        "        for i in range(data.num_nodes):\n",
        "            if i in target_train_idx:\n",
        "                ind += 1\n",
        "                target_vertex_map[i] = ind\n",
        "        for i in range(target_edge_index_inductive.shape[1]):\n",
        "            target_edge_index_inductive[0, i] = target_vertex_map[target_edge_index_inductive[0, i].tolist()]\n",
        "            target_edge_index_inductive[1, i] = target_vertex_map[target_edge_index_inductive[1, i].tolist()]\n",
        "\n",
        "        target_test_vertex_map = {}\n",
        "        ind = -1\n",
        "        for i in range(data.num_nodes):\n",
        "            if i in target_test_idx:\n",
        "                ind += 1\n",
        "                target_test_vertex_map[i] = ind\n",
        "        for i in range(target_test_edge_index_inductive.shape[1]):\n",
        "            target_test_edge_index_inductive[0, i] = target_test_vertex_map[\n",
        "                target_test_edge_index_inductive[0, i].tolist()]\n",
        "            target_test_edge_index_inductive[1, i] = target_test_vertex_map[\n",
        "                target_test_edge_index_inductive[1, i].tolist()]\n",
        "\n",
        "        shadow_vertex_map = {}\n",
        "        ind = -1\n",
        "        for i in range(data.num_nodes):\n",
        "            if i in shadow_train_idx:\n",
        "                ind += 1\n",
        "                shadow_vertex_map[i] = ind\n",
        "        for i in range(shadow_edge_index_inductive.shape[1]):\n",
        "            shadow_edge_index_inductive[0, i] = shadow_vertex_map[shadow_edge_index_inductive[0, i].tolist()]\n",
        "            shadow_edge_index_inductive[1, i] = shadow_vertex_map[shadow_edge_index_inductive[1, i].tolist()]\n",
        "\n",
        "        shadow_test_vertex_map = {}\n",
        "        ind = -1\n",
        "        for i in range(data.num_nodes):\n",
        "            if i in shadow_test_idx:\n",
        "                ind += 1\n",
        "                shadow_test_vertex_map[i] = ind\n",
        "        for i in range(shadow_test_edge_index_inductive.shape[1]):\n",
        "            shadow_test_edge_index_inductive[0, i] = shadow_test_vertex_map[\n",
        "                shadow_test_edge_index_inductive[0, i].tolist()]\n",
        "            shadow_test_edge_index_inductive[1, i] = shadow_test_vertex_map[\n",
        "                shadow_test_edge_index_inductive[1, i].tolist()]\n",
        "\n",
        "        # ---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*\n",
        "\n",
        "        # All graph data\n",
        "        all_x = data.x\n",
        "        all_y = data.y\n",
        "        all_edge_index = data.edge_index\n",
        "\n",
        "        '''\n",
        "        now we create a New data instances for save the all data with that we do inductive learning tasks\n",
        "        '''\n",
        "\n",
        "        data = Data(target_x=target_x_inductive, target_edge_index=target_edge_index_inductive,\n",
        "                    target_y=target_y_inductive,\n",
        "                    target_test_x=target_x_test_inductive, target_test_edge_index=target_test_edge_index_inductive,\n",
        "                    target_test_y=target_y_test_inductive,\n",
        "                    shadow_x=shadow_x_inductive, shadow_edge_index=shadow_edge_index_inductive,\n",
        "                    shadow_y=shadow_y_inductive,\n",
        "                    shadow_test_x=shadow_x_test_inductive, shadow_test_edge_index=shadow_test_edge_index_inductive,\n",
        "                    shadow_test_y=shadow_y_test_inductive,\n",
        "                    target_train_mask=target_train_mask, shadow_train_mask=shadow_train_mask, all_x=all_x,\n",
        "                    all_edge_index=all_edge_index, all_y=all_y, target_test_mask=target_test_mask,\n",
        "                    shadow_test_mask=shadow_test_mask)\n",
        "\n",
        "        # Original\n",
        "        # data = Data(target_x=target_x_inductive,target_edge_index=target_edge_index_inductive,target_y=target_y_inductive,\n",
        "        #                 shadow_x=shadow_x_inductive, shadow_edge_index=shadow_edge_index_inductive,shadow_y=shadow_y_inductive,\n",
        "        #                 target_train_mask=target_train_mask,shadow_train_mask=shadow_train_mask, all_x=all_x,\n",
        "        #                 all_edge_index=all_edge_index,all_y=all_y, target_test_mask=target_test_mask, shadow_test_mask=shadow_test_mask)\n",
        "\n",
        "        # # flipping the shadow to target and target to shadow\n",
        "        # data = Data(target_x=shadow_x_inductive,target_edge_index=shadow_edge_index_inductive,target_y=shadow_y_inductive,\n",
        "        #                 shadow_x=target_x_inductive, shadow_edge_index=target_edge_index_inductive,shadow_y=target_y_inductive,\n",
        "        #                 target_train_mask=shadow_train_mask,shadow_train_mask=target_train_mask,\n",
        "        #                 all_x=all_x,all_edge_index=all_edge_index,all_y=all_y,\n",
        "        #                 target_test_mask=shadow_test_mask, shadow_test_mask=target_test_mask)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def get_train_acc(data_new, pred, isTarget=True):\n",
        "        if isTarget:\n",
        "            # Removed train mask cos u r testing on the subgraph only tho\n",
        "            # train_acc = pred.eq(data_new.target_y[data_new.target_train_mask]).sum().item() / data_new.target_train_mask.sum().item()\n",
        "            train_acc = pred.eq(data_new.target_y).sum().item() / data_new.target_train_mask.sum().item()\n",
        "        else:\n",
        "            # train_acc = pred.eq(data_new.target_y[data_new.shadow_train_mask]).sum().item() / data_new.shadow_train_mask.sum().item()\n",
        "            train_acc = pred.eq(data_new.shadow_y).sum().item() / data_new.shadow_train_mask.sum().item()\n",
        "        # I changed this so as to get the prediction when u test on the train dataset cos all_y has all y. No this approach is not good. The apporach above is accurate\n",
        "        # train_acc = pred.eq(data_new.all_y[data_new.target_train_mask]).sum().item() / data_new.target_train_mask.sum().item()\n",
        "        return train_acc\n",
        "\n",
        "\n",
        "    def get_test_acc(data_new, pred, isTarget=True):\n",
        "        if isTarget:\n",
        "            # test_acc = pred.eq(data_new.all_y[data_new.target_test_mask]).sum().item() / data_new.target_test_mask.sum().item()\n",
        "            test_acc = pred.eq(data_new.target_test_y).sum().item() / data_new.target_test_mask.sum().item()\n",
        "        else:\n",
        "            # test_acc = pred.eq(\n",
        "            #     data_new.all_y[data_new.shadow_test_mask]).sum().item() / data_new.shadow_test_mask.sum().item()\n",
        "            test_acc = pred.eq(data_new.shadow_test_y).sum().item() / data_new.shadow_test_mask.sum().item()\n",
        "\n",
        "        return test_acc\n",
        "\n",
        "\n",
        "    def get_marco_f1(data_new, pred_labels, true_labels, label_list):\n",
        "        # f1_marco = f1_score(true_labels,pred_labels,label_list,average='macro')\n",
        "        f1_marco = f1_score(true_labels.cpu().numpy(), pred_labels, average='macro')\n",
        "        return f1_marco\n",
        "\n",
        "\n",
        "    def get_micro_f1(data_new, pred_labels, true_labels, label_list):\n",
        "        # f1_micro = f1_score(true_labels,pred_labels,label_list,average='micro')\n",
        "        f1_micro = f1_score(true_labels.cpu().numpy(), pred_labels, average='micro')\n",
        "        return f1_micro\n",
        "\n",
        "\n",
        "    '''\n",
        "    ########################## End Data Processing Inductive Split ###########################\n",
        "    '''\n",
        "\n",
        "    ''' Data initalization '''\n",
        "\n",
        "    # --- create labels_list---\n",
        "    label_list = [x for x in range(dataset.num_classes)]\n",
        "\n",
        "    # convert all label to list\n",
        "    label_idx = data.y.numpy().tolist()\n",
        "\n",
        "    data_new = get_inductive_spilt(data, dataset.num_classes, num_train_Train_per_class, num_train_Shadow_per_class,\n",
        "                                   num_test_Target, num_test_Shadow)\n",
        "\n",
        "    print(\"data new\", data_new)\n",
        "    print(\"data_new.shadow_test_mask.sum()\", data_new.shadow_test_mask.sum())\n",
        "    print(\"data_new.target_test_mask.sum()\", data_new.target_test_mask.sum())\n",
        "\n",
        "    print(dataset.num_classes)\n",
        "\n",
        "    bool_tensor = torch.ones(num_test_Target, dtype=torch.bool)\n",
        "    # print(\"bool_tensor\", bool_tensor) #using this increases attack precision to 0.604 instead of 0.590\n",
        "\n",
        "    target_train_loader = NeighborSampler(data_new.target_edge_index, node_idx=bool_tensor,\n",
        "                                   sizes=[25, 10], num_nodes = num_test_Target, batch_size=64, shuffle=False) #solution: node_idx to none since we wanna consider all nodes in the subgraph. Also num_test_Target is used cos its the total of nodes\n",
        "\n",
        "    target_test_loader = NeighborSampler(data_new.target_test_edge_index, node_idx=bool_tensor,\n",
        "                                   sizes=[25, 10], num_nodes = num_test_Target, batch_size=64, shuffle=False) #solution: node_idx to none since we wanna consider all nodes in the subgraph. Also num_test_Target is used cos its the total of nodes\n",
        "    print(\"data_new.target_edge_index\", data_new.target_edge_index.shape)\n",
        "    print(\"data_new.target_x\", data_new.target_x.shape)\n",
        "    # print(\"data_new.target_train_mask\", data_new.target_train_mask)\n",
        "\n",
        "    shadow_train_loader = NeighborSampler(data_new.shadow_edge_index, node_idx=bool_tensor,\n",
        "                                   sizes=[25, 10], num_nodes = num_test_Shadow, batch_size=64, shuffle=False)\n",
        "\n",
        "    shadow_test_loader = NeighborSampler(data_new.shadow_test_edge_index, node_idx=bool_tensor,\n",
        "                                   sizes=[25, 10], num_nodes = num_test_Shadow, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ''' Model initialization '''\n",
        "\n",
        "    target_model = TargetModel(dataset)\n",
        "    shadow_model = ShadowModel(dataset)  # Defining it as TargetModel(dataset) still produces the same result. I explicitly redefined each model for clarity\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    data_new = data_new.to(device)\n",
        "\n",
        "    target_model = target_model.to(device)\n",
        "\n",
        "    # # To reset the model to default\n",
        "    # for name, module in target_model.named_children():\n",
        "    #     print('resetting ', name)\n",
        "    #     module.reset_parameters()\n",
        "\n",
        "    shadow_model = shadow_model.to(device)\n",
        "\n",
        "    print(\"model\", target_model)\n",
        "    if model_type == \"SAGE\":\n",
        "        # better attack but slighly less test acc\n",
        "        if data_type == \"PubMed\":\n",
        "            target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "            shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.0001)  # 01\n",
        "        else:\n",
        "            target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "            shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.001) #01\n",
        "    else:\n",
        "        target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.0001)\n",
        "\n",
        "    '''\n",
        "    Train and Test function for model\n",
        "    '''\n",
        "\n",
        "\n",
        "    # --*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*\n",
        "    # #----------------------------- TRAIN FUCNTION---------------------------\n",
        "    def train(model, optimizer, isTarget=True):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if isTarget:\n",
        "            out, nodes_and_neighbors = model(data_new.target_x, data_new.target_edge_index)\n",
        "            loss = F.nll_loss(out, data_new.target_y)\n",
        "        else:\n",
        "            out, nodes_and_neighbors = model(data_new.shadow_x, data_new.shadow_edge_index)\n",
        "            loss = F.nll_loss(out, data_new.shadow_y)\n",
        "\n",
        "        pred = torch.exp(out)\n",
        "        # print(\"pred\", pred)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # approximate accuracy\n",
        "        if isTarget:\n",
        "            train_loss = loss.item() / int(data_new.target_train_mask.sum())\n",
        "            total_correct = int(pred.argmax(dim=-1).eq(data_new.target_y).sum()) / int(data_new.target_train_mask.sum())\n",
        "        else:\n",
        "            train_loss = loss.item() / int(data_new.shadow_train_mask.sum())\n",
        "            total_correct = int(pred.argmax(dim=-1).eq(data_new.shadow_y).sum()) / int(data_new.shadow_train_mask.sum())\n",
        "\n",
        "        # print(\"End train\")\n",
        "        return total_correct, train_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train_SAGE(model, optimizer, isTarget=True):\n",
        "        # print(\"================ Begin SAGE Train ==================\")\n",
        "        model.train()\n",
        "\n",
        "        if isTarget:\n",
        "            total_loss = total_correct = 0\n",
        "            for batch_size, n_id, adjs in target_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs\", adjs)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                out, nodes_and_neighbors = model(data_new.target_x[n_id], adjs)\n",
        "                # print(\"out traaaaain\", out.shape)\n",
        "                loss = F.nll_loss(out, data_new.target_y[n_id[:batch_size]])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += float(loss)\n",
        "                total_correct += int(out.argmax(dim=-1).eq(data_new.target_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            loss = total_loss / len(target_train_loader)\n",
        "            approx_acc = total_correct / int(data_new.target_train_mask.sum())\n",
        "\n",
        "        else:\n",
        "            # Shadow training\n",
        "            total_loss = total_correct = 0\n",
        "            for batch_size, n_id, adjs in shadow_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs\", adjs)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                out, nodes_and_neighbors = model(data_new.shadow_x[n_id], adjs)\n",
        "                # print(\"out traaaaain shadow\", out.shape)\n",
        "                loss = F.nll_loss(out, data_new.shadow_y[n_id[:batch_size]])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += float(loss)\n",
        "                total_correct += int(out.argmax(dim=-1).eq(data_new.shadow_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            loss = total_loss / len(shadow_train_loader)\n",
        "            approx_acc = total_correct / int(data_new.shadow_train_mask.sum())\n",
        "\n",
        "        # print(\"================ End SAGE Train ==================\")\n",
        "        return approx_acc, loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------------------------------------------------\n",
        "    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*\n",
        "    # -------------------------------- TEST FUNCTION -----------------------------------------\n",
        "    def test(model, isTarget=True):\n",
        "        model.eval()\n",
        "        # Also changed this to give true test using full graph. This will give the true train result--No it wont. See comment below\n",
        "\n",
        "        # This is a better n accurate approach\n",
        "        if isTarget:\n",
        "\n",
        "            '''# InTrain for target'''\n",
        "\n",
        "            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless\n",
        "            # pred_Intrain = model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask].max(1)[1]\n",
        "            pred, nodes_and_neighbors = model(data_new.target_x, data_new.target_edge_index)\n",
        "\n",
        "            pred_Intrain = pred.max(1)[1].to(device)\n",
        "            # Actual probabilities\n",
        "            # pred_Intrain_ps = torch.exp(model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask])\n",
        "            pred_Intrain_ps = torch.exp(pred)\n",
        "            np.savetxt(save_target_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "\n",
        "            np.save(save_target_InTrain_nodes_neigbors, nodes_and_neighbors)\n",
        "\n",
        "\n",
        "            # print(\"End InTrain for target\")\n",
        "\n",
        "            '''# OutTrain for target'''\n",
        "            # Now using another subgraph for testing rather than full graph\n",
        "            # pred_out = model(data_new.all_x, data_new.all_edge_index)[data_new.target_test_mask].max(1)[1]\n",
        "\n",
        "\n",
        "            preds, nodes_and_neighbors = model(data_new.target_test_x, data_new.target_test_edge_index)\n",
        "\n",
        "            pred_out = preds.max(1)[1].to(device)\n",
        "            # pred_out_ps = torch.exp(model(data_new.all_x, data_new.all_edge_index)[data_new.target_test_mask])\n",
        "            pred_out_ps = torch.exp(preds)\n",
        "\n",
        "            np.savetxt(save_target_OutTrain, pred_out_ps.cpu().detach().numpy())\n",
        "\n",
        "            # This will allow shuffling of posteriors in the attack model not to loose the node info\n",
        "            # print(\"newwwwwwww\", nodes_and_neighbors)\n",
        "\n",
        "\n",
        "            incremented_nodes_and_neighbors = [] #{}\n",
        "            # for i in range(len(nodes_and_neighbors)):\n",
        "            #     # print(nodes_and_neighbors[i])  # list\n",
        "            #     res = [x + num_test_Target for x in nodes_and_neighbors[i]]\n",
        "            #     incremented_nodes_and_neighbors[i+num_test_Target] = res\n",
        "\n",
        "            for i in range(len(nodes_and_neighbors)):\n",
        "                # print(nodes_and_neighbors[i])  # list\n",
        "                # print(nodes_and_neighbors[i][0])\n",
        "                # print(nodes_and_neighbors[i][1])\n",
        "\n",
        "                res = [x + num_test_Target for x in nodes_and_neighbors[i][1]]\n",
        "                res_0 = nodes_and_neighbors[i][0] + num_test_Target\n",
        "                incremented_nodes_and_neighbors.append((res_0, res))\n",
        "\n",
        "\n",
        "\n",
        "            # print(\"incremented_nodes_and_neighbors\", incremented_nodes_and_neighbors)\n",
        "\n",
        "\n",
        "            np.save(save_target_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)\n",
        "\n",
        "            # print(\"End OutTrain for target\")\n",
        "\n",
        "            pred_labels = pred_out.tolist()\n",
        "            # This uses the original target test\n",
        "            true_labels = data_new.target_test_y  # data_new.all_y[data_new.target_test_mask].tolist() # we can leave this for now or change it ti using the test_target_y\n",
        "\n",
        "            # The train accuracy is not on the full graph. It's similar to approx_train_acc\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out)\n",
        "\n",
        "        else:\n",
        "\n",
        "            '''# InTrain for Shadow'''\n",
        "            # pred_Intrain = model(data_new.shadow_x, data_new.shadow_edge_index)[data_new.shadow_train_mask].max(1)[1]\n",
        "            pred, nodes_and_neighbors = model(data_new.shadow_x, data_new.shadow_edge_index)\n",
        "\n",
        "            pred_Intrain = pred.max(1)[1].to(device)\n",
        "            # Actual probabilities\n",
        "            # pred_Intrain_ps = torch.exp(model(data_new.shadow_x, data_new.shadow_edge_index)[data_new.shadow_train_mask])\n",
        "            pred_Intrain_ps = torch.exp(pred)\n",
        "            np.savetxt(save_shadow_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "\n",
        "            '''# OutTrain for shadow'''\n",
        "            # Changing to use another subgraph for testing\n",
        "            # pred_out = model(data_new.all_x, data_new.all_edge_index)[data_new.shadow_test_mask].max(1)[1]\n",
        "            preds, nodes_and_neighbors = model(data_new.shadow_test_x, data_new.shadow_test_edge_index)\n",
        "\n",
        "            pred_out = preds.max(1)[1].to(device)\n",
        "            # pred_out_ps = torch.exp(model(data_new.all_x, data_new.all_edge_index)[data_new.shadow_test_mask])\n",
        "            pred_out_ps = torch.exp(preds)\n",
        "            np.savetxt(save_shadow_OutTrain, pred_out_ps.cpu().detach().numpy())\n",
        "\n",
        "            pred_labels = pred_out.tolist()\n",
        "            true_labels = data_new.shadow_test_y  # data_new.all_y[data_new.shadow_test_mask].tolist()\n",
        "\n",
        "            # The train accuracy is not on the full graph. It's similar to approx_train_acc\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain, False)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out, False)\n",
        "\n",
        "        # pred_Intrain = model(data_new.all_x,data_new.all_edge_index)[data_new.target_train_mask].max(1)[1]\n",
        "        # # Actual probabilities\n",
        "        # pred_Intrain_ps = torch.exp(model(data_new.all_x,data_new.all_edge_index)[data_new.target_train_mask])\n",
        "\n",
        "        # print(\"posteriors\", pred_Intrain)\n",
        "\n",
        "        # The f1 measures are on test dataset\n",
        "        f1_marco = get_marco_f1(data_new, pred_labels, true_labels, label_list)\n",
        "        f1_micro = get_micro_f1(data_new, pred_labels, true_labels, label_list)\n",
        "\n",
        "        return train_acc, test_acc, f1_marco, f1_micro\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def test_SAGE(model, isTarget=True):\n",
        "        # print(\"****************** SAGE test begin *************************\")\n",
        "        model.eval()\n",
        "        # Also changed this to give true test using full graph. This will give the true train result--No it wont. See comment below\n",
        "\n",
        "        # This is a better n accurate approach\n",
        "        if isTarget:\n",
        "\n",
        "            '''InTrain Target'''\n",
        "            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless\n",
        "            # pred_Intrain = model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask].max(1)[1]\n",
        "\n",
        "\n",
        "            pred = []\n",
        "            nodes_and_neighbors = []\n",
        "            total_target_train_correct = 0\n",
        "\n",
        "            for batch_size, n_id, adjs in target_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs test\", adjs)\n",
        "                # print(\"n_id\",len(n_id))\n",
        "\n",
        "                out, node_and_neigh = model(data_new.target_x[n_id], adjs)\n",
        "                out = torch.exp(out)\n",
        "                # print(\"out test\", out.shape)\n",
        "                pred.append(out.cpu())\n",
        "                nodes_and_neighbors.append(node_and_neigh)\n",
        "\n",
        "                total_target_train_correct += int(out.argmax(dim=-1).eq(data_new.target_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            target_train_acc = total_target_train_correct / int(data_new.target_train_mask.sum())\n",
        "            # print(\"target_train_acc\", target_train_acc)\n",
        "\n",
        "            # Need to concat all preds cos it's per batch\n",
        "            pred_all_inTrain = torch.cat(pred, dim=0)\n",
        "\n",
        "            # pred, nodes_and_neighbors = model(data_new.target_x,data_new.target_edge_index)\n",
        "            pred_Intrain = pred_all_inTrain.max(1)[1].to(device) #ensures it is on the same device for working on GPU\n",
        "            # # Actual probabilities\n",
        "            # # pred_Intrain_ps = torch.exp(model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask])\n",
        "            pred_Intrain_ps = pred_all_inTrain  # torch.exp(pred)\n",
        "            np.savetxt(save_target_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "            #\n",
        "            np.save(save_target_InTrain_nodes_neigbors, nodes_and_neighbors)\n",
        "            # print(\"nodes_and_neighborsnodes_and_neighbors\", nodes_and_neighbors) # 630\n",
        "\n",
        "            # print(\"End InTrain for target\")\n",
        "\n",
        "            '''OutTrain Target'''\n",
        "\n",
        "            preds = []\n",
        "            nodes_and_neighbors = []\n",
        "            total_target_out_correct = 0\n",
        "            for batch_size, n_id, adjs in target_test_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs test\", adjs)\n",
        "                # print(\"n_id\",len(n_id))\n",
        "\n",
        "                out, node_and_neigh = model(data_new.target_test_x[n_id], adjs)\n",
        "                out = torch.exp(out)\n",
        "                # print(\"out test\", out.shape)\n",
        "                preds.append(out.cpu())\n",
        "                nodes_and_neighbors.append(node_and_neigh)\n",
        "\n",
        "                total_target_out_correct += int(out.argmax(dim=-1).eq(data_new.target_test_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            target_out_acc = total_target_out_correct / int(data_new.target_test_mask.sum()) # same as get_train_acc()\n",
        "            # print(\"target_out_acc\", target_out_acc)\n",
        "\n",
        "            # Need to concat all preds cos it's per batch\n",
        "            pred_all_outTrain = torch.cat(preds, dim=0)\n",
        "\n",
        "            nodes_and_neighbors = list(itertools.chain.from_iterable(nodes_and_neighbors)) #turn list of list into one giant one them\n",
        "            nodes_and_neighbors = nodes_and_neighbors[\n",
        "                                  :len_y]\n",
        "\n",
        "            # print(\"nodes_and_neighbors\", nodes_and_neighbors)\n",
        "\n",
        "            pred_out = pred_all_outTrain.max(1)[1].to(device)\n",
        "            pred_out_ps = pred_all_outTrain #torch.exp(preds)\n",
        "            # print(\"pred_all_outTrain\", pred_all_outTrain)\n",
        "\n",
        "\n",
        "            np.savetxt(save_target_OutTrain, pred_out_ps.cpu().detach().numpy())\n",
        "            incremented_nodes_and_neighbors = []  # {}\n",
        "            # for i in range(len(nodes_and_neighbors)):\n",
        "            #     # print(nodes_and_neighbors[i])  # list\n",
        "            #     res = [x + num_test_Target for x in nodes_and_neighbors[i]]\n",
        "            #     incremented_nodes_and_neighbors[i+num_test_Target] = res\n",
        "\n",
        "            for i in range(len(nodes_and_neighbors)):\n",
        "                # print(nodes_and_neighbors[i])  # list\n",
        "                # print(nodes_and_neighbors[i][0])\n",
        "                # print(nodes_and_neighbors[i][1])\n",
        "\n",
        "                res = [x + num_test_Target for x in nodes_and_neighbors[i][1]]\n",
        "                res_0 = nodes_and_neighbors[i][0] + num_test_Target\n",
        "                incremented_nodes_and_neighbors.append((res_0, res))\n",
        "\n",
        "            np.save(save_target_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)\n",
        "\n",
        "            # print(\"End OutTrain for target\")\n",
        "\n",
        "            pred_labels = pred_out.tolist()\n",
        "            # This uses the original target test\n",
        "            true_labels = data_new.target_test_y  # data_new.all_y[data_new.target_test_mask].tolist() # we can leave this for now or change it ti using the test_target_y\n",
        "\n",
        "            # The train accuracy is not on the full graph. It's similar to approx_train_acc. Changed cos of the train_loader!\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out)\n",
        "\n",
        "        else:\n",
        "\n",
        "            '''InTrain Shadow'''\n",
        "            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless\n",
        "            # pred_Intrain = model(data_new.shadow_x,data_new.shadow_edge_index)[data_new.shadow_train_mask].max(1)[1]\n",
        "\n",
        "            pred = []\n",
        "            nodes_and_neighbors = []\n",
        "            total_shadow_train_correct = 0\n",
        "            for batch_size, n_id, adjs in shadow_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs test\", adjs)\n",
        "                # print(\"n_id\",len(n_id))\n",
        "\n",
        "                out, node_and_neigh = model(data_new.shadow_x[n_id], adjs)\n",
        "                out = torch.exp(out)\n",
        "                # print(\"out test\", out.shape)\n",
        "                pred.append(out.cpu())\n",
        "                nodes_and_neighbors.append(node_and_neigh)\n",
        "\n",
        "                total_shadow_train_correct += int(out.argmax(dim=-1).eq(data_new.shadow_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            shadow_train_acc = total_shadow_train_correct / int(data_new.shadow_train_mask.sum())\n",
        "            # print(\"shadow_train_acc\", shadow_train_acc)\n",
        "\n",
        "            # Need to concat all preds cos it's per batch\n",
        "            pred_all_inTrain = torch.cat(pred, dim=0)\n",
        "\n",
        "            # pred, nodes_and_neighbors = model(data_new.shadow_x,data_new.shadow_edge_index)\n",
        "            pred_Intrain = pred_all_inTrain.max(1)[1].to(device)\n",
        "            # # Actual probabilities\n",
        "            # # pred_Intrain_ps = torch.exp(model(data_new.shadow_x,data_new.shadow_edge_index)[data_new.shadow_train_mask])\n",
        "            pred_Intrain_ps = pred_all_inTrain  # torch.exp(pred)\n",
        "            np.savetxt(save_shadow_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "            #\n",
        "            # np.save(save_shadow_InTrain_nodes_neigbors, nodes_and_neighbors)  # Not necsaary\n",
        "            # print(\"nodes_and_neighborsnodes_and_neighbors\", nodes_and_neighbors) # 630\n",
        "\n",
        "            # print(\"End InTrain for shadow\")\n",
        "\n",
        "            '''OutTrain Shadow'''\n",
        "\n",
        "            preds = []\n",
        "            nodes_and_neighbors = []\n",
        "\n",
        "            total_shadow_out_correct = 0\n",
        "            for batch_size, n_id, adjs in shadow_test_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs test\", adjs)\n",
        "                # print(\"n_id\",len(n_id))\n",
        "\n",
        "                out, node_and_neigh = model(data_new.shadow_test_x[n_id], adjs)\n",
        "                out = torch.exp(out)\n",
        "                # print(\"out test\", out.shape)\n",
        "                preds.append(out.cpu())\n",
        "                nodes_and_neighbors.append(node_and_neigh)\n",
        "\n",
        "                total_shadow_out_correct += int(out.argmax(dim=-1).eq(data_new.shadow_test_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            shadow_out_acc = total_shadow_out_correct / int(data_new.shadow_test_mask.sum())\n",
        "            # print(\"shadow_out_acc\", shadow_out_acc)\n",
        "\n",
        "            # Need to concat all preds cos it's per batch\n",
        "            pred_all_outTrain = torch.cat(preds, dim=0)\n",
        "\n",
        "            nodes_and_neighbors = list(itertools.chain.from_iterable(nodes_and_neighbors)) #turn list of list into one giant one them\n",
        "\n",
        "\n",
        "            nodes_and_neighbors = nodes_and_neighbors[\n",
        "                                  :len_y]\n",
        "\n",
        "            # print(\"nodes_and_neighbors\", nodes_and_neighbors)\n",
        "\n",
        "            pred_out = pred_all_outTrain.max(1)[1].to(device)\n",
        "            pred_out_ps = pred_all_outTrain #torch.exp(preds)\n",
        "            # print(\"pred_all_outTrain\", pred_all_outTrain)\n",
        "\n",
        "\n",
        "            np.savetxt(save_shadow_OutTrain, pred_out_ps.cpu().detach().numpy())\n",
        "\n",
        "            incremented_nodes_and_neighbors = []  # {}\n",
        "            # for i in range(len(nodes_and_neighbors)):\n",
        "            #     # print(nodes_and_neighbors[i])  # list\n",
        "            #     res = [x + num_test_Shadow for x in nodes_and_neighbors[i]]\n",
        "            #     incremented_nodes_and_neighbors[i+num_test_Shadow] = res\n",
        "\n",
        "            for i in range(len(nodes_and_neighbors)):\n",
        "                # print(nodes_and_neighbors[i])  # list\n",
        "                # print(nodes_and_neighbors[i][0])\n",
        "                # print(nodes_and_neighbors[i][1])\n",
        "\n",
        "                res = [x + num_test_Shadow for x in nodes_and_neighbors[i][1]]\n",
        "                res_0 = nodes_and_neighbors[i][0] + num_test_Shadow\n",
        "                incremented_nodes_and_neighbors.append((res_0, res))\n",
        "\n",
        "            # print(\"incremented_nodes_and_neighbors\", incremented_nodes_and_neighbors)\n",
        "\n",
        "            # np.save(save_shadow_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)  # Not needed\n",
        "\n",
        "            # print(\"End OutTrain for shadow\")\n",
        "\n",
        "            pred_labels = pred_out.tolist()\n",
        "            # This uses the original shadow test\n",
        "            true_labels = data_new.shadow_test_y  # data_new.all_y[data_new.shadow_test_mask].tolist() # we can leave this for now or change it ti using the test_shadow_y\n",
        "\n",
        "            # The train accuracy is not on the full graph. It's similar to approx_train_acc\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain, False)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out, False)\n",
        "\n",
        "        # print(\"posteriors\", pred_Intrain)\n",
        "\n",
        "        # The f1 measures are on test dataset\n",
        "        f1_marco = get_marco_f1(data_new, pred_labels, true_labels, label_list)\n",
        "        f1_micro = get_micro_f1(data_new, pred_labels, true_labels, label_list)\n",
        "\n",
        "        # print(\"****************** SAGE test End *************************\")\n",
        "        return train_acc, test_acc, f1_marco, f1_micro\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*\n",
        "    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*\n",
        "    # --------------------------TRAIN PROCESS-----------------------------------------------\n",
        "    best_val_acc = best_val_acc = 0\n",
        "\n",
        "    '''\n",
        "    Training and testing target and shadow models\n",
        "    '''\n",
        "\n",
        "    if model_type == \"SAGE\":\n",
        "        if data_type == \"CiteSeer\" or data_type == \"Cora\":\n",
        "            model_training_epoch = 16 #301 #16 for CiteSeer n Cora, 101 for PubMed, 301 for Flickr n Reddit\n",
        "        elif data_type == \"PubMed\":\n",
        "            model_training_epoch = 101\n",
        "        else:\n",
        "            model_training_epoch = 301\n",
        "    else:\n",
        "        model_training_epoch = 301#301\n",
        "\n",
        "    # Target train\n",
        "    for epoch in range(1, model_training_epoch):\n",
        "        if model_type == \"SAGE\":\n",
        "            approx_train_acc, train_loss = train_SAGE(target_model, target_optimizer)\n",
        "            train_acc, test_acc, marco, micro = test_SAGE(target_model)\n",
        "        else:\n",
        "            approx_train_acc, train_loss = train(target_model, target_optimizer)\n",
        "            train_acc, test_acc, marco, micro = test(target_model)\n",
        "        # if val_acc > best_val_acc:\n",
        "        #     best_val_acc = val_acc\n",
        "        #     test_acc = tmp_test_acc\n",
        "        #     marco = tmp_marco\n",
        "        #     micro = tmp_micro\n",
        "\n",
        "        # log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        # print(log.format(epoch, train_acc, best_val_acc, test_acc,marco,micro))\n",
        "        log = 'TargetModel Epoch: {:03d}, Approx Train: {:.4f}, Train: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        print(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro))\n",
        "        if epoch == model_training_epoch - 1:\n",
        "            result_file.write(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro) + \"\\n\")\n",
        "\n",
        "    print()\n",
        "    print(\"=========================================================End Target Train ==============================\")\n",
        "\n",
        "    # shadow train\n",
        "    for epoch in range(1, model_training_epoch):\n",
        "        if model_type == \"SAGE\":\n",
        "            approx_train_acc, train_loss = train_SAGE(shadow_model, shadow_optimizer, False)\n",
        "            train_acc, test_acc, marco, micro = test_SAGE(shadow_model, False)\n",
        "        else:\n",
        "            approx_train_acc, train_loss = train(shadow_model, shadow_optimizer, False)\n",
        "            train_acc, test_acc, marco, micro = test(shadow_model, False)\n",
        "        # if val_acc > best_val_acc:\n",
        "        #     best_val_acc = val_acc\n",
        "        #     test_acc = tmp_test_acc\n",
        "        #     marco = tmp_marco\n",
        "        #     micro = tmp_micro\n",
        "\n",
        "        # log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        # print(log.format(epoch, train_acc, best_val_acc, test_acc,marco,micro))\n",
        "        log = 'ShadowModel Epoch: {:03d}, Approx Train: {:.4f}, Train: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        print(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro))\n",
        "\n",
        "        if epoch == model_training_epoch - 1:\n",
        "            result_file.write(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro) + \"\\n\")\n",
        "\n",
        "    # pred, nodes_and_neighbors = target_model(data_new.target_x, data_new.target_edge_index)\n",
        "    # print(\"single Pred\", torch.exp(pred[0])) # this is correct\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ''' =========================================== ATTACK ============================================== '''\n",
        "\n",
        "    # Use normal python to import posterior files.\n",
        "    # Split posterior data into 80 / 20 (train, test)\n",
        "\n",
        "    # add 2 dataset 2 gether pandas\n",
        "\n",
        "    positive_attack_data = pd.read_csv(save_shadow_InTrain, header=None,\n",
        "                                       sep=\" \")  # dataframe #\"posteriorsShadowTrain.txt\"\n",
        "\n",
        "    # target in and out data\n",
        "\n",
        "    target_data_for_testing_Intrain = pd.read_csv(save_target_InTrain, header=None,\n",
        "                                                  sep=\" \")  # dataframe \"posteriorsTargetTrain.txt\"\n",
        "    # Assign 1 to indata\n",
        "    target_data_for_testing_Intrain[\"labels\"] = 1\n",
        "\n",
        "    target_data_for_testing_Intrain['nodeID'] = range(0, num_test_Target)\n",
        "\n",
        "    # print(\"target_data_for_testing_Intrain.head(10)\", target_data_for_testing_Intrain.head(10))\n",
        "    # print(\"target_data_for_testing_Intrain.tail(10)\", target_data_for_testing_Intrain.tail(10))\n",
        "\n",
        "    # # randomly select 500\n",
        "    # chosen_idx = np.random.choice(153431, replace=False, size=50000)\n",
        "    # print(chosen_idx)\n",
        "    # positive_attack_data = positive_attack_data.iloc[chosen_idx]\n",
        "\n",
        "    target_data_for_testing_Outtrain = pd.read_csv(save_target_OutTrain, header=None,\n",
        "                                                   sep=\" \")  # dataframe \"posteriorsTargetOut.txt\"\n",
        "    # Assign 0 to outdata\n",
        "    target_data_for_testing_Outtrain[\"labels\"] = 0\n",
        "\n",
        "    target_data_for_testing_Outtrain['nodeID'] = range(num_test_Target, num_test_Target+num_test_Target)\n",
        "\n",
        "    # print(positive_attack_data.head())\n",
        "\n",
        "    # Assign 1 to training data\n",
        "    positive_attack_data[\"labels\"] = 1\n",
        "\n",
        "    print(\"positive_attack_data.shape\", positive_attack_data.shape)\n",
        "\n",
        "    negative_attack_data = pd.read_csv(save_shadow_OutTrain, header=None, sep=\" \")  # \"posteriorsShadowOut.txt\"\n",
        "\n",
        "    # # randomly select 140\n",
        "    # chosen_idx = np.random.choice(55703, replace=False, size=50000)\n",
        "    # print(chosen_idx)\n",
        "    # negative_attack_data = negative_attack_data.iloc[chosen_idx]\n",
        "\n",
        "    # print(negative_attack_data.head())\n",
        "\n",
        "    # Assign 0 to out data\n",
        "    negative_attack_data[\"labels\"] = 0\n",
        "    print(\"negative_attack_data.shape\", negative_attack_data.shape)\n",
        "\n",
        "    # Combine to single dataframe\n",
        "\n",
        "    # combine them together\n",
        "    attack_data_combo = [positive_attack_data, negative_attack_data]\n",
        "    attack_data = pd.concat(attack_data_combo)\n",
        "\n",
        "    target_data_for_testing_InAndOutTrain_combo = [target_data_for_testing_Intrain, target_data_for_testing_Outtrain]\n",
        "    target_data_for_testing_InAndOutTrain = pd.concat(target_data_for_testing_InAndOutTrain_combo)\n",
        "\n",
        "    print(\"attack_data.shape\", attack_data.shape)\n",
        "    # print(attack_data.head())\n",
        "\n",
        "    # # sample randomly\n",
        "    # # returns all but in a random fashion\n",
        "    # attack_data = attack_data.sample(frac=1)\n",
        "    # print(attack_data.head())\n",
        "\n",
        "    X_attack = attack_data.drop(\"labels\", axis=1)\n",
        "    print(\"X_attack.shape\", X_attack.shape)\n",
        "\n",
        "    y_attack = attack_data[\"labels\"]\n",
        "\n",
        "    # let's do in and out for attack data (shadow)\n",
        "    X_attack_InTrain = positive_attack_data.drop(\"labels\", axis=1)\n",
        "    y_attack_InTrain = positive_attack_data[\"labels\"]\n",
        "\n",
        "    X_attack_OutTrain = negative_attack_data.drop(\"labels\", axis=1)\n",
        "    y_attack_OutTrain = negative_attack_data[\"labels\"]\n",
        "\n",
        "    print(\"X_attack_InTrain\", X_attack_InTrain.shape)\n",
        "    print(\"X_attack_OutTrain\", X_attack_OutTrain.shape)\n",
        "\n",
        "    # For in train data (target)\n",
        "    X_InTrain = target_data_for_testing_Intrain.drop([\"labels\", \"nodeID\"], axis=1)\n",
        "    y_InTrain = target_data_for_testing_Intrain[\"labels\"]\n",
        "    nodeID_InTrain = target_data_for_testing_Intrain[\"nodeID\"]\n",
        "\n",
        "\n",
        "    # For Out train data\n",
        "    X_OutTrain = target_data_for_testing_Outtrain.drop([\"labels\", \"nodeID\"], axis=1)\n",
        "    y_OutTrain = target_data_for_testing_Outtrain[\"labels\"]\n",
        "    nodeID_OutTrain = target_data_for_testing_Outtrain[\"nodeID\"]\n",
        "\n",
        "    # For in out data\n",
        "    X_InOutTrain = target_data_for_testing_InAndOutTrain.drop([\"labels\", \"nodeID\"], axis=1)\n",
        "    print(\"X_InTrain.shape\", X_InOutTrain.shape)\n",
        "\n",
        "    y_InOutTrain = target_data_for_testing_InAndOutTrain[\"labels\"]\n",
        "    nodeID_InOutTrain = target_data_for_testing_InAndOutTrain[\"nodeID\"]\n",
        "\n",
        "    # convert to numpy\n",
        "    # for shadow\n",
        "    X_attack_InOut, y_attack_InOut = X_attack.to_numpy(), y_attack.to_numpy()\n",
        "\n",
        "    X_attack_InTrain, X_attack_OutTrain = X_attack_InTrain.to_numpy(), X_attack_OutTrain.to_numpy()\n",
        "    y_attack_InTrain, y_attack_OutTrain = y_attack_InTrain.to_numpy(), y_attack_OutTrain.to_numpy()\n",
        "\n",
        "    # for target\n",
        "    X_InTrain, y_InTrain, nodeID_InTrain = X_InTrain.to_numpy(), y_InTrain.to_numpy(), nodeID_InTrain.to_numpy()\n",
        "    X_OutTrain, y_OutTrain, nodeID_OutTrain  = X_OutTrain.to_numpy(), y_OutTrain.to_numpy(), nodeID_OutTrain.to_numpy()\n",
        "\n",
        "    # for target\n",
        "    X_InOutTrain, y_InOutTrain, nodeID_InOutTrain = X_InOutTrain.to_numpy(), y_InOutTrain.to_numpy(), nodeID_InOutTrain.to_numpy()\n",
        "\n",
        "    # # Plot graphs\n",
        "    #\n",
        "    # plt.imshow(X_attack_InTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Positive: In Train Posteriors')\n",
        "    # plt.show()\n",
        "    #\n",
        "    # plt.imshow(X_attack_OutTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Negative: Out Train Posteriors')\n",
        "    # plt.show()\n",
        "    #\n",
        "    #\n",
        "    # plt.imshow(X_InTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Positive: Target Posteriors')\n",
        "    # plt.show()\n",
        "    #\n",
        "    # plt.imshow(X_OutTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Negative: Target Posteriors')\n",
        "    # plt.show()\n",
        "\n",
        "    attack_train_data_X, attack_test_data_X, attack_train_data_y, attack_test_data_y = train_test_split(X_attack,\n",
        "                                                                                                        y_attack,\n",
        "                                                                                                        test_size=50,\n",
        "                                                                                                        stratify=y_attack,\n",
        "                                                                                                        random_state=rand_state)\n",
        "    # print(\"baba\")\n",
        "\n",
        "    # convert series data to numpy array\n",
        "    attack_train_data_X, attack_test_data_X, attack_train_data_y, attack_test_data_y = attack_train_data_X.to_numpy(), attack_test_data_X.to_numpy(), attack_train_data_y.to_numpy(), attack_test_data_y.to_numpy()\n",
        "\n",
        "    # print(\"Attack data printing...\")\n",
        "    # print(attack_test_data_X, attack_test_data_y)\n",
        "\n",
        "    # Attack_train\n",
        "    attack_train_data = torch.utils.data.TensorDataset(torch.from_numpy(attack_train_data_X).float(), torch.from_numpy(\n",
        "        attack_train_data_y))  # convert to float to fix  uint8_t overflow error\n",
        "    attack_train_data_loader = torch.utils.data.DataLoader(attack_train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Attack_test = combo of targettrain and targetOut\n",
        "    attack_test_data = torch.utils.data.TensorDataset(torch.from_numpy(attack_test_data_X).float(), torch.from_numpy(\n",
        "        attack_test_data_y))  # convert to float to fix  uint8_t overflow error\n",
        "    attack_test_data_loader = torch.utils.data.DataLoader(attack_test_data, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "    # Training InData\n",
        "    target_data_for_testing_InTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_InTrain).float(),\n",
        "                                                                          torch.from_numpy(y_InTrain), torch.from_numpy(nodeID_InTrain))\n",
        "    target_data_for_testing_InTrain_data_loader = torch.utils.data.DataLoader(target_data_for_testing_InTrain_data,\n",
        "                                                                              batch_size=64, shuffle=False)\n",
        "\n",
        "    # Training OutData\n",
        "    target_data_for_testing_OutTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_OutTrain).float(),\n",
        "                                                                           torch.from_numpy(y_OutTrain), torch.from_numpy(nodeID_OutTrain))\n",
        "    target_data_for_testing_OutTrain_data_loader = torch.utils.data.DataLoader(target_data_for_testing_OutTrain_data,\n",
        "                                                                               batch_size=64, shuffle=False)\n",
        "\n",
        "    # Training InOut Data\n",
        "    target_data_for_testing_InOutTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_InOutTrain).float(),\n",
        "                                                                             torch.from_numpy(y_InOutTrain), torch.from_numpy(nodeID_InOutTrain))\n",
        "    target_data_for_testing_InOutTrain_data_loader = torch.utils.data.DataLoader(\n",
        "        target_data_for_testing_InOutTrain_data,\n",
        "        batch_size=64, shuffle=True)\n",
        "\n",
        "    # features, labels = next(iter(attack_test_data_loader))\n",
        "    # print(features, labels)\n",
        "\n",
        "    class AttackModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # inputs to hidden layer linear transformation\n",
        "            # Note, when using Linear, weight and biases are randomly initialized for you\n",
        "            self.hidden = nn.Linear(dataset.num_classes, 100)\n",
        "            self.hidden2 = nn.Linear(100, 50)\n",
        "            # output layer, 10 units - one for each digits\n",
        "            self.output = nn.Linear(50, 2)\n",
        "\n",
        "            # # Define sigmoid activation and softmax output\n",
        "            # # comment this cos u can just define directly if u are using functional\n",
        "            # self.sigmoid = nn.Sigmoid()\n",
        "            # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # # pass the input tensor through each of our operations\n",
        "            # # comment to use functional\n",
        "            # x = self.hidden(x)\n",
        "            # x = self.sigmoid(x)\n",
        "            # x = self.output(x)\n",
        "            # x = self.softmax(x)\n",
        "\n",
        "            # Hidden layer with sigmoid activation\n",
        "            x = F.sigmoid(self.hidden(x))\n",
        "            x = F.sigmoid(self.hidden2(x))\n",
        "            # output layer with softmax activation\n",
        "            x = F.softmax(self.output(x), dim=1)\n",
        "            # print(\"xxxxxxxx\", x)\n",
        "\n",
        "            return x\n",
        "\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        # define nn\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            self.fc1 = nn.Linear(dataset.num_classes, 100)\n",
        "            self.fc2 = nn.Linear(100, 50)\n",
        "            self.fc3 = nn.Linear(50, 2)\n",
        "            self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        def forward(self, X):\n",
        "            # print(\"attack X\",X)\n",
        "            X = F.relu(self.fc1(X))\n",
        "            X = F.relu(self.fc2(X))\n",
        "            X = self.fc3(X)\n",
        "            X = self.softmax(X)\n",
        "\n",
        "            return X\n",
        "\n",
        "\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "    # create the ntwk\n",
        "    attack_model = Net()  # AttackModel()\n",
        "    attack_model = attack_model.to(device)\n",
        "    attack_model.apply(init_weights)  # initialize weight rather than randomly\n",
        "    print(attack_model)\n",
        "\n",
        "\n",
        "    def attack_train(model, trainloader, testloader, criterion, optimizer, epochs, steps=0):\n",
        "        # train ntwk\n",
        "\n",
        "        # Decay LR by a factor of 0.1 every 7 epochs\n",
        "        # scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "        final_train_loss = 0\n",
        "        train_losses, test_losses = [], []\n",
        "        posteriors = []\n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            train_accuracy = 0\n",
        "\n",
        "            # This is features, labels cos we dont care about nodeID during training! only during test\n",
        "            for features, labels in trainloader:\n",
        "                model.train()\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "                # print(\"post shape\", features.shape)\n",
        "                # print(\"labels\",labels)\n",
        "                optimizer.zero_grad()\n",
        "                # print(\"features\", features.shape)\n",
        "\n",
        "                # features = features.unsqueeze(1) #unsqueeze\n",
        "                # flatten features\n",
        "                features = features.view(features.shape[0], -1)\n",
        "\n",
        "                logps = model(features)  # log probabilities\n",
        "                # print(\"labelsssss\", labels.shape)\n",
        "                loss = criterion(logps, labels)\n",
        "\n",
        "                # Actual probabilities\n",
        "                ps = logps  # torch.exp(logps) #Only use this if the loss is nlloss\n",
        "                # print(\"ppppp\",ps)\n",
        "\n",
        "                top_p, top_class = ps.topk(1,\n",
        "                                           dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                # print(top_p)\n",
        "                equals = top_class == labels.view(\n",
        "                    *top_class.shape)  # making the shape of the label and top class the same\n",
        "                train_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "            else:\n",
        "                # Everything in this else block executes after every epock\n",
        "                # print(f\"training loss: {running_loss}\")\n",
        "\n",
        "                # test_loss = 0\n",
        "                # test_accuracy = 0\n",
        "                #\n",
        "                # # Turn off gradients for validation, saves memory and computations\n",
        "                # with torch.no_grad():\n",
        "                #     # Doing validation\n",
        "                #\n",
        "                #     # set model to evaluation mode\n",
        "                #     model.eval()\n",
        "                #\n",
        "                #     if e == epochs - 1:\n",
        "                #         print(\"Doing attack validation===========\")\n",
        "                #     # validation pass\n",
        "                #\n",
        "                #     for features, labels in testloader:\n",
        "                #         # features = features.unsqueeze(1)  # unsqueeze\n",
        "                #         features = features.view(features.shape[0], -1)\n",
        "                #         logps = model(features)\n",
        "                #         test_loss += criterion(logps, labels)\n",
        "                #\n",
        "                #         # Actual probabilities\n",
        "                #         ps = torch.exp(logps)\n",
        "                #\n",
        "                #         top_p, top_class = ps.topk(1,\n",
        "                #                                    dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                #         # print(top_p)\n",
        "                #         equals = top_class == labels.view(\n",
        "                #             *top_class.shape)  # making the shape of the label and top class the same\n",
        "                #         test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                test_loss, test_accuracy, _, _, _, _, _, _, _ = attack_test(model, testloader, trainTest=True)\n",
        "\n",
        "                # set model back yo train model\n",
        "                model.train()\n",
        "                # scheduler.step()\n",
        "\n",
        "                train_losses.append(running_loss / len(trainloader))\n",
        "                test_losses.append(test_loss)\n",
        "\n",
        "                # get final train loss. To be returned at the end of the training loop\n",
        "                final_train_loss = running_loss / len(trainloader)\n",
        "\n",
        "                print(\"Epoch: {}/{}..\".format(e + 1, epochs),\n",
        "                      \"Training loss: {:.5f}..\".format(running_loss / len(trainloader)),\n",
        "                      \"Test Loss: {:.5f}..\".format(test_loss),\n",
        "                      \"Train Accuracy: {:.3f}\".format(train_accuracy / len(trainloader)),\n",
        "                      \"Test Accuracy: {:.3f}\".format(test_accuracy)\n",
        "                      )\n",
        "\n",
        "        # # plot train and test loss\n",
        "        # plt.show()\n",
        "        # plt.plot(train_losses)\n",
        "        # plt.plot(test_losses)\n",
        "        # plt.title('Model Losses')\n",
        "        # plt.ylabel('loss')\n",
        "        # plt.xlabel('epoch')\n",
        "        # plt.legend(['train', 'val'], loc='upper left')\n",
        "        # plt.show()\n",
        "\n",
        "        return final_train_loss\n",
        "\n",
        "\n",
        "    def attack_test(model, testloader, singleClass=False, trainTest=False):\n",
        "        test_loss = 0\n",
        "        test_accuracy = 0\n",
        "        auroc = 0\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f_score = 0\n",
        "\n",
        "        posteriors = []\n",
        "        all_nodeIDs = []\n",
        "        true_predicted_nodeIDs_and_class = {}\n",
        "        false_predicted_nodeIDs_and_class = {}\n",
        "\n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            # Doing validation\n",
        "\n",
        "            # set model to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            if trainTest:\n",
        "                for features, labels in testloader:\n",
        "                    features, labels = features.to(device), labels.to(device)\n",
        "                    # features = features.unsqueeze(1)  # unsqueeze\n",
        "                    features = features.view(features.shape[0], -1)\n",
        "                    logps = model(features)\n",
        "                    test_loss += criterion(logps, labels)\n",
        "\n",
        "                    # Actual probabilities\n",
        "                    ps = logps  # torch.exp(logps)\n",
        "                    posteriors.append(ps)\n",
        "\n",
        "                    # if singleclass=false\n",
        "                    if not singleClass:\n",
        "                        y_true = labels.cpu().unsqueeze(-1)\n",
        "                        # print(\"y_true\", y_true)\n",
        "                        y_pred = ps.argmax(dim=-1, keepdim=True)\n",
        "                        # print(\"y_pred\", y_pred)\n",
        "\n",
        "                        # uncomment this to show AUROC\n",
        "                        auroc += roc_auc_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
        "                        # print(\"auroc\", auroc)\n",
        "\n",
        "                        precision += precision_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        recall += recall_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        f_score += f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                    top_p, top_class = ps.topk(1,\n",
        "                                               dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                    # print(top_p)\n",
        "                    equals = top_class == labels.view(\n",
        "                        *top_class.shape)  # making the shape of the label and top class the same\n",
        "                    test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            else:\n",
        "                for features, labels, nodeIDs in testloader:\n",
        "                    features, labels = features.to(device), labels.to(device)\n",
        "                    # features = features.unsqueeze(1)  # unsqueeze\n",
        "                    features = features.view(features.shape[0], -1)\n",
        "                    logps = model(features)\n",
        "                    test_loss += criterion(logps, labels)\n",
        "\n",
        "                    # Actual probabilities\n",
        "                    ps = logps  # torch.exp(logps)\n",
        "                    posteriors.append(ps)\n",
        "\n",
        "                    # print(\"ps\", ps)\n",
        "                    # print(\"nodeIDs\", nodeIDs)\n",
        "\n",
        "                    all_nodeIDs.append(nodeIDs)\n",
        "\n",
        "                    # if singleclass=false\n",
        "                    if not singleClass:\n",
        "                        y_true = labels.cpu().unsqueeze(-1)\n",
        "                        # print(\"y_true\", y_true)\n",
        "                        y_pred = ps.argmax(dim=-1, keepdim=True)\n",
        "                        # print(\"y_pred\", y_pred)\n",
        "\n",
        "                        # uncomment this to show AUROC\n",
        "                        auroc += roc_auc_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
        "                        # print(\"auroc\", auroc)\n",
        "\n",
        "                        precision += precision_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        recall += recall_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        f_score += f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                    top_p, top_class = ps.topk(1, dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                    # print(\"top_p\", top_p)\n",
        "                    # print(\"top_class\", top_class)\n",
        "\n",
        "                    equals = top_class == labels.view(*top_class.shape)  # making the shape of the label and top class the same\n",
        "                    # print(\"equals\", equals)\n",
        "                    for i in range(len(equals)):\n",
        "                        if equals[i]:\n",
        "                            # print(\"baba\")\n",
        "                            # print(\"true pred nodeIDs\", nodeIDs[i])\n",
        "\n",
        "                            true_predicted_nodeIDs_and_class[nodeIDs[i].item()] = top_class[i].item()\n",
        "                        else:\n",
        "                            false_predicted_nodeIDs_and_class[nodeIDs[i].item()] = top_class[i].item()\n",
        "\n",
        "\n",
        "\n",
        "                    test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "\n",
        "\n",
        "        test_accuracy = test_accuracy / len(testloader)\n",
        "        test_loss = test_loss / len(testloader)\n",
        "        final_auroc = auroc / len(testloader)\n",
        "        final_precision = precision / len(testloader)\n",
        "        final_recall = recall / len(testloader)\n",
        "        final_f_score = f_score / len(testloader)\n",
        "\n",
        "        # print('final precision', final_precision)\n",
        "        # print('final micro precision', final_recall)\n",
        "\n",
        "        # print(\"final auroc\", final_auroc)\n",
        "        # if all_nodeIDs:\n",
        "        #     print(\"all_nodeIDs\", torch.cat(all_nodeIDs, 0), \"shape Cat\", torch.cat(all_nodeIDs, 0).shape)\n",
        "        #     # true_predicted_nodeIDs_and_class = torch.cat(true_predicted_nodeIDs_and_class)\n",
        "\n",
        "        return test_loss, test_accuracy, posteriors, final_auroc, final_precision, final_recall, final_f_score, true_predicted_nodeIDs_and_class, false_predicted_nodeIDs_and_class\n",
        "\n",
        "\n",
        "    '''Initialization / params for attack model'''\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # nn.NLLLoss() # cross entropy loss\n",
        "\n",
        "    optimizer = torch.optim.Adam(attack_model.parameters(), lr=0.01)  # 0.01 #0.00001\n",
        "\n",
        "    epochs = 100  # 1000\n",
        "\n",
        "    '''==============Train and test Attack model ========== '''\n",
        "\n",
        "    attack_train(attack_model, attack_train_data_loader, attack_test_data_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    # test to confirm using attack_test_data_loader\n",
        "    _, test_accuracyConfirmTest, posteriors, auroc, precision, recall, f_score, _, _ = attack_test(attack_model,\n",
        "                                                                                             attack_test_data_loader, trainTest=True)\n",
        "    # print(posteriors)\n",
        "\n",
        "    # This is d result on the test set we used i.e split the attack data into train and test.\n",
        "    # Size of the test = 50\n",
        "    print(\"To confirm using attack_test_data_loader (50 test samples): {:.3f}\".format(test_accuracyConfirmTest),\n",
        "          \"AUROC: {:.3f}\".format(auroc), \"precision: {:.3f}\".format(precision), \"recall {:.3f}\".format(recall))\n",
        "\n",
        "    # test for InOut train target data\n",
        "    # This is the one we are interested in\n",
        "    _, test_accuracyInOut, posteriors, auroc, precision, recall, f_score, true_predicted_nodeIDs_and_class, false_predicted_nodeIDs_and_class = attack_test(attack_model,\n",
        "                                                                                       target_data_for_testing_InOutTrain_data_loader)\n",
        "    # # print(posteriors)\n",
        "    # print(\"true_predicted_nodeIDs_and_class\", true_predicted_nodeIDs_and_class)\n",
        "    # print(\"false_predicted_nodeIDs_and_class\", false_predicted_nodeIDs_and_class)\n",
        "\n",
        "    print(\"Test accuracy with Target Train InOut: {:.3f}\".format(test_accuracyInOut), \"AUROC: {:.3f}\".format(auroc),\n",
        "          \"precision: {:.3f}\".format(precision), \"recall {:.3f}\".format(recall), \"F1 score {:.3f}\".format(f_score),\n",
        "          \"===> Attack Performance!\")\n",
        "\n",
        "    result_file.write(\n",
        "        \"Test accuracy with Target Train InOut: {:.3f} \".format(test_accuracyInOut) + \" AUROC: {:.3f}\".format(auroc) +\n",
        "        \" precision: {:.3f}\".format(precision) + \" recall {:.3f}\".format(recall) + \" F1 score {:.3f}\".format(f_score) +\n",
        "        \"===> Attack Performance! \\n\")\n",
        "\n",
        "    # test for Only In train target data\n",
        "    _, test_accuracyIn, posteriors, _, precision, recall, f_score, _, _ = attack_test(attack_model,\n",
        "                                                                                target_data_for_testing_InTrain_data_loader,\n",
        "                                                                                True)\n",
        "    # print(\"Test accuracy with Target Train In: {:.3f}\".format(test_accuracyIn), \"precision: {:.3f}\".format(precision),\n",
        "    #       \"recall {:.3f}\".format(recall), \"F1 score {:.3f}\".format(f_score))\n",
        "\n",
        "    # test for Only Out train target data\n",
        "    _, test_accuracyOut, posteriors, _, precision, recall, f_score, _, _ = attack_test(attack_model,\n",
        "                                                                                 target_data_for_testing_OutTrain_data_loader,\n",
        "                                                                                 True)\n",
        "    # print(\"Test accuracy with Target Train Out: {:.3f}\".format(test_accuracyOut), \"precision: {:.3f}\".format(precision),\n",
        "    #       \"recall {:.3f}\".format(recall), \"F1 score {:.3f}\".format(f_score))\n",
        "\n",
        "    result_file.write(\n",
        "        \"Test accuracy with Target Train In: {:.3f} \".format(\n",
        "            test_accuracyIn) + \" |=====| Test accuracy with Target Train Out: {:.3f}\".format(test_accuracyOut) + \"\\n\")\n",
        "\n",
        "    print(\"data_type\", data_type)\n",
        "    print(\"model_type\", model_type)\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_time = round(end_time - start_time, 3)\n",
        "    print(\"WhichRun\", which_run, \" Total time\", total_time)\n",
        "\n",
        "    # result_file.write(\"Data:\"+data_type +\" Model:\"+ model_type+\"\\n\\n\\n\")\n",
        "    result_file.write(\" ================ WhichRun: \" + str(\n",
        "        which_run) + \" || Data: \" + data_type + \" || Model: \" + model_type + \" || Time: \" + str(\n",
        "        total_time) + \" || rand_state: \" + str(rand_state) + \" ================== \\n\\n\\n\")\n",
        "\n",
        "    result_file.close()\n",
        "\n",
        "    # Uncomment unless SAGE. TODO merge SAGE\n",
        "    # print(\"==================== Begin Quick analysis===============================\")\n",
        "    # # Analyzing lookup Quick analysis\n",
        "    # # Look at the look up table, merge the files of both n then print out the connectivity / values of the lookup table wrt true_predicted_nodeIDs_and_class\n",
        "    #\n",
        "    # # Load\n",
        "    # read_target_InTrain_nodes_neigbors_lookup = np.load(save_target_InTrain_nodes_neigbors, allow_pickle='TRUE')\n",
        "    #\n",
        "    # read_target_OutTrain_nodes_neigbors_lookup = np.load(save_target_OutTrain_nodes_neigbors, allow_pickle='TRUE')\n",
        "    #\n",
        "    # print(\"read_target_InTrain_nodes_neigbors_lookup\", len(read_target_InTrain_nodes_neigbors_lookup))\n",
        "    # print(\"read_target_OutTrain_nodes_neigbors_lookup\", len(read_target_OutTrain_nodes_neigbors_lookup))\n",
        "    # # Merge the 2 list #dictionary\n",
        "    # # all_nodes_lookup = {**read_target_InTrain_nodes_neigbors_lookup, **read_target_OutTrain_nodes_neigbors_lookup}\n",
        "    # all_nodes_lookup = np.concatenate((read_target_InTrain_nodes_neigbors_lookup, read_target_OutTrain_nodes_neigbors_lookup), axis=0)\n",
        "    # print(len(all_nodes_lookup))\n",
        "    #\n",
        "    # print(\"all_nodes_lookup\", all_nodes_lookup)\n",
        "    #\n",
        "    # # convert list to dict\n",
        "    # all_nodes_lookup_dict = {all_nodes_lookup[nodeID][0]: all_nodes_lookup[nodeID][1] for nodeID in range(0, len(all_nodes_lookup))}\n",
        "    # print(\"all_nodes_lookup_dict\", all_nodes_lookup_dict)\n",
        "    #\n",
        "    # # this is done for the display of the graph\n",
        "    # all_correct_classified_nodes = []\n",
        "    # all_incorrect_classified_nodes = []\n",
        "    # correct_edge_index = []\n",
        "    # incorrect_edge_index = []\n",
        "    #\n",
        "    # for nodeID in true_predicted_nodeIDs_and_class.keys():\n",
        "    #     # get value not index for all_nodes_lookup cos its now a list?\n",
        "    #     print(\"Node\", nodeID, \"==>\", all_nodes_lookup_dict[nodeID], \"True Predicted class ==>\", true_predicted_nodeIDs_and_class[nodeID])\n",
        "    #\n",
        "    #     all_correct_classified_nodes.append(nodeID)\n",
        "    #     for i in range(0, len(all_nodes_lookup_dict[nodeID])):\n",
        "    #         edge_ind = (nodeID, all_nodes_lookup_dict[nodeID][i])\n",
        "    #         # print(\"edge_ind\", edge_ind)\n",
        "    #         correct_edge_index.append((edge_ind))\n",
        "    #\n",
        "    # print(\"======================End True predicted=============================\")\n",
        "    #\n",
        "    # for nodeID in false_predicted_nodeIDs_and_class.keys():\n",
        "    #     # get value not index for all_nodes_lookup cos its now a list?\n",
        "    #     print(\"Node\", nodeID, \"==>\", all_nodes_lookup_dict[nodeID], \"False Predicted class ==>\", false_predicted_nodeIDs_and_class[nodeID])\n",
        "    #\n",
        "    #     all_incorrect_classified_nodes.append(nodeID)\n",
        "    #     for i in range(0, len(all_nodes_lookup_dict[nodeID])):\n",
        "    #         edge_ind = (nodeID, all_nodes_lookup_dict[nodeID][i])\n",
        "    #         # print(\"edge_ind\", edge_ind)\n",
        "    #         incorrect_edge_index.append((edge_ind))\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    # def plot_graph_result(edges, nodes, edges2, nodes2, color = \"blue\"):\n",
        "    #     print(\"edges\", edges)\n",
        "    #     # labels = labels.numpy() #dataset.data.y.numpy()\n",
        "    #\n",
        "    #     G = nx.Graph()\n",
        "    #     G.add_nodes_from(nodes)\n",
        "    #     G.add_edges_from(edges)\n",
        "    #\n",
        "    #     G2 = nx.Graph()\n",
        "    #     G2.add_nodes_from(nodes2)\n",
        "    #     G2.add_edges_from(edges2)\n",
        "    #\n",
        "    #     # plt.subplot(111)\n",
        "    #     options = {\n",
        "    #         'node_size': 30,\n",
        "    #         'width': 0.2,\n",
        "    #     }\n",
        "    #     nx.draw(G, with_labels=False, node_color=\"blue\", cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    #     nx.draw(G2, with_labels=False, node_color=\"red\", cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    #     # plt.savefig(save_pics_filename)\n",
        "    #     plt.show()\n",
        "    #\n",
        "    #\n",
        "    # plot_graph_result(correct_edge_index, all_correct_classified_nodes, incorrect_edge_index, all_incorrect_classified_nodes, color=\"purple\")\n",
        "    #\n",
        "    # # plot_graph_result(incorrect_edge_index, all_incorrect_classified_nodes, color=\"pink\")\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    # print(\"==================== End Quick analysis===============================\")\n",
        "    # sys.exit()\n",
        "\n",
        "    # from sklearn.svm import SVC\n",
        "    # from sklearn.ensemble import VotingClassifier\n",
        "    # from sklearn.linear_model import LogisticRegression\n",
        "    # from sklearn.tree import DecisionTreeClassifier\n",
        "    #\n",
        "    # # Ensemble method using Logistic regression & Decision trees\n",
        "    # lr_clf = LogisticRegression(random_state=0)\n",
        "    #\n",
        "    # dec_clf = DecisionTreeClassifier()\n",
        "    #\n",
        "    # voting_clf2 = VotingClassifier(\n",
        "    #     estimators=[('lr', lr_clf), ('decision', dec_clf)],\n",
        "    #     voting='hard')\n",
        "    # voting_clf2.fit(X_attack_InOut, y_attack_InOut) # for shadow\n",
        "    #\n",
        "    # # performance\n",
        "    # print(\"Ensemble accuracy performance! Interested\", voting_clf2.score(X_InOutTrain, y_InOutTrain)) # for target\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "rand_state 3117792998\n",
            "data Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n",
            "label_idx 2708\n",
            "idx.size(0) 351\n",
            "idx.size(0) 217\n",
            "idx.size(0) 418\n",
            "idx.size(0) 818\n",
            "idx.size(0) 426\n",
            "idx.size(0) 298\n",
            "idx.size(0) 180\n",
            "shadow_train_idx 630\n",
            "Target_train_idx 630\n",
            "done others\n",
            "done target test\n",
            "done shadow test\n",
            "target_test_idx 630\n",
            "shadow_test_idx 630\n",
            "data new Data(all_edge_index=[2, 10556], all_x=[2708, 1433], all_y=[2708], shadow_edge_index=[2, 706], shadow_test_edge_index=[2, 584], shadow_test_mask=[2708], shadow_test_x=[630, 1433], shadow_test_y=[630], shadow_train_mask=[630], shadow_x=[630, 1433], shadow_y=[630], target_edge_index=[2, 650], target_test_edge_index=[2, 522], target_test_mask=[2708], target_test_x=[630, 1433], target_test_y=[630], target_train_mask=[630], target_x=[630, 1433], target_y=[630])\n",
            "data_new.shadow_test_mask.sum() tensor(630)\n",
            "data_new.target_test_mask.sum() tensor(630)\n",
            "7\n",
            "data_new.target_edge_index torch.Size([2, 650])\n",
            "data_new.target_x torch.Size([630, 1433])\n",
            "model TargetModel(\n",
            "  (conv1): GCNConv(1433, 256)\n",
            "  (conv2): GCNConv(256, 7)\n",
            ")\n",
            "TargetModel Epoch: 001, Approx Train: 0.1317, Train: 0.1413, Test: 0.1159,marco: 0.1068,micro: 0.1159\n",
            "TargetModel Epoch: 002, Approx Train: 0.1413, Train: 0.1571, Test: 0.1143,marco: 0.1057,micro: 0.1143\n",
            "TargetModel Epoch: 003, Approx Train: 0.1571, Train: 0.1778, Test: 0.1190,marco: 0.1107,micro: 0.1190\n",
            "TargetModel Epoch: 004, Approx Train: 0.1778, Train: 0.1937, Test: 0.1222,marco: 0.1133,micro: 0.1222\n",
            "TargetModel Epoch: 005, Approx Train: 0.1937, Train: 0.2127, Test: 0.1286,marco: 0.1200,micro: 0.1286\n",
            "TargetModel Epoch: 006, Approx Train: 0.2127, Train: 0.2317, Test: 0.1381,marco: 0.1297,micro: 0.1381\n",
            "TargetModel Epoch: 007, Approx Train: 0.2317, Train: 0.2508, Test: 0.1429,marco: 0.1357,micro: 0.1429\n",
            "TargetModel Epoch: 008, Approx Train: 0.2508, Train: 0.2698, Test: 0.1476,marco: 0.1417,micro: 0.1476\n",
            "TargetModel Epoch: 009, Approx Train: 0.2698, Train: 0.2889, Test: 0.1587,marco: 0.1509,micro: 0.1587\n",
            "TargetModel Epoch: 010, Approx Train: 0.2889, Train: 0.3032, Test: 0.1667,marco: 0.1600,micro: 0.1667\n",
            "TargetModel Epoch: 011, Approx Train: 0.3032, Train: 0.3270, Test: 0.1698,marco: 0.1627,micro: 0.1698\n",
            "TargetModel Epoch: 012, Approx Train: 0.3270, Train: 0.3381, Test: 0.1794,marco: 0.1732,micro: 0.1794\n",
            "TargetModel Epoch: 013, Approx Train: 0.3381, Train: 0.3524, Test: 0.1825,marco: 0.1761,micro: 0.1825\n",
            "TargetModel Epoch: 014, Approx Train: 0.3524, Train: 0.3714, Test: 0.1937,marco: 0.1867,micro: 0.1937\n",
            "TargetModel Epoch: 015, Approx Train: 0.3714, Train: 0.3810, Test: 0.2000,marco: 0.1914,micro: 0.2000\n",
            "TargetModel Epoch: 016, Approx Train: 0.3810, Train: 0.3968, Test: 0.2111,marco: 0.2018,micro: 0.2111\n",
            "TargetModel Epoch: 017, Approx Train: 0.3968, Train: 0.4143, Test: 0.2206,marco: 0.2155,micro: 0.2206\n",
            "TargetModel Epoch: 018, Approx Train: 0.4143, Train: 0.4302, Test: 0.2333,marco: 0.2306,micro: 0.2333\n",
            "TargetModel Epoch: 019, Approx Train: 0.4302, Train: 0.4540, Test: 0.2381,marco: 0.2343,micro: 0.2381\n",
            "TargetModel Epoch: 020, Approx Train: 0.4540, Train: 0.4825, Test: 0.2476,marco: 0.2474,micro: 0.2476\n",
            "TargetModel Epoch: 021, Approx Train: 0.4825, Train: 0.4937, Test: 0.2556,marco: 0.2553,micro: 0.2556\n",
            "TargetModel Epoch: 022, Approx Train: 0.4937, Train: 0.5111, Test: 0.2619,marco: 0.2622,micro: 0.2619\n",
            "TargetModel Epoch: 023, Approx Train: 0.5111, Train: 0.5302, Test: 0.2635,marco: 0.2626,micro: 0.2635\n",
            "TargetModel Epoch: 024, Approx Train: 0.5302, Train: 0.5460, Test: 0.2698,marco: 0.2677,micro: 0.2698\n",
            "TargetModel Epoch: 025, Approx Train: 0.5460, Train: 0.5603, Test: 0.2778,marco: 0.2771,micro: 0.2778\n",
            "TargetModel Epoch: 026, Approx Train: 0.5603, Train: 0.5762, Test: 0.2873,marco: 0.2872,micro: 0.2873\n",
            "TargetModel Epoch: 027, Approx Train: 0.5762, Train: 0.5921, Test: 0.2984,marco: 0.2975,micro: 0.2984\n",
            "TargetModel Epoch: 028, Approx Train: 0.5921, Train: 0.6000, Test: 0.3079,marco: 0.3068,micro: 0.3079\n",
            "TargetModel Epoch: 029, Approx Train: 0.6000, Train: 0.6111, Test: 0.3159,marco: 0.3144,micro: 0.3159\n",
            "TargetModel Epoch: 030, Approx Train: 0.6111, Train: 0.6143, Test: 0.3190,marco: 0.3172,micro: 0.3190\n",
            "TargetModel Epoch: 031, Approx Train: 0.6143, Train: 0.6175, Test: 0.3254,marco: 0.3243,micro: 0.3254\n",
            "TargetModel Epoch: 032, Approx Train: 0.6175, Train: 0.6349, Test: 0.3302,marco: 0.3284,micro: 0.3302\n",
            "TargetModel Epoch: 033, Approx Train: 0.6349, Train: 0.6444, Test: 0.3333,marco: 0.3317,micro: 0.3333\n",
            "TargetModel Epoch: 034, Approx Train: 0.6444, Train: 0.6587, Test: 0.3413,marco: 0.3405,micro: 0.3413\n",
            "TargetModel Epoch: 035, Approx Train: 0.6587, Train: 0.6635, Test: 0.3444,marco: 0.3435,micro: 0.3444\n",
            "TargetModel Epoch: 036, Approx Train: 0.6635, Train: 0.6714, Test: 0.3492,marco: 0.3486,micro: 0.3492\n",
            "TargetModel Epoch: 037, Approx Train: 0.6714, Train: 0.6762, Test: 0.3524,marco: 0.3503,micro: 0.3524\n",
            "TargetModel Epoch: 038, Approx Train: 0.6762, Train: 0.6762, Test: 0.3556,marco: 0.3540,micro: 0.3556\n",
            "TargetModel Epoch: 039, Approx Train: 0.6762, Train: 0.6762, Test: 0.3587,marco: 0.3573,micro: 0.3587\n",
            "TargetModel Epoch: 040, Approx Train: 0.6762, Train: 0.6810, Test: 0.3635,marco: 0.3622,micro: 0.3635\n",
            "TargetModel Epoch: 041, Approx Train: 0.6810, Train: 0.6873, Test: 0.3683,marco: 0.3654,micro: 0.3683\n",
            "TargetModel Epoch: 042, Approx Train: 0.6873, Train: 0.6921, Test: 0.3730,marco: 0.3685,micro: 0.3730\n",
            "TargetModel Epoch: 043, Approx Train: 0.6921, Train: 0.7000, Test: 0.3746,marco: 0.3701,micro: 0.3746\n",
            "TargetModel Epoch: 044, Approx Train: 0.7000, Train: 0.7048, Test: 0.3762,marco: 0.3715,micro: 0.3762\n",
            "TargetModel Epoch: 045, Approx Train: 0.7048, Train: 0.7127, Test: 0.3825,marco: 0.3773,micro: 0.3825\n",
            "TargetModel Epoch: 046, Approx Train: 0.7127, Train: 0.7159, Test: 0.3889,marco: 0.3820,micro: 0.3889\n",
            "TargetModel Epoch: 047, Approx Train: 0.7159, Train: 0.7222, Test: 0.3952,marco: 0.3881,micro: 0.3952\n",
            "TargetModel Epoch: 048, Approx Train: 0.7222, Train: 0.7238, Test: 0.3952,marco: 0.3883,micro: 0.3952\n",
            "TargetModel Epoch: 049, Approx Train: 0.7238, Train: 0.7317, Test: 0.4016,marco: 0.3961,micro: 0.4016\n",
            "TargetModel Epoch: 050, Approx Train: 0.7317, Train: 0.7349, Test: 0.4095,marco: 0.4042,micro: 0.4095\n",
            "TargetModel Epoch: 051, Approx Train: 0.7349, Train: 0.7349, Test: 0.4095,marco: 0.4042,micro: 0.4095\n",
            "TargetModel Epoch: 052, Approx Train: 0.7349, Train: 0.7397, Test: 0.4127,marco: 0.4071,micro: 0.4127\n",
            "TargetModel Epoch: 053, Approx Train: 0.7397, Train: 0.7397, Test: 0.4222,marco: 0.4170,micro: 0.4222\n",
            "TargetModel Epoch: 054, Approx Train: 0.7397, Train: 0.7413, Test: 0.4222,marco: 0.4168,micro: 0.4222\n",
            "TargetModel Epoch: 055, Approx Train: 0.7413, Train: 0.7460, Test: 0.4302,marco: 0.4230,micro: 0.4302\n",
            "TargetModel Epoch: 056, Approx Train: 0.7460, Train: 0.7460, Test: 0.4365,marco: 0.4304,micro: 0.4365\n",
            "TargetModel Epoch: 057, Approx Train: 0.7460, Train: 0.7476, Test: 0.4365,marco: 0.4289,micro: 0.4365\n",
            "TargetModel Epoch: 058, Approx Train: 0.7476, Train: 0.7508, Test: 0.4349,marco: 0.4266,micro: 0.4349\n",
            "TargetModel Epoch: 059, Approx Train: 0.7508, Train: 0.7524, Test: 0.4397,marco: 0.4299,micro: 0.4397\n",
            "TargetModel Epoch: 060, Approx Train: 0.7524, Train: 0.7556, Test: 0.4429,marco: 0.4321,micro: 0.4429\n",
            "TargetModel Epoch: 061, Approx Train: 0.7556, Train: 0.7571, Test: 0.4444,marco: 0.4354,micro: 0.4444\n",
            "TargetModel Epoch: 062, Approx Train: 0.7571, Train: 0.7571, Test: 0.4460,marco: 0.4367,micro: 0.4460\n",
            "TargetModel Epoch: 063, Approx Train: 0.7571, Train: 0.7571, Test: 0.4476,marco: 0.4386,micro: 0.4476\n",
            "TargetModel Epoch: 064, Approx Train: 0.7571, Train: 0.7556, Test: 0.4540,marco: 0.4437,micro: 0.4540\n",
            "TargetModel Epoch: 065, Approx Train: 0.7556, Train: 0.7571, Test: 0.4571,marco: 0.4467,micro: 0.4571\n",
            "TargetModel Epoch: 066, Approx Train: 0.7571, Train: 0.7603, Test: 0.4571,marco: 0.4467,micro: 0.4571\n",
            "TargetModel Epoch: 067, Approx Train: 0.7603, Train: 0.7619, Test: 0.4571,marco: 0.4464,micro: 0.4571\n",
            "TargetModel Epoch: 068, Approx Train: 0.7619, Train: 0.7619, Test: 0.4603,marco: 0.4497,micro: 0.4603\n",
            "TargetModel Epoch: 069, Approx Train: 0.7619, Train: 0.7619, Test: 0.4603,marco: 0.4497,micro: 0.4603\n",
            "TargetModel Epoch: 070, Approx Train: 0.7619, Train: 0.7635, Test: 0.4651,marco: 0.4552,micro: 0.4651\n",
            "TargetModel Epoch: 071, Approx Train: 0.7635, Train: 0.7635, Test: 0.4667,marco: 0.4561,micro: 0.4667\n",
            "TargetModel Epoch: 072, Approx Train: 0.7635, Train: 0.7635, Test: 0.4667,marco: 0.4561,micro: 0.4667\n",
            "TargetModel Epoch: 073, Approx Train: 0.7635, Train: 0.7651, Test: 0.4683,marco: 0.4590,micro: 0.4683\n",
            "TargetModel Epoch: 074, Approx Train: 0.7651, Train: 0.7651, Test: 0.4714,marco: 0.4620,micro: 0.4714\n",
            "TargetModel Epoch: 075, Approx Train: 0.7651, Train: 0.7651, Test: 0.4698,marco: 0.4592,micro: 0.4698\n",
            "TargetModel Epoch: 076, Approx Train: 0.7651, Train: 0.7667, Test: 0.4714,marco: 0.4614,micro: 0.4714\n",
            "TargetModel Epoch: 077, Approx Train: 0.7667, Train: 0.7667, Test: 0.4714,marco: 0.4614,micro: 0.4714\n",
            "TargetModel Epoch: 078, Approx Train: 0.7667, Train: 0.7698, Test: 0.4698,marco: 0.4597,micro: 0.4698\n",
            "TargetModel Epoch: 079, Approx Train: 0.7698, Train: 0.7698, Test: 0.4714,marco: 0.4615,micro: 0.4714\n",
            "TargetModel Epoch: 080, Approx Train: 0.7698, Train: 0.7683, Test: 0.4778,marco: 0.4672,micro: 0.4778\n",
            "TargetModel Epoch: 081, Approx Train: 0.7683, Train: 0.7683, Test: 0.4794,marco: 0.4681,micro: 0.4794\n",
            "TargetModel Epoch: 082, Approx Train: 0.7683, Train: 0.7683, Test: 0.4794,marco: 0.4674,micro: 0.4794\n",
            "TargetModel Epoch: 083, Approx Train: 0.7683, Train: 0.7683, Test: 0.4841,marco: 0.4717,micro: 0.4841\n",
            "TargetModel Epoch: 084, Approx Train: 0.7683, Train: 0.7683, Test: 0.4905,marco: 0.4773,micro: 0.4905\n",
            "TargetModel Epoch: 085, Approx Train: 0.7683, Train: 0.7683, Test: 0.4921,marco: 0.4813,micro: 0.4921\n",
            "TargetModel Epoch: 086, Approx Train: 0.7683, Train: 0.7667, Test: 0.4921,marco: 0.4811,micro: 0.4921\n",
            "TargetModel Epoch: 087, Approx Train: 0.7667, Train: 0.7667, Test: 0.4937,marco: 0.4820,micro: 0.4937\n",
            "TargetModel Epoch: 088, Approx Train: 0.7667, Train: 0.7714, Test: 0.4968,marco: 0.4855,micro: 0.4968\n",
            "TargetModel Epoch: 089, Approx Train: 0.7714, Train: 0.7714, Test: 0.5000,marco: 0.4886,micro: 0.5000\n",
            "TargetModel Epoch: 090, Approx Train: 0.7714, Train: 0.7714, Test: 0.5000,marco: 0.4892,micro: 0.5000\n",
            "TargetModel Epoch: 091, Approx Train: 0.7714, Train: 0.7714, Test: 0.5048,marco: 0.4932,micro: 0.5048\n",
            "TargetModel Epoch: 092, Approx Train: 0.7714, Train: 0.7714, Test: 0.5048,marco: 0.4932,micro: 0.5048\n",
            "TargetModel Epoch: 093, Approx Train: 0.7714, Train: 0.7714, Test: 0.5063,marco: 0.4954,micro: 0.5063\n",
            "TargetModel Epoch: 094, Approx Train: 0.7714, Train: 0.7714, Test: 0.5063,marco: 0.4954,micro: 0.5063\n",
            "TargetModel Epoch: 095, Approx Train: 0.7714, Train: 0.7714, Test: 0.5095,marco: 0.4994,micro: 0.5095\n",
            "TargetModel Epoch: 096, Approx Train: 0.7714, Train: 0.7714, Test: 0.5111,marco: 0.5005,micro: 0.5111\n",
            "TargetModel Epoch: 097, Approx Train: 0.7714, Train: 0.7714, Test: 0.5159,marco: 0.5048,micro: 0.5159\n",
            "TargetModel Epoch: 098, Approx Train: 0.7714, Train: 0.7730, Test: 0.5159,marco: 0.5041,micro: 0.5159\n",
            "TargetModel Epoch: 099, Approx Train: 0.7730, Train: 0.7730, Test: 0.5159,marco: 0.5039,micro: 0.5159\n",
            "TargetModel Epoch: 100, Approx Train: 0.7730, Train: 0.7714, Test: 0.5175,marco: 0.5072,micro: 0.5175\n",
            "TargetModel Epoch: 101, Approx Train: 0.7714, Train: 0.7730, Test: 0.5190,marco: 0.5083,micro: 0.5190\n",
            "TargetModel Epoch: 102, Approx Train: 0.7730, Train: 0.7746, Test: 0.5222,marco: 0.5111,micro: 0.5222\n",
            "TargetModel Epoch: 103, Approx Train: 0.7746, Train: 0.7746, Test: 0.5222,marco: 0.5104,micro: 0.5222\n",
            "TargetModel Epoch: 104, Approx Train: 0.7746, Train: 0.7746, Test: 0.5238,marco: 0.5114,micro: 0.5238\n",
            "TargetModel Epoch: 105, Approx Train: 0.7746, Train: 0.7746, Test: 0.5286,marco: 0.5170,micro: 0.5286\n",
            "TargetModel Epoch: 106, Approx Train: 0.7746, Train: 0.7746, Test: 0.5286,marco: 0.5169,micro: 0.5286\n",
            "TargetModel Epoch: 107, Approx Train: 0.7746, Train: 0.7762, Test: 0.5333,marco: 0.5223,micro: 0.5333\n",
            "TargetModel Epoch: 108, Approx Train: 0.7762, Train: 0.7762, Test: 0.5349,marco: 0.5235,micro: 0.5349\n",
            "TargetModel Epoch: 109, Approx Train: 0.7762, Train: 0.7778, Test: 0.5349,marco: 0.5235,micro: 0.5349\n",
            "TargetModel Epoch: 110, Approx Train: 0.7778, Train: 0.7762, Test: 0.5365,marco: 0.5247,micro: 0.5365\n",
            "TargetModel Epoch: 111, Approx Train: 0.7762, Train: 0.7778, Test: 0.5413,marco: 0.5286,micro: 0.5413\n",
            "TargetModel Epoch: 112, Approx Train: 0.7778, Train: 0.7778, Test: 0.5413,marco: 0.5288,micro: 0.5413\n",
            "TargetModel Epoch: 113, Approx Train: 0.7778, Train: 0.7810, Test: 0.5429,marco: 0.5292,micro: 0.5429\n",
            "TargetModel Epoch: 114, Approx Train: 0.7810, Train: 0.7825, Test: 0.5429,marco: 0.5292,micro: 0.5429\n",
            "TargetModel Epoch: 115, Approx Train: 0.7825, Train: 0.7841, Test: 0.5444,marco: 0.5319,micro: 0.5444\n",
            "TargetModel Epoch: 116, Approx Train: 0.7841, Train: 0.7841, Test: 0.5444,marco: 0.5316,micro: 0.5444\n",
            "TargetModel Epoch: 117, Approx Train: 0.7841, Train: 0.7857, Test: 0.5444,marco: 0.5323,micro: 0.5444\n",
            "TargetModel Epoch: 118, Approx Train: 0.7857, Train: 0.7857, Test: 0.5444,marco: 0.5323,micro: 0.5444\n",
            "TargetModel Epoch: 119, Approx Train: 0.7857, Train: 0.7841, Test: 0.5444,marco: 0.5319,micro: 0.5444\n",
            "TargetModel Epoch: 120, Approx Train: 0.7841, Train: 0.7841, Test: 0.5492,marco: 0.5366,micro: 0.5492\n",
            "TargetModel Epoch: 121, Approx Train: 0.7841, Train: 0.7841, Test: 0.5540,marco: 0.5429,micro: 0.5540\n",
            "TargetModel Epoch: 122, Approx Train: 0.7841, Train: 0.7841, Test: 0.5540,marco: 0.5429,micro: 0.5540\n",
            "TargetModel Epoch: 123, Approx Train: 0.7841, Train: 0.7841, Test: 0.5540,marco: 0.5429,micro: 0.5540\n",
            "TargetModel Epoch: 124, Approx Train: 0.7841, Train: 0.7841, Test: 0.5540,marco: 0.5429,micro: 0.5540\n",
            "TargetModel Epoch: 125, Approx Train: 0.7841, Train: 0.7857, Test: 0.5556,marco: 0.5443,micro: 0.5556\n",
            "TargetModel Epoch: 126, Approx Train: 0.7857, Train: 0.7857, Test: 0.5556,marco: 0.5443,micro: 0.5556\n",
            "TargetModel Epoch: 127, Approx Train: 0.7857, Train: 0.7857, Test: 0.5587,marco: 0.5474,micro: 0.5587\n",
            "TargetModel Epoch: 128, Approx Train: 0.7857, Train: 0.7857, Test: 0.5587,marco: 0.5474,micro: 0.5587\n",
            "TargetModel Epoch: 129, Approx Train: 0.7857, Train: 0.7857, Test: 0.5619,marco: 0.5508,micro: 0.5619\n",
            "TargetModel Epoch: 130, Approx Train: 0.7857, Train: 0.7857, Test: 0.5651,marco: 0.5532,micro: 0.5651\n",
            "TargetModel Epoch: 131, Approx Train: 0.7857, Train: 0.7873, Test: 0.5651,marco: 0.5532,micro: 0.5651\n",
            "TargetModel Epoch: 132, Approx Train: 0.7873, Train: 0.7873, Test: 0.5667,marco: 0.5552,micro: 0.5667\n",
            "TargetModel Epoch: 133, Approx Train: 0.7873, Train: 0.7889, Test: 0.5667,marco: 0.5552,micro: 0.5667\n",
            "TargetModel Epoch: 134, Approx Train: 0.7889, Train: 0.7889, Test: 0.5683,marco: 0.5565,micro: 0.5683\n",
            "TargetModel Epoch: 135, Approx Train: 0.7889, Train: 0.7921, Test: 0.5698,marco: 0.5577,micro: 0.5698\n",
            "TargetModel Epoch: 136, Approx Train: 0.7921, Train: 0.7937, Test: 0.5698,marco: 0.5577,micro: 0.5698\n",
            "TargetModel Epoch: 137, Approx Train: 0.7937, Train: 0.7921, Test: 0.5714,marco: 0.5593,micro: 0.5714\n",
            "TargetModel Epoch: 138, Approx Train: 0.7921, Train: 0.7921, Test: 0.5714,marco: 0.5593,micro: 0.5714\n",
            "TargetModel Epoch: 139, Approx Train: 0.7921, Train: 0.7921, Test: 0.5730,marco: 0.5619,micro: 0.5730\n",
            "TargetModel Epoch: 140, Approx Train: 0.7921, Train: 0.7921, Test: 0.5746,marco: 0.5638,micro: 0.5746\n",
            "TargetModel Epoch: 141, Approx Train: 0.7921, Train: 0.7921, Test: 0.5746,marco: 0.5638,micro: 0.5746\n",
            "TargetModel Epoch: 142, Approx Train: 0.7921, Train: 0.7921, Test: 0.5746,marco: 0.5642,micro: 0.5746\n",
            "TargetModel Epoch: 143, Approx Train: 0.7921, Train: 0.7921, Test: 0.5746,marco: 0.5642,micro: 0.5746\n",
            "TargetModel Epoch: 144, Approx Train: 0.7921, Train: 0.7921, Test: 0.5746,marco: 0.5642,micro: 0.5746\n",
            "TargetModel Epoch: 145, Approx Train: 0.7921, Train: 0.7937, Test: 0.5762,marco: 0.5664,micro: 0.5762\n",
            "TargetModel Epoch: 146, Approx Train: 0.7937, Train: 0.7937, Test: 0.5794,marco: 0.5697,micro: 0.5794\n",
            "TargetModel Epoch: 147, Approx Train: 0.7937, Train: 0.7937, Test: 0.5810,marco: 0.5718,micro: 0.5810\n",
            "TargetModel Epoch: 148, Approx Train: 0.7937, Train: 0.7937, Test: 0.5810,marco: 0.5718,micro: 0.5810\n",
            "TargetModel Epoch: 149, Approx Train: 0.7937, Train: 0.7921, Test: 0.5810,marco: 0.5716,micro: 0.5810\n",
            "TargetModel Epoch: 150, Approx Train: 0.7921, Train: 0.7921, Test: 0.5810,marco: 0.5716,micro: 0.5810\n",
            "TargetModel Epoch: 151, Approx Train: 0.7921, Train: 0.7921, Test: 0.5810,marco: 0.5716,micro: 0.5810\n",
            "TargetModel Epoch: 152, Approx Train: 0.7921, Train: 0.7937, Test: 0.5810,marco: 0.5716,micro: 0.5810\n",
            "TargetModel Epoch: 153, Approx Train: 0.7937, Train: 0.7952, Test: 0.5810,marco: 0.5716,micro: 0.5810\n",
            "TargetModel Epoch: 154, Approx Train: 0.7952, Train: 0.7937, Test: 0.5810,marco: 0.5716,micro: 0.5810\n",
            "TargetModel Epoch: 155, Approx Train: 0.7937, Train: 0.7952, Test: 0.5794,marco: 0.5699,micro: 0.5794\n",
            "TargetModel Epoch: 156, Approx Train: 0.7952, Train: 0.7952, Test: 0.5794,marco: 0.5699,micro: 0.5794\n",
            "TargetModel Epoch: 157, Approx Train: 0.7952, Train: 0.7952, Test: 0.5825,marco: 0.5724,micro: 0.5825\n",
            "TargetModel Epoch: 158, Approx Train: 0.7952, Train: 0.7952, Test: 0.5825,marco: 0.5724,micro: 0.5825\n",
            "TargetModel Epoch: 159, Approx Train: 0.7952, Train: 0.7968, Test: 0.5825,marco: 0.5718,micro: 0.5825\n",
            "TargetModel Epoch: 160, Approx Train: 0.7968, Train: 0.7968, Test: 0.5825,marco: 0.5718,micro: 0.5825\n",
            "TargetModel Epoch: 161, Approx Train: 0.7968, Train: 0.7984, Test: 0.5825,marco: 0.5715,micro: 0.5825\n",
            "TargetModel Epoch: 162, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5715,micro: 0.5825\n",
            "TargetModel Epoch: 163, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5715,micro: 0.5825\n",
            "TargetModel Epoch: 164, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5713,micro: 0.5825\n",
            "TargetModel Epoch: 165, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5713,micro: 0.5825\n",
            "TargetModel Epoch: 166, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5713,micro: 0.5825\n",
            "TargetModel Epoch: 167, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5713,micro: 0.5825\n",
            "TargetModel Epoch: 168, Approx Train: 0.7984, Train: 0.7984, Test: 0.5841,marco: 0.5725,micro: 0.5841\n",
            "TargetModel Epoch: 169, Approx Train: 0.7984, Train: 0.7968, Test: 0.5841,marco: 0.5725,micro: 0.5841\n",
            "TargetModel Epoch: 170, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5725,micro: 0.5841\n",
            "TargetModel Epoch: 171, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5725,micro: 0.5841\n",
            "TargetModel Epoch: 172, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5725,micro: 0.5841\n",
            "TargetModel Epoch: 173, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5733,micro: 0.5841\n",
            "TargetModel Epoch: 174, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5733,micro: 0.5841\n",
            "TargetModel Epoch: 175, Approx Train: 0.7968, Train: 0.7968, Test: 0.5857,marco: 0.5750,micro: 0.5857\n",
            "TargetModel Epoch: 176, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 177, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 178, Approx Train: 0.7968, Train: 0.7968, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 179, Approx Train: 0.7968, Train: 0.7984, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 180, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5694,micro: 0.5825\n",
            "TargetModel Epoch: 181, Approx Train: 0.7984, Train: 0.7984, Test: 0.5825,marco: 0.5693,micro: 0.5825\n",
            "TargetModel Epoch: 182, Approx Train: 0.7984, Train: 0.8016, Test: 0.5841,marco: 0.5705,micro: 0.5841\n",
            "TargetModel Epoch: 183, Approx Train: 0.8016, Train: 0.8016, Test: 0.5825,marco: 0.5691,micro: 0.5825\n",
            "TargetModel Epoch: 184, Approx Train: 0.8016, Train: 0.8032, Test: 0.5841,marco: 0.5710,micro: 0.5841\n",
            "TargetModel Epoch: 185, Approx Train: 0.8032, Train: 0.8032, Test: 0.5857,marco: 0.5737,micro: 0.5857\n",
            "TargetModel Epoch: 186, Approx Train: 0.8032, Train: 0.8032, Test: 0.5857,marco: 0.5737,micro: 0.5857\n",
            "TargetModel Epoch: 187, Approx Train: 0.8032, Train: 0.8032, Test: 0.5857,marco: 0.5737,micro: 0.5857\n",
            "TargetModel Epoch: 188, Approx Train: 0.8032, Train: 0.8048, Test: 0.5857,marco: 0.5737,micro: 0.5857\n",
            "TargetModel Epoch: 189, Approx Train: 0.8048, Train: 0.8032, Test: 0.5857,marco: 0.5740,micro: 0.5857\n",
            "TargetModel Epoch: 190, Approx Train: 0.8032, Train: 0.8032, Test: 0.5857,marco: 0.5740,micro: 0.5857\n",
            "TargetModel Epoch: 191, Approx Train: 0.8032, Train: 0.8048, Test: 0.5873,marco: 0.5751,micro: 0.5873\n",
            "TargetModel Epoch: 192, Approx Train: 0.8048, Train: 0.8048, Test: 0.5857,marco: 0.5738,micro: 0.5857\n",
            "TargetModel Epoch: 193, Approx Train: 0.8048, Train: 0.8063, Test: 0.5873,marco: 0.5758,micro: 0.5873\n",
            "TargetModel Epoch: 194, Approx Train: 0.8063, Train: 0.8063, Test: 0.5889,marco: 0.5779,micro: 0.5889\n",
            "TargetModel Epoch: 195, Approx Train: 0.8063, Train: 0.8063, Test: 0.5889,marco: 0.5779,micro: 0.5889\n",
            "TargetModel Epoch: 196, Approx Train: 0.8063, Train: 0.8063, Test: 0.5905,marco: 0.5791,micro: 0.5905\n",
            "TargetModel Epoch: 197, Approx Train: 0.8063, Train: 0.8063, Test: 0.5889,marco: 0.5774,micro: 0.5889\n",
            "TargetModel Epoch: 198, Approx Train: 0.8063, Train: 0.8048, Test: 0.5873,marco: 0.5761,micro: 0.5873\n",
            "TargetModel Epoch: 199, Approx Train: 0.8048, Train: 0.8048, Test: 0.5873,marco: 0.5761,micro: 0.5873\n",
            "TargetModel Epoch: 200, Approx Train: 0.8048, Train: 0.8048, Test: 0.5873,marco: 0.5761,micro: 0.5873\n",
            "TargetModel Epoch: 201, Approx Train: 0.8048, Train: 0.8048, Test: 0.5857,marco: 0.5740,micro: 0.5857\n",
            "TargetModel Epoch: 202, Approx Train: 0.8048, Train: 0.8048, Test: 0.5857,marco: 0.5742,micro: 0.5857\n",
            "TargetModel Epoch: 203, Approx Train: 0.8048, Train: 0.8048, Test: 0.5841,marco: 0.5723,micro: 0.5841\n",
            "TargetModel Epoch: 204, Approx Train: 0.8048, Train: 0.8048, Test: 0.5841,marco: 0.5723,micro: 0.5841\n",
            "TargetModel Epoch: 205, Approx Train: 0.8048, Train: 0.8048, Test: 0.5857,marco: 0.5737,micro: 0.5857\n",
            "TargetModel Epoch: 206, Approx Train: 0.8048, Train: 0.8063, Test: 0.5873,marco: 0.5753,micro: 0.5873\n",
            "TargetModel Epoch: 207, Approx Train: 0.8063, Train: 0.8063, Test: 0.5857,marco: 0.5737,micro: 0.5857\n",
            "TargetModel Epoch: 208, Approx Train: 0.8063, Train: 0.8079, Test: 0.5857,marco: 0.5737,micro: 0.5857\n",
            "TargetModel Epoch: 209, Approx Train: 0.8079, Train: 0.8079, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 210, Approx Train: 0.8079, Train: 0.8079, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 211, Approx Train: 0.8079, Train: 0.8079, Test: 0.5841,marco: 0.5720,micro: 0.5841\n",
            "TargetModel Epoch: 212, Approx Train: 0.8079, Train: 0.8079, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 213, Approx Train: 0.8079, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 214, Approx Train: 0.8111, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 215, Approx Train: 0.8111, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 216, Approx Train: 0.8111, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 217, Approx Train: 0.8111, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 218, Approx Train: 0.8111, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 219, Approx Train: 0.8111, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 220, Approx Train: 0.8111, Train: 0.8111, Test: 0.5841,marco: 0.5722,micro: 0.5841\n",
            "TargetModel Epoch: 221, Approx Train: 0.8111, Train: 0.8127, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 222, Approx Train: 0.8127, Train: 0.8127, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 223, Approx Train: 0.8127, Train: 0.8143, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 224, Approx Train: 0.8143, Train: 0.8143, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 225, Approx Train: 0.8143, Train: 0.8143, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 226, Approx Train: 0.8143, Train: 0.8143, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 227, Approx Train: 0.8143, Train: 0.8159, Test: 0.5841,marco: 0.5721,micro: 0.5841\n",
            "TargetModel Epoch: 228, Approx Train: 0.8159, Train: 0.8175, Test: 0.5857,marco: 0.5740,micro: 0.5857\n",
            "TargetModel Epoch: 229, Approx Train: 0.8175, Train: 0.8190, Test: 0.5857,marco: 0.5740,micro: 0.5857\n",
            "TargetModel Epoch: 230, Approx Train: 0.8190, Train: 0.8190, Test: 0.5873,marco: 0.5754,micro: 0.5873\n",
            "TargetModel Epoch: 231, Approx Train: 0.8190, Train: 0.8206, Test: 0.5873,marco: 0.5754,micro: 0.5873\n",
            "TargetModel Epoch: 232, Approx Train: 0.8206, Train: 0.8206, Test: 0.5873,marco: 0.5754,micro: 0.5873\n",
            "TargetModel Epoch: 233, Approx Train: 0.8206, Train: 0.8206, Test: 0.5873,marco: 0.5757,micro: 0.5873\n",
            "TargetModel Epoch: 234, Approx Train: 0.8206, Train: 0.8206, Test: 0.5873,marco: 0.5757,micro: 0.5873\n",
            "TargetModel Epoch: 235, Approx Train: 0.8206, Train: 0.8206, Test: 0.5873,marco: 0.5757,micro: 0.5873\n",
            "TargetModel Epoch: 236, Approx Train: 0.8206, Train: 0.8206, Test: 0.5873,marco: 0.5757,micro: 0.5873\n",
            "TargetModel Epoch: 237, Approx Train: 0.8206, Train: 0.8206, Test: 0.5873,marco: 0.5756,micro: 0.5873\n",
            "TargetModel Epoch: 238, Approx Train: 0.8206, Train: 0.8206, Test: 0.5873,marco: 0.5756,micro: 0.5873\n",
            "TargetModel Epoch: 239, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 240, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 241, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 242, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 243, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 244, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 245, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 246, Approx Train: 0.8206, Train: 0.8206, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 247, Approx Train: 0.8206, Train: 0.8222, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 248, Approx Train: 0.8222, Train: 0.8222, Test: 0.5889,marco: 0.5767,micro: 0.5889\n",
            "TargetModel Epoch: 249, Approx Train: 0.8222, Train: 0.8222, Test: 0.5873,marco: 0.5747,micro: 0.5873\n",
            "TargetModel Epoch: 250, Approx Train: 0.8222, Train: 0.8222, Test: 0.5873,marco: 0.5749,micro: 0.5873\n",
            "TargetModel Epoch: 251, Approx Train: 0.8222, Train: 0.8238, Test: 0.5873,marco: 0.5749,micro: 0.5873\n",
            "TargetModel Epoch: 252, Approx Train: 0.8238, Train: 0.8238, Test: 0.5873,marco: 0.5749,micro: 0.5873\n",
            "TargetModel Epoch: 253, Approx Train: 0.8238, Train: 0.8238, Test: 0.5873,marco: 0.5749,micro: 0.5873\n",
            "TargetModel Epoch: 254, Approx Train: 0.8238, Train: 0.8254, Test: 0.5873,marco: 0.5749,micro: 0.5873\n",
            "TargetModel Epoch: 255, Approx Train: 0.8254, Train: 0.8254, Test: 0.5889,marco: 0.5764,micro: 0.5889\n",
            "TargetModel Epoch: 256, Approx Train: 0.8254, Train: 0.8254, Test: 0.5905,marco: 0.5777,micro: 0.5905\n",
            "TargetModel Epoch: 257, Approx Train: 0.8254, Train: 0.8270, Test: 0.5905,marco: 0.5777,micro: 0.5905\n",
            "TargetModel Epoch: 258, Approx Train: 0.8270, Train: 0.8270, Test: 0.5905,marco: 0.5777,micro: 0.5905\n",
            "TargetModel Epoch: 259, Approx Train: 0.8270, Train: 0.8270, Test: 0.5889,marco: 0.5766,micro: 0.5889\n",
            "TargetModel Epoch: 260, Approx Train: 0.8270, Train: 0.8286, Test: 0.5889,marco: 0.5766,micro: 0.5889\n",
            "TargetModel Epoch: 261, Approx Train: 0.8286, Train: 0.8286, Test: 0.5889,marco: 0.5766,micro: 0.5889\n",
            "TargetModel Epoch: 262, Approx Train: 0.8286, Train: 0.8302, Test: 0.5905,marco: 0.5779,micro: 0.5905\n",
            "TargetModel Epoch: 263, Approx Train: 0.8302, Train: 0.8302, Test: 0.5905,marco: 0.5783,micro: 0.5905\n",
            "TargetModel Epoch: 264, Approx Train: 0.8302, Train: 0.8302, Test: 0.5905,marco: 0.5783,micro: 0.5905\n",
            "TargetModel Epoch: 265, Approx Train: 0.8302, Train: 0.8302, Test: 0.5905,marco: 0.5783,micro: 0.5905\n",
            "TargetModel Epoch: 266, Approx Train: 0.8302, Train: 0.8317, Test: 0.5921,marco: 0.5799,micro: 0.5921\n",
            "TargetModel Epoch: 267, Approx Train: 0.8317, Train: 0.8317, Test: 0.5921,marco: 0.5799,micro: 0.5921\n",
            "TargetModel Epoch: 268, Approx Train: 0.8317, Train: 0.8317, Test: 0.5921,marco: 0.5799,micro: 0.5921\n",
            "TargetModel Epoch: 269, Approx Train: 0.8317, Train: 0.8317, Test: 0.5921,marco: 0.5799,micro: 0.5921\n",
            "TargetModel Epoch: 270, Approx Train: 0.8317, Train: 0.8317, Test: 0.5905,marco: 0.5789,micro: 0.5905\n",
            "TargetModel Epoch: 271, Approx Train: 0.8317, Train: 0.8317, Test: 0.5905,marco: 0.5789,micro: 0.5905\n",
            "TargetModel Epoch: 272, Approx Train: 0.8317, Train: 0.8317, Test: 0.5905,marco: 0.5789,micro: 0.5905\n",
            "TargetModel Epoch: 273, Approx Train: 0.8317, Train: 0.8317, Test: 0.5905,marco: 0.5789,micro: 0.5905\n",
            "TargetModel Epoch: 274, Approx Train: 0.8317, Train: 0.8317, Test: 0.5905,marco: 0.5789,micro: 0.5905\n",
            "TargetModel Epoch: 275, Approx Train: 0.8317, Train: 0.8317, Test: 0.5905,marco: 0.5789,micro: 0.5905\n",
            "TargetModel Epoch: 276, Approx Train: 0.8317, Train: 0.8317, Test: 0.5921,marco: 0.5806,micro: 0.5921\n",
            "TargetModel Epoch: 277, Approx Train: 0.8317, Train: 0.8317, Test: 0.5921,marco: 0.5807,micro: 0.5921\n",
            "TargetModel Epoch: 278, Approx Train: 0.8317, Train: 0.8333, Test: 0.5937,marco: 0.5822,micro: 0.5937\n",
            "TargetModel Epoch: 279, Approx Train: 0.8333, Train: 0.8333, Test: 0.5937,marco: 0.5822,micro: 0.5937\n",
            "TargetModel Epoch: 280, Approx Train: 0.8333, Train: 0.8333, Test: 0.5937,marco: 0.5822,micro: 0.5937\n",
            "TargetModel Epoch: 281, Approx Train: 0.8333, Train: 0.8333, Test: 0.5937,marco: 0.5822,micro: 0.5937\n",
            "TargetModel Epoch: 282, Approx Train: 0.8333, Train: 0.8333, Test: 0.5937,marco: 0.5822,micro: 0.5937\n",
            "TargetModel Epoch: 283, Approx Train: 0.8333, Train: 0.8349, Test: 0.5937,marco: 0.5822,micro: 0.5937\n",
            "TargetModel Epoch: 284, Approx Train: 0.8349, Train: 0.8365, Test: 0.5937,marco: 0.5821,micro: 0.5937\n",
            "TargetModel Epoch: 285, Approx Train: 0.8365, Train: 0.8365, Test: 0.5937,marco: 0.5821,micro: 0.5937\n",
            "TargetModel Epoch: 286, Approx Train: 0.8365, Train: 0.8397, Test: 0.5952,marco: 0.5837,micro: 0.5952\n",
            "TargetModel Epoch: 287, Approx Train: 0.8397, Train: 0.8413, Test: 0.5952,marco: 0.5837,micro: 0.5952\n",
            "TargetModel Epoch: 288, Approx Train: 0.8413, Train: 0.8413, Test: 0.5952,marco: 0.5836,micro: 0.5952\n",
            "TargetModel Epoch: 289, Approx Train: 0.8413, Train: 0.8413, Test: 0.5968,marco: 0.5848,micro: 0.5968\n",
            "TargetModel Epoch: 290, Approx Train: 0.8413, Train: 0.8413, Test: 0.5968,marco: 0.5848,micro: 0.5968\n",
            "TargetModel Epoch: 291, Approx Train: 0.8413, Train: 0.8413, Test: 0.5984,marco: 0.5867,micro: 0.5984\n",
            "TargetModel Epoch: 292, Approx Train: 0.8413, Train: 0.8429, Test: 0.5968,marco: 0.5852,micro: 0.5968\n",
            "TargetModel Epoch: 293, Approx Train: 0.8429, Train: 0.8429, Test: 0.5968,marco: 0.5852,micro: 0.5968\n",
            "TargetModel Epoch: 294, Approx Train: 0.8429, Train: 0.8429, Test: 0.5968,marco: 0.5852,micro: 0.5968\n",
            "TargetModel Epoch: 295, Approx Train: 0.8429, Train: 0.8429, Test: 0.5968,marco: 0.5852,micro: 0.5968\n",
            "TargetModel Epoch: 296, Approx Train: 0.8429, Train: 0.8429, Test: 0.5952,marco: 0.5839,micro: 0.5952\n",
            "TargetModel Epoch: 297, Approx Train: 0.8429, Train: 0.8429, Test: 0.5952,marco: 0.5839,micro: 0.5952\n",
            "TargetModel Epoch: 298, Approx Train: 0.8429, Train: 0.8429, Test: 0.5952,marco: 0.5839,micro: 0.5952\n",
            "TargetModel Epoch: 299, Approx Train: 0.8429, Train: 0.8429, Test: 0.5952,marco: 0.5839,micro: 0.5952\n",
            "TargetModel Epoch: 300, Approx Train: 0.8429, Train: 0.8429, Test: 0.5952,marco: 0.5839,micro: 0.5952\n",
            "\n",
            "=========================================================End Target Train ==============================\n",
            "ShadowModel Epoch: 001, Approx Train: 0.1397, Train: 0.1460, Test: 0.1889,marco: 0.1282,micro: 0.1889\n",
            "ShadowModel Epoch: 002, Approx Train: 0.1460, Train: 0.1556, Test: 0.1921,marco: 0.1301,micro: 0.1921\n",
            "ShadowModel Epoch: 003, Approx Train: 0.1556, Train: 0.1698, Test: 0.1984,marco: 0.1418,micro: 0.1984\n",
            "ShadowModel Epoch: 004, Approx Train: 0.1698, Train: 0.1778, Test: 0.2000,marco: 0.1452,micro: 0.2000\n",
            "ShadowModel Epoch: 005, Approx Train: 0.1778, Train: 0.1873, Test: 0.2111,marco: 0.1520,micro: 0.2111\n",
            "ShadowModel Epoch: 006, Approx Train: 0.1873, Train: 0.2048, Test: 0.2111,marco: 0.1529,micro: 0.2111\n",
            "ShadowModel Epoch: 007, Approx Train: 0.2048, Train: 0.2302, Test: 0.2175,marco: 0.1611,micro: 0.2175\n",
            "ShadowModel Epoch: 008, Approx Train: 0.2302, Train: 0.2460, Test: 0.2190,marco: 0.1620,micro: 0.2190\n",
            "ShadowModel Epoch: 009, Approx Train: 0.2460, Train: 0.2635, Test: 0.2270,marco: 0.1693,micro: 0.2270\n",
            "ShadowModel Epoch: 010, Approx Train: 0.2635, Train: 0.2905, Test: 0.2333,marco: 0.1757,micro: 0.2333\n",
            "ShadowModel Epoch: 011, Approx Train: 0.2905, Train: 0.3048, Test: 0.2429,marco: 0.1936,micro: 0.2429\n",
            "ShadowModel Epoch: 012, Approx Train: 0.3048, Train: 0.3222, Test: 0.2524,marco: 0.2024,micro: 0.2524\n",
            "ShadowModel Epoch: 013, Approx Train: 0.3222, Train: 0.3429, Test: 0.2587,marco: 0.2076,micro: 0.2587\n",
            "ShadowModel Epoch: 014, Approx Train: 0.3429, Train: 0.3540, Test: 0.2587,marco: 0.2071,micro: 0.2587\n",
            "ShadowModel Epoch: 015, Approx Train: 0.3540, Train: 0.3714, Test: 0.2603,marco: 0.2097,micro: 0.2603\n",
            "ShadowModel Epoch: 016, Approx Train: 0.3714, Train: 0.3873, Test: 0.2635,marco: 0.2184,micro: 0.2635\n",
            "ShadowModel Epoch: 017, Approx Train: 0.3873, Train: 0.4111, Test: 0.2746,marco: 0.2294,micro: 0.2746\n",
            "ShadowModel Epoch: 018, Approx Train: 0.4111, Train: 0.4254, Test: 0.2810,marco: 0.2362,micro: 0.2810\n",
            "ShadowModel Epoch: 019, Approx Train: 0.4254, Train: 0.4508, Test: 0.2873,marco: 0.2473,micro: 0.2873\n",
            "ShadowModel Epoch: 020, Approx Train: 0.4508, Train: 0.4635, Test: 0.2905,marco: 0.2519,micro: 0.2905\n",
            "ShadowModel Epoch: 021, Approx Train: 0.4635, Train: 0.4778, Test: 0.2968,marco: 0.2592,micro: 0.2968\n",
            "ShadowModel Epoch: 022, Approx Train: 0.4778, Train: 0.4952, Test: 0.3063,marco: 0.2710,micro: 0.3063\n",
            "ShadowModel Epoch: 023, Approx Train: 0.4952, Train: 0.5095, Test: 0.3175,marco: 0.2830,micro: 0.3175\n",
            "ShadowModel Epoch: 024, Approx Train: 0.5095, Train: 0.5286, Test: 0.3222,marco: 0.2900,micro: 0.3222\n",
            "ShadowModel Epoch: 025, Approx Train: 0.5286, Train: 0.5460, Test: 0.3349,marco: 0.3024,micro: 0.3349\n",
            "ShadowModel Epoch: 026, Approx Train: 0.5460, Train: 0.5540, Test: 0.3444,marco: 0.3146,micro: 0.3444\n",
            "ShadowModel Epoch: 027, Approx Train: 0.5540, Train: 0.5762, Test: 0.3460,marco: 0.3189,micro: 0.3460\n",
            "ShadowModel Epoch: 028, Approx Train: 0.5762, Train: 0.5873, Test: 0.3460,marco: 0.3188,micro: 0.3460\n",
            "ShadowModel Epoch: 029, Approx Train: 0.5873, Train: 0.5952, Test: 0.3556,marco: 0.3310,micro: 0.3556\n",
            "ShadowModel Epoch: 030, Approx Train: 0.5952, Train: 0.6159, Test: 0.3571,marco: 0.3317,micro: 0.3571\n",
            "ShadowModel Epoch: 031, Approx Train: 0.6159, Train: 0.6206, Test: 0.3587,marco: 0.3339,micro: 0.3587\n",
            "ShadowModel Epoch: 032, Approx Train: 0.6206, Train: 0.6254, Test: 0.3635,marco: 0.3411,micro: 0.3635\n",
            "ShadowModel Epoch: 033, Approx Train: 0.6254, Train: 0.6333, Test: 0.3698,marco: 0.3499,micro: 0.3698\n",
            "ShadowModel Epoch: 034, Approx Train: 0.6333, Train: 0.6349, Test: 0.3746,marco: 0.3528,micro: 0.3746\n",
            "ShadowModel Epoch: 035, Approx Train: 0.6349, Train: 0.6508, Test: 0.3810,marco: 0.3598,micro: 0.3810\n",
            "ShadowModel Epoch: 036, Approx Train: 0.6508, Train: 0.6619, Test: 0.3825,marco: 0.3606,micro: 0.3825\n",
            "ShadowModel Epoch: 037, Approx Train: 0.6619, Train: 0.6635, Test: 0.3841,marco: 0.3632,micro: 0.3841\n",
            "ShadowModel Epoch: 038, Approx Train: 0.6635, Train: 0.6667, Test: 0.3873,marco: 0.3677,micro: 0.3873\n",
            "ShadowModel Epoch: 039, Approx Train: 0.6667, Train: 0.6698, Test: 0.3905,marco: 0.3701,micro: 0.3905\n",
            "ShadowModel Epoch: 040, Approx Train: 0.6698, Train: 0.6762, Test: 0.3905,marco: 0.3704,micro: 0.3905\n",
            "ShadowModel Epoch: 041, Approx Train: 0.6762, Train: 0.6825, Test: 0.4000,marco: 0.3813,micro: 0.4000\n",
            "ShadowModel Epoch: 042, Approx Train: 0.6825, Train: 0.6857, Test: 0.4016,marco: 0.3825,micro: 0.4016\n",
            "ShadowModel Epoch: 043, Approx Train: 0.6857, Train: 0.6921, Test: 0.4063,marco: 0.3900,micro: 0.4063\n",
            "ShadowModel Epoch: 044, Approx Train: 0.6921, Train: 0.6984, Test: 0.4063,marco: 0.3904,micro: 0.4063\n",
            "ShadowModel Epoch: 045, Approx Train: 0.6984, Train: 0.7032, Test: 0.4079,marco: 0.3911,micro: 0.4079\n",
            "ShadowModel Epoch: 046, Approx Train: 0.7032, Train: 0.7032, Test: 0.4095,marco: 0.3947,micro: 0.4095\n",
            "ShadowModel Epoch: 047, Approx Train: 0.7032, Train: 0.7063, Test: 0.4127,marco: 0.3981,micro: 0.4127\n",
            "ShadowModel Epoch: 048, Approx Train: 0.7063, Train: 0.7079, Test: 0.4175,marco: 0.4026,micro: 0.4175\n",
            "ShadowModel Epoch: 049, Approx Train: 0.7079, Train: 0.7127, Test: 0.4190,marco: 0.4052,micro: 0.4190\n",
            "ShadowModel Epoch: 050, Approx Train: 0.7127, Train: 0.7127, Test: 0.4222,marco: 0.4089,micro: 0.4222\n",
            "ShadowModel Epoch: 051, Approx Train: 0.7127, Train: 0.7127, Test: 0.4254,marco: 0.4125,micro: 0.4254\n",
            "ShadowModel Epoch: 052, Approx Train: 0.7127, Train: 0.7127, Test: 0.4254,marco: 0.4128,micro: 0.4254\n",
            "ShadowModel Epoch: 053, Approx Train: 0.7127, Train: 0.7127, Test: 0.4286,marco: 0.4150,micro: 0.4286\n",
            "ShadowModel Epoch: 054, Approx Train: 0.7127, Train: 0.7127, Test: 0.4349,marco: 0.4203,micro: 0.4349\n",
            "ShadowModel Epoch: 055, Approx Train: 0.7127, Train: 0.7159, Test: 0.4349,marco: 0.4208,micro: 0.4349\n",
            "ShadowModel Epoch: 056, Approx Train: 0.7159, Train: 0.7206, Test: 0.4413,marco: 0.4261,micro: 0.4413\n",
            "ShadowModel Epoch: 057, Approx Train: 0.7206, Train: 0.7254, Test: 0.4460,marco: 0.4326,micro: 0.4460\n",
            "ShadowModel Epoch: 058, Approx Train: 0.7254, Train: 0.7238, Test: 0.4492,marco: 0.4367,micro: 0.4492\n",
            "ShadowModel Epoch: 059, Approx Train: 0.7238, Train: 0.7254, Test: 0.4492,marco: 0.4357,micro: 0.4492\n",
            "ShadowModel Epoch: 060, Approx Train: 0.7254, Train: 0.7286, Test: 0.4492,marco: 0.4353,micro: 0.4492\n",
            "ShadowModel Epoch: 061, Approx Train: 0.7286, Train: 0.7317, Test: 0.4524,marco: 0.4378,micro: 0.4524\n",
            "ShadowModel Epoch: 062, Approx Train: 0.7317, Train: 0.7317, Test: 0.4556,marco: 0.4414,micro: 0.4556\n",
            "ShadowModel Epoch: 063, Approx Train: 0.7317, Train: 0.7333, Test: 0.4571,marco: 0.4429,micro: 0.4571\n",
            "ShadowModel Epoch: 064, Approx Train: 0.7333, Train: 0.7317, Test: 0.4587,marco: 0.4445,micro: 0.4587\n",
            "ShadowModel Epoch: 065, Approx Train: 0.7317, Train: 0.7333, Test: 0.4619,marco: 0.4494,micro: 0.4619\n",
            "ShadowModel Epoch: 066, Approx Train: 0.7333, Train: 0.7333, Test: 0.4698,marco: 0.4587,micro: 0.4698\n",
            "ShadowModel Epoch: 067, Approx Train: 0.7333, Train: 0.7333, Test: 0.4730,marco: 0.4629,micro: 0.4730\n",
            "ShadowModel Epoch: 068, Approx Train: 0.7333, Train: 0.7333, Test: 0.4714,marco: 0.4600,micro: 0.4714\n",
            "ShadowModel Epoch: 069, Approx Train: 0.7333, Train: 0.7349, Test: 0.4794,marco: 0.4692,micro: 0.4794\n",
            "ShadowModel Epoch: 070, Approx Train: 0.7349, Train: 0.7349, Test: 0.4778,marco: 0.4673,micro: 0.4778\n",
            "ShadowModel Epoch: 071, Approx Train: 0.7349, Train: 0.7365, Test: 0.4778,marco: 0.4670,micro: 0.4778\n",
            "ShadowModel Epoch: 072, Approx Train: 0.7365, Train: 0.7381, Test: 0.4778,marco: 0.4670,micro: 0.4778\n",
            "ShadowModel Epoch: 073, Approx Train: 0.7381, Train: 0.7381, Test: 0.4810,marco: 0.4688,micro: 0.4810\n",
            "ShadowModel Epoch: 074, Approx Train: 0.7381, Train: 0.7381, Test: 0.4857,marco: 0.4734,micro: 0.4857\n",
            "ShadowModel Epoch: 075, Approx Train: 0.7381, Train: 0.7381, Test: 0.4873,marco: 0.4754,micro: 0.4873\n",
            "ShadowModel Epoch: 076, Approx Train: 0.7381, Train: 0.7381, Test: 0.4921,marco: 0.4799,micro: 0.4921\n",
            "ShadowModel Epoch: 077, Approx Train: 0.7381, Train: 0.7397, Test: 0.4952,marco: 0.4829,micro: 0.4952\n",
            "ShadowModel Epoch: 078, Approx Train: 0.7397, Train: 0.7413, Test: 0.4984,marco: 0.4856,micro: 0.4984\n",
            "ShadowModel Epoch: 079, Approx Train: 0.7413, Train: 0.7413, Test: 0.5000,marco: 0.4877,micro: 0.5000\n",
            "ShadowModel Epoch: 080, Approx Train: 0.7413, Train: 0.7413, Test: 0.5063,marco: 0.4964,micro: 0.5063\n",
            "ShadowModel Epoch: 081, Approx Train: 0.7413, Train: 0.7413, Test: 0.5127,marco: 0.5033,micro: 0.5127\n",
            "ShadowModel Epoch: 082, Approx Train: 0.7413, Train: 0.7413, Test: 0.5175,marco: 0.5085,micro: 0.5175\n",
            "ShadowModel Epoch: 083, Approx Train: 0.7413, Train: 0.7397, Test: 0.5206,marco: 0.5115,micro: 0.5206\n",
            "ShadowModel Epoch: 084, Approx Train: 0.7397, Train: 0.7397, Test: 0.5206,marco: 0.5116,micro: 0.5206\n",
            "ShadowModel Epoch: 085, Approx Train: 0.7397, Train: 0.7429, Test: 0.5206,marco: 0.5116,micro: 0.5206\n",
            "ShadowModel Epoch: 086, Approx Train: 0.7429, Train: 0.7429, Test: 0.5206,marco: 0.5122,micro: 0.5206\n",
            "ShadowModel Epoch: 087, Approx Train: 0.7429, Train: 0.7444, Test: 0.5206,marco: 0.5122,micro: 0.5206\n",
            "ShadowModel Epoch: 088, Approx Train: 0.7444, Train: 0.7444, Test: 0.5190,marco: 0.5106,micro: 0.5190\n",
            "ShadowModel Epoch: 089, Approx Train: 0.7444, Train: 0.7460, Test: 0.5222,marco: 0.5126,micro: 0.5222\n",
            "ShadowModel Epoch: 090, Approx Train: 0.7460, Train: 0.7476, Test: 0.5238,marco: 0.5150,micro: 0.5238\n",
            "ShadowModel Epoch: 091, Approx Train: 0.7476, Train: 0.7476, Test: 0.5254,marco: 0.5160,micro: 0.5254\n",
            "ShadowModel Epoch: 092, Approx Train: 0.7476, Train: 0.7492, Test: 0.5254,marco: 0.5160,micro: 0.5254\n",
            "ShadowModel Epoch: 093, Approx Train: 0.7492, Train: 0.7492, Test: 0.5270,marco: 0.5173,micro: 0.5270\n",
            "ShadowModel Epoch: 094, Approx Train: 0.7492, Train: 0.7476, Test: 0.5286,marco: 0.5182,micro: 0.5286\n",
            "ShadowModel Epoch: 095, Approx Train: 0.7476, Train: 0.7476, Test: 0.5317,marco: 0.5229,micro: 0.5317\n",
            "ShadowModel Epoch: 096, Approx Train: 0.7476, Train: 0.7460, Test: 0.5317,marco: 0.5229,micro: 0.5317\n",
            "ShadowModel Epoch: 097, Approx Train: 0.7460, Train: 0.7460, Test: 0.5349,marco: 0.5256,micro: 0.5349\n",
            "ShadowModel Epoch: 098, Approx Train: 0.7460, Train: 0.7460, Test: 0.5349,marco: 0.5256,micro: 0.5349\n",
            "ShadowModel Epoch: 099, Approx Train: 0.7460, Train: 0.7476, Test: 0.5365,marco: 0.5273,micro: 0.5365\n",
            "ShadowModel Epoch: 100, Approx Train: 0.7476, Train: 0.7460, Test: 0.5397,marco: 0.5311,micro: 0.5397\n",
            "ShadowModel Epoch: 101, Approx Train: 0.7460, Train: 0.7444, Test: 0.5413,marco: 0.5337,micro: 0.5413\n",
            "ShadowModel Epoch: 102, Approx Train: 0.7444, Train: 0.7444, Test: 0.5413,marco: 0.5337,micro: 0.5413\n",
            "ShadowModel Epoch: 103, Approx Train: 0.7444, Train: 0.7444, Test: 0.5429,marco: 0.5354,micro: 0.5429\n",
            "ShadowModel Epoch: 104, Approx Train: 0.7444, Train: 0.7444, Test: 0.5429,marco: 0.5351,micro: 0.5429\n",
            "ShadowModel Epoch: 105, Approx Train: 0.7444, Train: 0.7460, Test: 0.5413,marco: 0.5340,micro: 0.5413\n",
            "ShadowModel Epoch: 106, Approx Train: 0.7460, Train: 0.7476, Test: 0.5476,marco: 0.5396,micro: 0.5476\n",
            "ShadowModel Epoch: 107, Approx Train: 0.7476, Train: 0.7476, Test: 0.5492,marco: 0.5408,micro: 0.5492\n",
            "ShadowModel Epoch: 108, Approx Train: 0.7476, Train: 0.7476, Test: 0.5508,marco: 0.5419,micro: 0.5508\n",
            "ShadowModel Epoch: 109, Approx Train: 0.7476, Train: 0.7476, Test: 0.5508,marco: 0.5411,micro: 0.5508\n",
            "ShadowModel Epoch: 110, Approx Train: 0.7476, Train: 0.7476, Test: 0.5524,marco: 0.5423,micro: 0.5524\n",
            "ShadowModel Epoch: 111, Approx Train: 0.7476, Train: 0.7476, Test: 0.5540,marco: 0.5438,micro: 0.5540\n",
            "ShadowModel Epoch: 112, Approx Train: 0.7476, Train: 0.7460, Test: 0.5524,marco: 0.5416,micro: 0.5524\n",
            "ShadowModel Epoch: 113, Approx Train: 0.7460, Train: 0.7460, Test: 0.5540,marco: 0.5426,micro: 0.5540\n",
            "ShadowModel Epoch: 114, Approx Train: 0.7460, Train: 0.7476, Test: 0.5571,marco: 0.5472,micro: 0.5571\n",
            "ShadowModel Epoch: 115, Approx Train: 0.7476, Train: 0.7476, Test: 0.5571,marco: 0.5470,micro: 0.5571\n",
            "ShadowModel Epoch: 116, Approx Train: 0.7476, Train: 0.7476, Test: 0.5571,marco: 0.5474,micro: 0.5571\n",
            "ShadowModel Epoch: 117, Approx Train: 0.7476, Train: 0.7476, Test: 0.5587,marco: 0.5499,micro: 0.5587\n",
            "ShadowModel Epoch: 118, Approx Train: 0.7476, Train: 0.7476, Test: 0.5603,marco: 0.5524,micro: 0.5603\n",
            "ShadowModel Epoch: 119, Approx Train: 0.7476, Train: 0.7492, Test: 0.5587,marco: 0.5515,micro: 0.5587\n",
            "ShadowModel Epoch: 120, Approx Train: 0.7492, Train: 0.7492, Test: 0.5587,marco: 0.5515,micro: 0.5587\n",
            "ShadowModel Epoch: 121, Approx Train: 0.7492, Train: 0.7492, Test: 0.5587,marco: 0.5515,micro: 0.5587\n",
            "ShadowModel Epoch: 122, Approx Train: 0.7492, Train: 0.7492, Test: 0.5587,marco: 0.5515,micro: 0.5587\n",
            "ShadowModel Epoch: 123, Approx Train: 0.7492, Train: 0.7492, Test: 0.5587,marco: 0.5515,micro: 0.5587\n",
            "ShadowModel Epoch: 124, Approx Train: 0.7492, Train: 0.7492, Test: 0.5603,marco: 0.5525,micro: 0.5603\n",
            "ShadowModel Epoch: 125, Approx Train: 0.7492, Train: 0.7508, Test: 0.5635,marco: 0.5565,micro: 0.5635\n",
            "ShadowModel Epoch: 126, Approx Train: 0.7508, Train: 0.7508, Test: 0.5635,marco: 0.5559,micro: 0.5635\n",
            "ShadowModel Epoch: 127, Approx Train: 0.7508, Train: 0.7508, Test: 0.5651,marco: 0.5577,micro: 0.5651\n",
            "ShadowModel Epoch: 128, Approx Train: 0.7508, Train: 0.7508, Test: 0.5667,marco: 0.5596,micro: 0.5667\n",
            "ShadowModel Epoch: 129, Approx Train: 0.7508, Train: 0.7508, Test: 0.5651,marco: 0.5587,micro: 0.5651\n",
            "ShadowModel Epoch: 130, Approx Train: 0.7508, Train: 0.7508, Test: 0.5651,marco: 0.5589,micro: 0.5651\n",
            "ShadowModel Epoch: 131, Approx Train: 0.7508, Train: 0.7508, Test: 0.5667,marco: 0.5599,micro: 0.5667\n",
            "ShadowModel Epoch: 132, Approx Train: 0.7508, Train: 0.7508, Test: 0.5683,marco: 0.5613,micro: 0.5683\n",
            "ShadowModel Epoch: 133, Approx Train: 0.7508, Train: 0.7508, Test: 0.5683,marco: 0.5611,micro: 0.5683\n",
            "ShadowModel Epoch: 134, Approx Train: 0.7508, Train: 0.7508, Test: 0.5698,marco: 0.5624,micro: 0.5698\n",
            "ShadowModel Epoch: 135, Approx Train: 0.7508, Train: 0.7508, Test: 0.5698,marco: 0.5625,micro: 0.5698\n",
            "ShadowModel Epoch: 136, Approx Train: 0.7508, Train: 0.7508, Test: 0.5698,marco: 0.5630,micro: 0.5698\n",
            "ShadowModel Epoch: 137, Approx Train: 0.7508, Train: 0.7508, Test: 0.5698,marco: 0.5630,micro: 0.5698\n",
            "ShadowModel Epoch: 138, Approx Train: 0.7508, Train: 0.7524, Test: 0.5714,marco: 0.5641,micro: 0.5714\n",
            "ShadowModel Epoch: 139, Approx Train: 0.7524, Train: 0.7524, Test: 0.5714,marco: 0.5644,micro: 0.5714\n",
            "ShadowModel Epoch: 140, Approx Train: 0.7524, Train: 0.7556, Test: 0.5698,marco: 0.5633,micro: 0.5698\n",
            "ShadowModel Epoch: 141, Approx Train: 0.7556, Train: 0.7556, Test: 0.5698,marco: 0.5633,micro: 0.5698\n",
            "ShadowModel Epoch: 142, Approx Train: 0.7556, Train: 0.7571, Test: 0.5714,marco: 0.5647,micro: 0.5714\n",
            "ShadowModel Epoch: 143, Approx Train: 0.7571, Train: 0.7571, Test: 0.5746,marco: 0.5670,micro: 0.5746\n",
            "ShadowModel Epoch: 144, Approx Train: 0.7571, Train: 0.7571, Test: 0.5746,marco: 0.5675,micro: 0.5746\n",
            "ShadowModel Epoch: 145, Approx Train: 0.7571, Train: 0.7571, Test: 0.5746,marco: 0.5675,micro: 0.5746\n",
            "ShadowModel Epoch: 146, Approx Train: 0.7571, Train: 0.7571, Test: 0.5746,marco: 0.5675,micro: 0.5746\n",
            "ShadowModel Epoch: 147, Approx Train: 0.7571, Train: 0.7571, Test: 0.5746,marco: 0.5675,micro: 0.5746\n",
            "ShadowModel Epoch: 148, Approx Train: 0.7571, Train: 0.7571, Test: 0.5746,marco: 0.5689,micro: 0.5746\n",
            "ShadowModel Epoch: 149, Approx Train: 0.7571, Train: 0.7571, Test: 0.5762,marco: 0.5714,micro: 0.5762\n",
            "ShadowModel Epoch: 150, Approx Train: 0.7571, Train: 0.7571, Test: 0.5778,marco: 0.5727,micro: 0.5778\n",
            "ShadowModel Epoch: 151, Approx Train: 0.7571, Train: 0.7571, Test: 0.5778,marco: 0.5727,micro: 0.5778\n",
            "ShadowModel Epoch: 152, Approx Train: 0.7571, Train: 0.7571, Test: 0.5778,marco: 0.5727,micro: 0.5778\n",
            "ShadowModel Epoch: 153, Approx Train: 0.7571, Train: 0.7571, Test: 0.5778,marco: 0.5727,micro: 0.5778\n",
            "ShadowModel Epoch: 154, Approx Train: 0.7571, Train: 0.7571, Test: 0.5778,marco: 0.5727,micro: 0.5778\n",
            "ShadowModel Epoch: 155, Approx Train: 0.7571, Train: 0.7571, Test: 0.5778,marco: 0.5732,micro: 0.5778\n",
            "ShadowModel Epoch: 156, Approx Train: 0.7571, Train: 0.7571, Test: 0.5810,marco: 0.5752,micro: 0.5810\n",
            "ShadowModel Epoch: 157, Approx Train: 0.7571, Train: 0.7571, Test: 0.5841,marco: 0.5774,micro: 0.5841\n",
            "ShadowModel Epoch: 158, Approx Train: 0.7571, Train: 0.7571, Test: 0.5857,marco: 0.5787,micro: 0.5857\n",
            "ShadowModel Epoch: 159, Approx Train: 0.7571, Train: 0.7587, Test: 0.5857,marco: 0.5787,micro: 0.5857\n",
            "ShadowModel Epoch: 160, Approx Train: 0.7587, Train: 0.7603, Test: 0.5857,marco: 0.5791,micro: 0.5857\n",
            "ShadowModel Epoch: 161, Approx Train: 0.7603, Train: 0.7603, Test: 0.5857,marco: 0.5791,micro: 0.5857\n",
            "ShadowModel Epoch: 162, Approx Train: 0.7603, Train: 0.7619, Test: 0.5857,marco: 0.5791,micro: 0.5857\n",
            "ShadowModel Epoch: 163, Approx Train: 0.7619, Train: 0.7635, Test: 0.5889,marco: 0.5823,micro: 0.5889\n",
            "ShadowModel Epoch: 164, Approx Train: 0.7635, Train: 0.7635, Test: 0.5889,marco: 0.5823,micro: 0.5889\n",
            "ShadowModel Epoch: 165, Approx Train: 0.7635, Train: 0.7635, Test: 0.5889,marco: 0.5823,micro: 0.5889\n",
            "ShadowModel Epoch: 166, Approx Train: 0.7635, Train: 0.7635, Test: 0.5889,marco: 0.5823,micro: 0.5889\n",
            "ShadowModel Epoch: 167, Approx Train: 0.7635, Train: 0.7635, Test: 0.5889,marco: 0.5824,micro: 0.5889\n",
            "ShadowModel Epoch: 168, Approx Train: 0.7635, Train: 0.7635, Test: 0.5889,marco: 0.5824,micro: 0.5889\n",
            "ShadowModel Epoch: 169, Approx Train: 0.7635, Train: 0.7635, Test: 0.5889,marco: 0.5824,micro: 0.5889\n",
            "ShadowModel Epoch: 170, Approx Train: 0.7635, Train: 0.7635, Test: 0.5889,marco: 0.5824,micro: 0.5889\n",
            "ShadowModel Epoch: 171, Approx Train: 0.7635, Train: 0.7651, Test: 0.5889,marco: 0.5824,micro: 0.5889\n",
            "ShadowModel Epoch: 172, Approx Train: 0.7651, Train: 0.7651, Test: 0.5889,marco: 0.5824,micro: 0.5889\n",
            "ShadowModel Epoch: 173, Approx Train: 0.7651, Train: 0.7651, Test: 0.5889,marco: 0.5830,micro: 0.5889\n",
            "ShadowModel Epoch: 174, Approx Train: 0.7651, Train: 0.7651, Test: 0.5905,marco: 0.5848,micro: 0.5905\n",
            "ShadowModel Epoch: 175, Approx Train: 0.7651, Train: 0.7651, Test: 0.5921,marco: 0.5874,micro: 0.5921\n",
            "ShadowModel Epoch: 176, Approx Train: 0.7651, Train: 0.7651, Test: 0.5921,marco: 0.5871,micro: 0.5921\n",
            "ShadowModel Epoch: 177, Approx Train: 0.7651, Train: 0.7651, Test: 0.5937,marco: 0.5890,micro: 0.5937\n",
            "ShadowModel Epoch: 178, Approx Train: 0.7651, Train: 0.7651, Test: 0.5937,marco: 0.5890,micro: 0.5937\n",
            "ShadowModel Epoch: 179, Approx Train: 0.7651, Train: 0.7651, Test: 0.5937,marco: 0.5889,micro: 0.5937\n",
            "ShadowModel Epoch: 180, Approx Train: 0.7651, Train: 0.7683, Test: 0.5937,marco: 0.5889,micro: 0.5937\n",
            "ShadowModel Epoch: 181, Approx Train: 0.7683, Train: 0.7683, Test: 0.5937,marco: 0.5889,micro: 0.5937\n",
            "ShadowModel Epoch: 182, Approx Train: 0.7683, Train: 0.7683, Test: 0.5937,marco: 0.5889,micro: 0.5937\n",
            "ShadowModel Epoch: 183, Approx Train: 0.7683, Train: 0.7683, Test: 0.5937,marco: 0.5889,micro: 0.5937\n",
            "ShadowModel Epoch: 184, Approx Train: 0.7683, Train: 0.7683, Test: 0.5937,marco: 0.5889,micro: 0.5937\n",
            "ShadowModel Epoch: 185, Approx Train: 0.7683, Train: 0.7698, Test: 0.5937,marco: 0.5889,micro: 0.5937\n",
            "ShadowModel Epoch: 186, Approx Train: 0.7698, Train: 0.7698, Test: 0.5937,marco: 0.5891,micro: 0.5937\n",
            "ShadowModel Epoch: 187, Approx Train: 0.7698, Train: 0.7698, Test: 0.5937,marco: 0.5891,micro: 0.5937\n",
            "ShadowModel Epoch: 188, Approx Train: 0.7698, Train: 0.7698, Test: 0.5937,marco: 0.5891,micro: 0.5937\n",
            "ShadowModel Epoch: 189, Approx Train: 0.7698, Train: 0.7698, Test: 0.5937,marco: 0.5891,micro: 0.5937\n",
            "ShadowModel Epoch: 190, Approx Train: 0.7698, Train: 0.7714, Test: 0.5952,marco: 0.5905,micro: 0.5952\n",
            "ShadowModel Epoch: 191, Approx Train: 0.7714, Train: 0.7714, Test: 0.5952,marco: 0.5905,micro: 0.5952\n",
            "ShadowModel Epoch: 192, Approx Train: 0.7714, Train: 0.7714, Test: 0.5921,marco: 0.5871,micro: 0.5921\n",
            "ShadowModel Epoch: 193, Approx Train: 0.7714, Train: 0.7714, Test: 0.5937,marco: 0.5883,micro: 0.5937\n",
            "ShadowModel Epoch: 194, Approx Train: 0.7714, Train: 0.7730, Test: 0.5937,marco: 0.5883,micro: 0.5937\n",
            "ShadowModel Epoch: 195, Approx Train: 0.7730, Train: 0.7730, Test: 0.5937,marco: 0.5883,micro: 0.5937\n",
            "ShadowModel Epoch: 196, Approx Train: 0.7730, Train: 0.7730, Test: 0.5937,marco: 0.5883,micro: 0.5937\n",
            "ShadowModel Epoch: 197, Approx Train: 0.7730, Train: 0.7730, Test: 0.5937,marco: 0.5883,micro: 0.5937\n",
            "ShadowModel Epoch: 198, Approx Train: 0.7730, Train: 0.7746, Test: 0.5952,marco: 0.5902,micro: 0.5952\n",
            "ShadowModel Epoch: 199, Approx Train: 0.7746, Train: 0.7746, Test: 0.5952,marco: 0.5899,micro: 0.5952\n",
            "ShadowModel Epoch: 200, Approx Train: 0.7746, Train: 0.7730, Test: 0.5952,marco: 0.5899,micro: 0.5952\n",
            "ShadowModel Epoch: 201, Approx Train: 0.7730, Train: 0.7730, Test: 0.5952,marco: 0.5899,micro: 0.5952\n",
            "ShadowModel Epoch: 202, Approx Train: 0.7730, Train: 0.7730, Test: 0.5952,marco: 0.5905,micro: 0.5952\n",
            "ShadowModel Epoch: 203, Approx Train: 0.7730, Train: 0.7730, Test: 0.5952,marco: 0.5905,micro: 0.5952\n",
            "ShadowModel Epoch: 204, Approx Train: 0.7730, Train: 0.7730, Test: 0.5952,marco: 0.5905,micro: 0.5952\n",
            "ShadowModel Epoch: 205, Approx Train: 0.7730, Train: 0.7730, Test: 0.5968,marco: 0.5918,micro: 0.5968\n",
            "ShadowModel Epoch: 206, Approx Train: 0.7730, Train: 0.7730, Test: 0.5968,marco: 0.5918,micro: 0.5968\n",
            "ShadowModel Epoch: 207, Approx Train: 0.7730, Train: 0.7730, Test: 0.5968,marco: 0.5920,micro: 0.5968\n",
            "ShadowModel Epoch: 208, Approx Train: 0.7730, Train: 0.7730, Test: 0.5968,marco: 0.5920,micro: 0.5968\n",
            "ShadowModel Epoch: 209, Approx Train: 0.7730, Train: 0.7730, Test: 0.5968,marco: 0.5920,micro: 0.5968\n",
            "ShadowModel Epoch: 210, Approx Train: 0.7730, Train: 0.7762, Test: 0.5984,marco: 0.5944,micro: 0.5984\n",
            "ShadowModel Epoch: 211, Approx Train: 0.7762, Train: 0.7762, Test: 0.5984,marco: 0.5944,micro: 0.5984\n",
            "ShadowModel Epoch: 212, Approx Train: 0.7762, Train: 0.7762, Test: 0.5984,marco: 0.5944,micro: 0.5984\n",
            "ShadowModel Epoch: 213, Approx Train: 0.7762, Train: 0.7778, Test: 0.5984,marco: 0.5944,micro: 0.5984\n",
            "ShadowModel Epoch: 214, Approx Train: 0.7778, Train: 0.7778, Test: 0.5984,marco: 0.5944,micro: 0.5984\n",
            "ShadowModel Epoch: 215, Approx Train: 0.7778, Train: 0.7794, Test: 0.5984,marco: 0.5944,micro: 0.5984\n",
            "ShadowModel Epoch: 216, Approx Train: 0.7794, Train: 0.7794, Test: 0.6000,marco: 0.5955,micro: 0.6000\n",
            "ShadowModel Epoch: 217, Approx Train: 0.7794, Train: 0.7794, Test: 0.6000,marco: 0.5955,micro: 0.6000\n",
            "ShadowModel Epoch: 218, Approx Train: 0.7794, Train: 0.7794, Test: 0.6000,marco: 0.5954,micro: 0.6000\n",
            "ShadowModel Epoch: 219, Approx Train: 0.7794, Train: 0.7794, Test: 0.5984,marco: 0.5934,micro: 0.5984\n",
            "ShadowModel Epoch: 220, Approx Train: 0.7794, Train: 0.7794, Test: 0.6000,marco: 0.5954,micro: 0.6000\n",
            "ShadowModel Epoch: 221, Approx Train: 0.7794, Train: 0.7794, Test: 0.6000,marco: 0.5954,micro: 0.6000\n",
            "ShadowModel Epoch: 222, Approx Train: 0.7794, Train: 0.7794, Test: 0.6000,marco: 0.5954,micro: 0.6000\n",
            "ShadowModel Epoch: 223, Approx Train: 0.7794, Train: 0.7810, Test: 0.6016,marco: 0.5979,micro: 0.6016\n",
            "ShadowModel Epoch: 224, Approx Train: 0.7810, Train: 0.7810, Test: 0.6016,marco: 0.5979,micro: 0.6016\n",
            "ShadowModel Epoch: 225, Approx Train: 0.7810, Train: 0.7810, Test: 0.6016,marco: 0.5979,micro: 0.6016\n",
            "ShadowModel Epoch: 226, Approx Train: 0.7810, Train: 0.7810, Test: 0.6016,marco: 0.5979,micro: 0.6016\n",
            "ShadowModel Epoch: 227, Approx Train: 0.7810, Train: 0.7810, Test: 0.6016,marco: 0.5979,micro: 0.6016\n",
            "ShadowModel Epoch: 228, Approx Train: 0.7810, Train: 0.7810, Test: 0.6048,marco: 0.6009,micro: 0.6048\n",
            "ShadowModel Epoch: 229, Approx Train: 0.7810, Train: 0.7810, Test: 0.6048,marco: 0.6009,micro: 0.6048\n",
            "ShadowModel Epoch: 230, Approx Train: 0.7810, Train: 0.7825, Test: 0.6048,marco: 0.6009,micro: 0.6048\n",
            "ShadowModel Epoch: 231, Approx Train: 0.7825, Train: 0.7825, Test: 0.6048,marco: 0.6003,micro: 0.6048\n",
            "ShadowModel Epoch: 232, Approx Train: 0.7825, Train: 0.7825, Test: 0.6063,marco: 0.6013,micro: 0.6063\n",
            "ShadowModel Epoch: 233, Approx Train: 0.7825, Train: 0.7841, Test: 0.6063,marco: 0.6013,micro: 0.6063\n",
            "ShadowModel Epoch: 234, Approx Train: 0.7841, Train: 0.7857, Test: 0.6063,marco: 0.6014,micro: 0.6063\n",
            "ShadowModel Epoch: 235, Approx Train: 0.7857, Train: 0.7857, Test: 0.6063,marco: 0.6015,micro: 0.6063\n",
            "ShadowModel Epoch: 236, Approx Train: 0.7857, Train: 0.7857, Test: 0.6063,marco: 0.6015,micro: 0.6063\n",
            "ShadowModel Epoch: 237, Approx Train: 0.7857, Train: 0.7873, Test: 0.6048,marco: 0.6003,micro: 0.6048\n",
            "ShadowModel Epoch: 238, Approx Train: 0.7873, Train: 0.7873, Test: 0.6048,marco: 0.6003,micro: 0.6048\n",
            "ShadowModel Epoch: 239, Approx Train: 0.7873, Train: 0.7873, Test: 0.6063,marco: 0.6024,micro: 0.6063\n",
            "ShadowModel Epoch: 240, Approx Train: 0.7873, Train: 0.7873, Test: 0.6063,marco: 0.6022,micro: 0.6063\n",
            "ShadowModel Epoch: 241, Approx Train: 0.7873, Train: 0.7873, Test: 0.6063,marco: 0.6022,micro: 0.6063\n",
            "ShadowModel Epoch: 242, Approx Train: 0.7873, Train: 0.7889, Test: 0.6063,marco: 0.6022,micro: 0.6063\n",
            "ShadowModel Epoch: 243, Approx Train: 0.7889, Train: 0.7905, Test: 0.6063,marco: 0.6022,micro: 0.6063\n",
            "ShadowModel Epoch: 244, Approx Train: 0.7905, Train: 0.7905, Test: 0.6063,marco: 0.6022,micro: 0.6063\n",
            "ShadowModel Epoch: 245, Approx Train: 0.7905, Train: 0.7905, Test: 0.6079,marco: 0.6044,micro: 0.6079\n",
            "ShadowModel Epoch: 246, Approx Train: 0.7905, Train: 0.7905, Test: 0.6079,marco: 0.6044,micro: 0.6079\n",
            "ShadowModel Epoch: 247, Approx Train: 0.7905, Train: 0.7905, Test: 0.6079,marco: 0.6044,micro: 0.6079\n",
            "ShadowModel Epoch: 248, Approx Train: 0.7905, Train: 0.7905, Test: 0.6063,marco: 0.6028,micro: 0.6063\n",
            "ShadowModel Epoch: 249, Approx Train: 0.7905, Train: 0.7905, Test: 0.6079,marco: 0.6039,micro: 0.6079\n",
            "ShadowModel Epoch: 250, Approx Train: 0.7905, Train: 0.7921, Test: 0.6079,marco: 0.6040,micro: 0.6079\n",
            "ShadowModel Epoch: 251, Approx Train: 0.7921, Train: 0.7921, Test: 0.6079,marco: 0.6040,micro: 0.6079\n",
            "ShadowModel Epoch: 252, Approx Train: 0.7921, Train: 0.7921, Test: 0.6063,marco: 0.6029,micro: 0.6063\n",
            "ShadowModel Epoch: 253, Approx Train: 0.7921, Train: 0.7921, Test: 0.6063,marco: 0.6029,micro: 0.6063\n",
            "ShadowModel Epoch: 254, Approx Train: 0.7921, Train: 0.7937, Test: 0.6063,marco: 0.6029,micro: 0.6063\n",
            "ShadowModel Epoch: 255, Approx Train: 0.7937, Train: 0.7968, Test: 0.6063,marco: 0.6029,micro: 0.6063\n",
            "ShadowModel Epoch: 256, Approx Train: 0.7968, Train: 0.7968, Test: 0.6079,marco: 0.6044,micro: 0.6079\n",
            "ShadowModel Epoch: 257, Approx Train: 0.7968, Train: 0.7968, Test: 0.6095,marco: 0.6052,micro: 0.6095\n",
            "ShadowModel Epoch: 258, Approx Train: 0.7968, Train: 0.7968, Test: 0.6095,marco: 0.6052,micro: 0.6095\n",
            "ShadowModel Epoch: 259, Approx Train: 0.7968, Train: 0.7968, Test: 0.6111,marco: 0.6063,micro: 0.6111\n",
            "ShadowModel Epoch: 260, Approx Train: 0.7968, Train: 0.7968, Test: 0.6111,marco: 0.6065,micro: 0.6111\n",
            "ShadowModel Epoch: 261, Approx Train: 0.7968, Train: 0.7984, Test: 0.6111,marco: 0.6065,micro: 0.6111\n",
            "ShadowModel Epoch: 262, Approx Train: 0.7984, Train: 0.7984, Test: 0.6111,marco: 0.6065,micro: 0.6111\n",
            "ShadowModel Epoch: 263, Approx Train: 0.7984, Train: 0.7984, Test: 0.6111,marco: 0.6065,micro: 0.6111\n",
            "ShadowModel Epoch: 264, Approx Train: 0.7984, Train: 0.8000, Test: 0.6111,marco: 0.6065,micro: 0.6111\n",
            "ShadowModel Epoch: 265, Approx Train: 0.8000, Train: 0.8000, Test: 0.6111,marco: 0.6065,micro: 0.6111\n",
            "ShadowModel Epoch: 266, Approx Train: 0.8000, Train: 0.8016, Test: 0.6111,marco: 0.6065,micro: 0.6111\n",
            "ShadowModel Epoch: 267, Approx Train: 0.8016, Train: 0.8016, Test: 0.6127,marco: 0.6083,micro: 0.6127\n",
            "ShadowModel Epoch: 268, Approx Train: 0.8016, Train: 0.8016, Test: 0.6143,marco: 0.6094,micro: 0.6143\n",
            "ShadowModel Epoch: 269, Approx Train: 0.8016, Train: 0.8016, Test: 0.6127,marco: 0.6085,micro: 0.6127\n",
            "ShadowModel Epoch: 270, Approx Train: 0.8016, Train: 0.8016, Test: 0.6127,marco: 0.6085,micro: 0.6127\n",
            "ShadowModel Epoch: 271, Approx Train: 0.8016, Train: 0.8016, Test: 0.6127,marco: 0.6085,micro: 0.6127\n",
            "ShadowModel Epoch: 272, Approx Train: 0.8016, Train: 0.8016, Test: 0.6127,marco: 0.6080,micro: 0.6127\n",
            "ShadowModel Epoch: 273, Approx Train: 0.8016, Train: 0.8016, Test: 0.6127,marco: 0.6080,micro: 0.6127\n",
            "ShadowModel Epoch: 274, Approx Train: 0.8016, Train: 0.8016, Test: 0.6143,marco: 0.6104,micro: 0.6143\n",
            "ShadowModel Epoch: 275, Approx Train: 0.8016, Train: 0.8016, Test: 0.6143,marco: 0.6104,micro: 0.6143\n",
            "ShadowModel Epoch: 276, Approx Train: 0.8016, Train: 0.8032, Test: 0.6143,marco: 0.6104,micro: 0.6143\n",
            "ShadowModel Epoch: 277, Approx Train: 0.8032, Train: 0.8032, Test: 0.6143,marco: 0.6104,micro: 0.6143\n",
            "ShadowModel Epoch: 278, Approx Train: 0.8032, Train: 0.8032, Test: 0.6143,marco: 0.6104,micro: 0.6143\n",
            "ShadowModel Epoch: 279, Approx Train: 0.8032, Train: 0.8032, Test: 0.6143,marco: 0.6104,micro: 0.6143\n",
            "ShadowModel Epoch: 280, Approx Train: 0.8032, Train: 0.8032, Test: 0.6143,marco: 0.6097,micro: 0.6143\n",
            "ShadowModel Epoch: 281, Approx Train: 0.8032, Train: 0.8032, Test: 0.6143,marco: 0.6097,micro: 0.6143\n",
            "ShadowModel Epoch: 282, Approx Train: 0.8032, Train: 0.8048, Test: 0.6143,marco: 0.6097,micro: 0.6143\n",
            "ShadowModel Epoch: 283, Approx Train: 0.8048, Train: 0.8063, Test: 0.6143,marco: 0.6097,micro: 0.6143\n",
            "ShadowModel Epoch: 284, Approx Train: 0.8063, Train: 0.8063, Test: 0.6143,marco: 0.6097,micro: 0.6143\n",
            "ShadowModel Epoch: 285, Approx Train: 0.8063, Train: 0.8063, Test: 0.6143,marco: 0.6095,micro: 0.6143\n",
            "ShadowModel Epoch: 286, Approx Train: 0.8063, Train: 0.8063, Test: 0.6143,marco: 0.6095,micro: 0.6143\n",
            "ShadowModel Epoch: 287, Approx Train: 0.8063, Train: 0.8063, Test: 0.6143,marco: 0.6095,micro: 0.6143\n",
            "ShadowModel Epoch: 288, Approx Train: 0.8063, Train: 0.8079, Test: 0.6143,marco: 0.6095,micro: 0.6143\n",
            "ShadowModel Epoch: 289, Approx Train: 0.8079, Train: 0.8079, Test: 0.6143,marco: 0.6095,micro: 0.6143\n",
            "ShadowModel Epoch: 290, Approx Train: 0.8079, Train: 0.8079, Test: 0.6143,marco: 0.6095,micro: 0.6143\n",
            "ShadowModel Epoch: 291, Approx Train: 0.8079, Train: 0.8079, Test: 0.6143,marco: 0.6095,micro: 0.6143\n",
            "ShadowModel Epoch: 292, Approx Train: 0.8079, Train: 0.8079, Test: 0.6175,marco: 0.6125,micro: 0.6175\n",
            "ShadowModel Epoch: 293, Approx Train: 0.8079, Train: 0.8095, Test: 0.6175,marco: 0.6125,micro: 0.6175\n",
            "ShadowModel Epoch: 294, Approx Train: 0.8095, Train: 0.8095, Test: 0.6175,marco: 0.6125,micro: 0.6175\n",
            "ShadowModel Epoch: 295, Approx Train: 0.8095, Train: 0.8095, Test: 0.6175,marco: 0.6125,micro: 0.6175\n",
            "ShadowModel Epoch: 296, Approx Train: 0.8095, Train: 0.8095, Test: 0.6175,marco: 0.6125,micro: 0.6175\n",
            "ShadowModel Epoch: 297, Approx Train: 0.8095, Train: 0.8095, Test: 0.6175,marco: 0.6127,micro: 0.6175\n",
            "ShadowModel Epoch: 298, Approx Train: 0.8095, Train: 0.8095, Test: 0.6175,marco: 0.6127,micro: 0.6175\n",
            "ShadowModel Epoch: 299, Approx Train: 0.8095, Train: 0.8095, Test: 0.6175,marco: 0.6127,micro: 0.6175\n",
            "ShadowModel Epoch: 300, Approx Train: 0.8095, Train: 0.8095, Test: 0.6175,marco: 0.6127,micro: 0.6175\n",
            "positive_attack_data.shape (630, 8)\n",
            "negative_attack_data.shape (630, 8)\n",
            "attack_data.shape (1260, 8)\n",
            "X_attack.shape (1260, 7)\n",
            "X_attack_InTrain (630, 7)\n",
            "X_attack_OutTrain (630, 7)\n",
            "X_InTrain.shape (1260, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1487: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "Epoch: 1/100.. Training loss: 0.64257.. Test Loss: 0.66316.. Train Accuracy: 0.653 Test Accuracy: 0.628\n",
            "Epoch: 2/100.. Training loss: 0.60048.. Test Loss: 0.62608.. Train Accuracy: 0.696 Test Accuracy: 0.658\n",
            "Epoch: 3/100.. Training loss: 0.59101.. Test Loss: 0.65331.. Train Accuracy: 0.712 Test Accuracy: 0.590\n",
            "Epoch: 4/100.. Training loss: 0.57438.. Test Loss: 0.62695.. Train Accuracy: 0.729 Test Accuracy: 0.677\n",
            "Epoch: 5/100.. Training loss: 0.56905.. Test Loss: 0.61584.. Train Accuracy: 0.738 Test Accuracy: 0.689\n",
            "Epoch: 6/100.. Training loss: 0.57315.. Test Loss: 0.64111.. Train Accuracy: 0.733 Test Accuracy: 0.653\n",
            "Epoch: 7/100.. Training loss: 0.56769.. Test Loss: 0.60620.. Train Accuracy: 0.735 Test Accuracy: 0.674\n",
            "Epoch: 8/100.. Training loss: 0.56183.. Test Loss: 0.59845.. Train Accuracy: 0.744 Test Accuracy: 0.705\n",
            "Epoch: 9/100.. Training loss: 0.56140.. Test Loss: 0.61692.. Train Accuracy: 0.740 Test Accuracy: 0.681\n",
            "Epoch: 10/100.. Training loss: 0.55401.. Test Loss: 0.61670.. Train Accuracy: 0.757 Test Accuracy: 0.665\n",
            "Epoch: 11/100.. Training loss: 0.55295.. Test Loss: 0.55876.. Train Accuracy: 0.747 Test Accuracy: 0.745\n",
            "Epoch: 12/100.. Training loss: 0.56086.. Test Loss: 0.59745.. Train Accuracy: 0.739 Test Accuracy: 0.705\n",
            "Epoch: 13/100.. Training loss: 0.55949.. Test Loss: 0.53865.. Train Accuracy: 0.752 Test Accuracy: 0.788\n",
            "Epoch: 14/100.. Training loss: 0.55695.. Test Loss: 0.60012.. Train Accuracy: 0.751 Test Accuracy: 0.689\n",
            "Epoch: 15/100.. Training loss: 0.55135.. Test Loss: 0.59021.. Train Accuracy: 0.756 Test Accuracy: 0.736\n",
            "Epoch: 16/100.. Training loss: 0.54775.. Test Loss: 0.63341.. Train Accuracy: 0.761 Test Accuracy: 0.637\n",
            "Epoch: 17/100.. Training loss: 0.55310.. Test Loss: 0.62101.. Train Accuracy: 0.759 Test Accuracy: 0.668\n",
            "Epoch: 18/100.. Training loss: 0.54772.. Test Loss: 0.58076.. Train Accuracy: 0.753 Test Accuracy: 0.720\n",
            "Epoch: 19/100.. Training loss: 0.54486.. Test Loss: 0.58348.. Train Accuracy: 0.769 Test Accuracy: 0.708\n",
            "Epoch: 20/100.. Training loss: 0.54701.. Test Loss: 0.61721.. Train Accuracy: 0.761 Test Accuracy: 0.665\n",
            "Epoch: 21/100.. Training loss: 0.55475.. Test Loss: 0.56543.. Train Accuracy: 0.752 Test Accuracy: 0.748\n",
            "Epoch: 22/100.. Training loss: 0.55023.. Test Loss: 0.56584.. Train Accuracy: 0.753 Test Accuracy: 0.733\n",
            "Epoch: 23/100.. Training loss: 0.54314.. Test Loss: 0.54030.. Train Accuracy: 0.765 Test Accuracy: 0.760\n",
            "Epoch: 24/100.. Training loss: 0.54381.. Test Loss: 0.62603.. Train Accuracy: 0.765 Test Accuracy: 0.665\n",
            "Epoch: 25/100.. Training loss: 0.54101.. Test Loss: 0.60454.. Train Accuracy: 0.768 Test Accuracy: 0.696\n",
            "Epoch: 26/100.. Training loss: 0.54345.. Test Loss: 0.55813.. Train Accuracy: 0.764 Test Accuracy: 0.745\n",
            "Epoch: 27/100.. Training loss: 0.53905.. Test Loss: 0.56959.. Train Accuracy: 0.764 Test Accuracy: 0.708\n",
            "Epoch: 28/100.. Training loss: 0.53376.. Test Loss: 0.60368.. Train Accuracy: 0.782 Test Accuracy: 0.693\n",
            "Epoch: 29/100.. Training loss: 0.53448.. Test Loss: 0.59433.. Train Accuracy: 0.776 Test Accuracy: 0.708\n",
            "Epoch: 30/100.. Training loss: 0.53799.. Test Loss: 0.60626.. Train Accuracy: 0.770 Test Accuracy: 0.701\n",
            "Epoch: 31/100.. Training loss: 0.54953.. Test Loss: 0.55998.. Train Accuracy: 0.758 Test Accuracy: 0.752\n",
            "Epoch: 32/100.. Training loss: 0.53805.. Test Loss: 0.59544.. Train Accuracy: 0.768 Test Accuracy: 0.712\n",
            "Epoch: 33/100.. Training loss: 0.53954.. Test Loss: 0.59042.. Train Accuracy: 0.770 Test Accuracy: 0.705\n",
            "Epoch: 34/100.. Training loss: 0.53263.. Test Loss: 0.63110.. Train Accuracy: 0.780 Test Accuracy: 0.653\n",
            "Epoch: 35/100.. Training loss: 0.53629.. Test Loss: 0.52001.. Train Accuracy: 0.771 Test Accuracy: 0.788\n",
            "Epoch: 36/100.. Training loss: 0.53127.. Test Loss: 0.59406.. Train Accuracy: 0.781 Test Accuracy: 0.701\n",
            "Epoch: 37/100.. Training loss: 0.52898.. Test Loss: 0.54849.. Train Accuracy: 0.782 Test Accuracy: 0.729\n",
            "Epoch: 38/100.. Training loss: 0.53186.. Test Loss: 0.59794.. Train Accuracy: 0.777 Test Accuracy: 0.701\n",
            "Epoch: 39/100.. Training loss: 0.53819.. Test Loss: 0.61346.. Train Accuracy: 0.772 Test Accuracy: 0.696\n",
            "Epoch: 40/100.. Training loss: 0.53799.. Test Loss: 0.57718.. Train Accuracy: 0.775 Test Accuracy: 0.736\n",
            "Epoch: 41/100.. Training loss: 0.53476.. Test Loss: 0.60071.. Train Accuracy: 0.777 Test Accuracy: 0.693\n",
            "Epoch: 42/100.. Training loss: 0.53522.. Test Loss: 0.65086.. Train Accuracy: 0.774 Test Accuracy: 0.634\n",
            "Epoch: 43/100.. Training loss: 0.53448.. Test Loss: 0.66207.. Train Accuracy: 0.776 Test Accuracy: 0.634\n",
            "Epoch: 44/100.. Training loss: 0.52739.. Test Loss: 0.63068.. Train Accuracy: 0.784 Test Accuracy: 0.668\n",
            "Epoch: 45/100.. Training loss: 0.52753.. Test Loss: 0.58915.. Train Accuracy: 0.783 Test Accuracy: 0.729\n",
            "Epoch: 46/100.. Training loss: 0.52951.. Test Loss: 0.59364.. Train Accuracy: 0.782 Test Accuracy: 0.720\n",
            "Epoch: 47/100.. Training loss: 0.53492.. Test Loss: 0.62125.. Train Accuracy: 0.774 Test Accuracy: 0.693\n",
            "Epoch: 48/100.. Training loss: 0.52597.. Test Loss: 0.63232.. Train Accuracy: 0.785 Test Accuracy: 0.630\n",
            "Epoch: 49/100.. Training loss: 0.53794.. Test Loss: 0.62734.. Train Accuracy: 0.775 Test Accuracy: 0.693\n",
            "Epoch: 50/100.. Training loss: 0.52874.. Test Loss: 0.59584.. Train Accuracy: 0.781 Test Accuracy: 0.705\n",
            "Epoch: 51/100.. Training loss: 0.52045.. Test Loss: 0.62893.. Train Accuracy: 0.792 Test Accuracy: 0.677\n",
            "Epoch: 52/100.. Training loss: 0.52170.. Test Loss: 0.61787.. Train Accuracy: 0.792 Test Accuracy: 0.674\n",
            "Epoch: 53/100.. Training loss: 0.52317.. Test Loss: 0.59122.. Train Accuracy: 0.789 Test Accuracy: 0.720\n",
            "Epoch: 54/100.. Training loss: 0.53175.. Test Loss: 0.58755.. Train Accuracy: 0.776 Test Accuracy: 0.705\n",
            "Epoch: 55/100.. Training loss: 0.53939.. Test Loss: 0.59528.. Train Accuracy: 0.772 Test Accuracy: 0.724\n",
            "Epoch: 56/100.. Training loss: 0.53061.. Test Loss: 0.61647.. Train Accuracy: 0.779 Test Accuracy: 0.677\n",
            "Epoch: 57/100.. Training loss: 0.53590.. Test Loss: 0.63131.. Train Accuracy: 0.772 Test Accuracy: 0.681\n",
            "Epoch: 58/100.. Training loss: 0.51983.. Test Loss: 0.60246.. Train Accuracy: 0.789 Test Accuracy: 0.677\n",
            "Epoch: 59/100.. Training loss: 0.52022.. Test Loss: 0.60653.. Train Accuracy: 0.793 Test Accuracy: 0.689\n",
            "Epoch: 60/100.. Training loss: 0.51496.. Test Loss: 0.56072.. Train Accuracy: 0.795 Test Accuracy: 0.745\n",
            "Epoch: 61/100.. Training loss: 0.51529.. Test Loss: 0.61458.. Train Accuracy: 0.798 Test Accuracy: 0.689\n",
            "Epoch: 62/100.. Training loss: 0.51461.. Test Loss: 0.61740.. Train Accuracy: 0.797 Test Accuracy: 0.689\n",
            "Epoch: 63/100.. Training loss: 0.51631.. Test Loss: 0.60761.. Train Accuracy: 0.796 Test Accuracy: 0.677\n",
            "Epoch: 64/100.. Training loss: 0.52119.. Test Loss: 0.61865.. Train Accuracy: 0.790 Test Accuracy: 0.689\n",
            "Epoch: 65/100.. Training loss: 0.51818.. Test Loss: 0.59251.. Train Accuracy: 0.791 Test Accuracy: 0.705\n",
            "Epoch: 66/100.. Training loss: 0.51196.. Test Loss: 0.63199.. Train Accuracy: 0.799 Test Accuracy: 0.653\n",
            "Epoch: 67/100.. Training loss: 0.51408.. Test Loss: 0.60276.. Train Accuracy: 0.797 Test Accuracy: 0.677\n",
            "Epoch: 68/100.. Training loss: 0.51435.. Test Loss: 0.60997.. Train Accuracy: 0.794 Test Accuracy: 0.689\n",
            "Epoch: 69/100.. Training loss: 0.50861.. Test Loss: 0.61607.. Train Accuracy: 0.803 Test Accuracy: 0.693\n",
            "Epoch: 70/100.. Training loss: 0.50883.. Test Loss: 0.57204.. Train Accuracy: 0.802 Test Accuracy: 0.752\n",
            "Epoch: 71/100.. Training loss: 0.50799.. Test Loss: 0.57922.. Train Accuracy: 0.807 Test Accuracy: 0.733\n",
            "Epoch: 72/100.. Training loss: 0.50834.. Test Loss: 0.60134.. Train Accuracy: 0.807 Test Accuracy: 0.705\n",
            "Epoch: 73/100.. Training loss: 0.50699.. Test Loss: 0.61602.. Train Accuracy: 0.806 Test Accuracy: 0.693\n",
            "Epoch: 74/100.. Training loss: 0.51430.. Test Loss: 0.62635.. Train Accuracy: 0.797 Test Accuracy: 0.665\n",
            "Epoch: 75/100.. Training loss: 0.51662.. Test Loss: 0.63517.. Train Accuracy: 0.795 Test Accuracy: 0.649\n",
            "Epoch: 76/100.. Training loss: 0.51175.. Test Loss: 0.62372.. Train Accuracy: 0.799 Test Accuracy: 0.658\n",
            "Epoch: 77/100.. Training loss: 0.52262.. Test Loss: 0.63329.. Train Accuracy: 0.789 Test Accuracy: 0.674\n",
            "Epoch: 78/100.. Training loss: 0.50980.. Test Loss: 0.60759.. Train Accuracy: 0.804 Test Accuracy: 0.693\n",
            "Epoch: 79/100.. Training loss: 0.50738.. Test Loss: 0.59420.. Train Accuracy: 0.805 Test Accuracy: 0.717\n",
            "Epoch: 80/100.. Training loss: 0.51049.. Test Loss: 0.61633.. Train Accuracy: 0.802 Test Accuracy: 0.693\n",
            "Epoch: 81/100.. Training loss: 0.51172.. Test Loss: 0.60700.. Train Accuracy: 0.799 Test Accuracy: 0.705\n",
            "Epoch: 82/100.. Training loss: 0.51293.. Test Loss: 0.59924.. Train Accuracy: 0.799 Test Accuracy: 0.705\n",
            "Epoch: 83/100.. Training loss: 0.50817.. Test Loss: 0.60095.. Train Accuracy: 0.805 Test Accuracy: 0.717\n",
            "Epoch: 84/100.. Training loss: 0.50910.. Test Loss: 0.61927.. Train Accuracy: 0.803 Test Accuracy: 0.696\n",
            "Epoch: 85/100.. Training loss: 0.50204.. Test Loss: 0.57203.. Train Accuracy: 0.810 Test Accuracy: 0.729\n",
            "Epoch: 86/100.. Training loss: 0.50456.. Test Loss: 0.61646.. Train Accuracy: 0.810 Test Accuracy: 0.693\n",
            "Epoch: 87/100.. Training loss: 0.50418.. Test Loss: 0.60184.. Train Accuracy: 0.807 Test Accuracy: 0.677\n",
            "Epoch: 88/100.. Training loss: 0.51002.. Test Loss: 0.62212.. Train Accuracy: 0.803 Test Accuracy: 0.681\n",
            "Epoch: 89/100.. Training loss: 0.50955.. Test Loss: 0.59132.. Train Accuracy: 0.801 Test Accuracy: 0.714\n",
            "Epoch: 90/100.. Training loss: 0.51932.. Test Loss: 0.61537.. Train Accuracy: 0.792 Test Accuracy: 0.689\n",
            "Epoch: 91/100.. Training loss: 0.52208.. Test Loss: 0.61109.. Train Accuracy: 0.788 Test Accuracy: 0.705\n",
            "Epoch: 92/100.. Training loss: 0.50641.. Test Loss: 0.58534.. Train Accuracy: 0.807 Test Accuracy: 0.705\n",
            "Epoch: 93/100.. Training loss: 0.50826.. Test Loss: 0.62735.. Train Accuracy: 0.804 Test Accuracy: 0.677\n",
            "Epoch: 94/100.. Training loss: 0.50607.. Test Loss: 0.60559.. Train Accuracy: 0.804 Test Accuracy: 0.693\n",
            "Epoch: 95/100.. Training loss: 0.50165.. Test Loss: 0.58140.. Train Accuracy: 0.813 Test Accuracy: 0.729\n",
            "Epoch: 96/100.. Training loss: 0.51916.. Test Loss: 0.57059.. Train Accuracy: 0.790 Test Accuracy: 0.733\n",
            "Epoch: 97/100.. Training loss: 0.51208.. Test Loss: 0.57017.. Train Accuracy: 0.799 Test Accuracy: 0.745\n",
            "Epoch: 98/100.. Training loss: 0.50635.. Test Loss: 0.59632.. Train Accuracy: 0.807 Test Accuracy: 0.720\n",
            "Epoch: 99/100.. Training loss: 0.50812.. Test Loss: 0.58396.. Train Accuracy: 0.804 Test Accuracy: 0.720\n",
            "Epoch: 100/100.. Training loss: 0.50309.. Test Loss: 0.59308.. Train Accuracy: 0.812 Test Accuracy: 0.705\n",
            "To confirm using attack_test_data_loader (50 test samples): 0.729 AUROC: 0.723 precision: 0.739 recall 0.729\n",
            "Test accuracy with Target Train InOut: 0.699 AUROC: 0.693 precision: 0.704 recall 0.699 F1 score 0.697 ===> Attack Performance!\n",
            "data_type Cora\n",
            "model_type GCN\n",
            "WhichRun 1  Total time 99.501\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
