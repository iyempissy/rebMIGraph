{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership Leakage Graph TSTF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMtBbRZYqfsA",
        "outputId": "c9144ba3-2d0b-4178-9e8b-20442f6c8f09"
      },
      "source": [
        "# This is for the TSTF setting\n",
        "# Install required packages.\n",
        "!pip install -q torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git\n",
        "\n",
        "'''\n",
        "# Train on subgraph and Test on Full graph. Thus TSTF\n",
        "'''\n",
        "\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid, Reddit, Flickr\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, SGConv, GATConv\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import NeighborSampler\n",
        "\n",
        "from torch_geometric.utils import subgraph\n",
        "from torch_geometric.data import Data\n",
        "import random\n",
        "import sys\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, roc_curve, roc_auc_score, f1_score\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(3)  # change this cos sometimes port 0 is full\n",
        "\n",
        "num_of_runs = 2  # 11  # this runs the program 10 times\n",
        "\n",
        "for which_run in range(1, num_of_runs):\n",
        "    random_data = os.urandom(4)\n",
        "\n",
        "    rand_state = int.from_bytes(random_data, byteorder=\"big\")\n",
        "    print(\"rand_state\", rand_state)\n",
        "    torch.manual_seed(rand_state)\n",
        "    random.seed(rand_state)\n",
        "    np.random.seed(seed=rand_state)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # we run each data_type e.g cora against all model type\n",
        "\n",
        "    '''\n",
        "    Set parameters here ===================================================================================?????????\n",
        "    '''\n",
        "\n",
        "    model_type = \"GCN\"  # GCN, GAT, SAGE, SGC\n",
        "    data_type = \"Cora\"  # CiteSeer, Cora, PubMed, Flickr, Reddit\n",
        "\n",
        "    mode = \"TSTF\"  # train on subgraph, test on full grah\n",
        "\n",
        "    save_shadow_OutTrain = \"posteriorsShadowOut_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_shadow_InTrain = \"posteriorsShadowTrain_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_target_OutTrain = \"posteriorsTargetOut_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_target_InTrain = \"posteriorsTargetTrain_\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "\n",
        "    save_correct_incorrect_homophily_prediction = \"correct_incorrect_homo_pred\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_global_true_homophily = \"true_homophily\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "    save_global_pred_homophily = \"pred_homophily\" + mode + \"_\" + data_type + \"_\" + model_type + \".txt\"\n",
        "\n",
        "    save_target_InTrain_nodes_neigbors = \"nodesNeigborsTargetTrain_\" + mode + \"_\" + data_type + \"_\" + model_type + \".npy\"\n",
        "    save_target_OutTrain_nodes_neigbors = \"nodesNeigborsTargetOut_\" + mode + \"_\" + data_type + \"_\" + model_type + \".npy\"\n",
        "\n",
        "    result_file = open(\"resultfile_\" + mode + \"_\" + model_type + \".txt\", \"a\")\n",
        "\n",
        "    '''\n",
        "    ######################################## Data ##############################################\n",
        "    '''\n",
        "    if data_type == \"Reddit\":\n",
        "        ###################################### Reddit ##################################\n",
        "\n",
        "        # path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Reddit')\n",
        "        path = \"./data/Reddit\"\n",
        "        dataset = Reddit(path)\n",
        "        # data = dataset[0]\n",
        "        # print(\"len(dataset)\", len(dataset)) # Reddit dataset consists of 1 graph\n",
        "        # print(\"data\", data) # Data(edge_index=[2, 114615892], test_mask=[232965], train_mask=[232965], val_mask=[232965], x=[232965, 602], y=[232965])\n",
        "        # print(\"Total Num of nodes in dataset\", data.num_nodes) # 232965\n",
        "        # print(\"Total Num of edges in dataset\", data.num_edges) # 114615892\n",
        "        # print(\"Total Num of node features in dataset\", data.num_node_features) # 602\n",
        "        # print(\"Total Num of features in dataset\", dataset.num_features) # same as node features # 602\n",
        "        # print(\"Num classes\", dataset.num_classes) #41\n",
        "\n",
        "        # Reduced this cos it's taking too long to create subgraph\n",
        "        num_train_Train_per_class = 500  # 1000\n",
        "        num_train_Shadow_per_class = 500  # 1000\n",
        "        num_test_Target = 20500  # 41000\n",
        "        num_test_Shadow = 20500  # 41000\n",
        "\n",
        "    elif data_type == \"Flickr\":\n",
        "\n",
        "        ###################################### Flickr ##################################\n",
        "\n",
        "        path = \"./data/Flickr\"\n",
        "        dataset = Flickr(path)\n",
        "        data = dataset[0]\n",
        "        # print(\"len(dataset)\", len(dataset))  # Flikr dataset consists of 1 graph\n",
        "        # print(\"data\",\n",
        "        #       data)  # Data(edge_index=[2, 899756], test_mask=[89250], train_mask=[89250], val_mask=[89250], x=[89250, 500], y=[89250])\n",
        "        # print(\"Total Num of nodes in dataset\", data.num_nodes)  # 89250\n",
        "        # print(\"Total Num of edges in dataset\", data.num_edges)  # 899756\n",
        "        # print(\"Total Num of node features in dataset\", data.num_node_features)  # 500\n",
        "        # print(\"Total Num of features in dataset\", dataset.num_features)  # same as node features # 500\n",
        "        # print(\"Num classes\", dataset.num_classes)  # 7\n",
        "\n",
        "        num_train_Train_per_class = 1500  # cos min of all classes only have 3k nodes\n",
        "        num_train_Shadow_per_class = 1500\n",
        "        num_test_Target = 10500\n",
        "        num_test_Shadow = 10500\n",
        "\n",
        "    elif data_type == \"Cora\":\n",
        "\n",
        "        ###################################### Cora ##################################\n",
        "\n",
        "        dataset = Planetoid(root='./data/Cora', name='Cora', split=\"random\")  # set test to 1320 to match train\n",
        "        num_train_Train_per_class = 90  # 180\n",
        "        num_train_Shadow_per_class = 90\n",
        "        num_test_Target = 630\n",
        "        num_test_Shadow = 630\n",
        "\n",
        "\n",
        "    elif data_type == \"CiteSeer\":\n",
        "\n",
        "        ###################################### CiteSeer ##################################\n",
        "\n",
        "        dataset = Planetoid(root='./data/CiteSeer', name='CiteSeer', split=\"random\")\n",
        "        num_train_Train_per_class = 100\n",
        "        num_train_Shadow_per_class = 100\n",
        "        num_test_Target = 600\n",
        "        num_test_Shadow = 600\n",
        "\n",
        "\n",
        "    elif data_type == \"PubMed\":\n",
        "\n",
        "        ###################################### Reddit ##################################\n",
        "\n",
        "        dataset = Planetoid(root='./data/PubMed', name='PubMed', split=\"random\")\n",
        "\n",
        "        num_train_Train_per_class = 1500\n",
        "        num_train_Shadow_per_class = 1500\n",
        "        num_test_Target = 4500\n",
        "        num_test_Shadow = 4500\n",
        "\n",
        "    else:\n",
        "        print(\"Error: No data specified\")\n",
        "\n",
        "    data = dataset[0]\n",
        "    print(\"data\", data)\n",
        "\n",
        "    '''\n",
        "    ############################################# Target and Shadow Models ###############################################\n",
        "    '''\n",
        "\n",
        "\n",
        "    class TargetModel(torch.nn.Module):\n",
        "        def __init__(self, dataset):\n",
        "            super(TargetModel, self).__init__()\n",
        "\n",
        "            if model_type == \"GCN\":\n",
        "                # GCN\n",
        "                self.conv1 = GCNConv(dataset.num_node_features, 256)\n",
        "                self.conv2 = GCNConv(256, dataset.num_classes)\n",
        "            elif model_type == \"SAGE\":\n",
        "                # GraphSage\n",
        "                # self.conv1 = SAGEConv(dataset.num_node_features, 256)\n",
        "                # self.conv2 = SAGEConv(256, dataset.num_classes)\n",
        "\n",
        "                self.num_layers = 2\n",
        "\n",
        "                self.convs = torch.nn.ModuleList()\n",
        "                self.convs.append(SAGEConv(dataset.num_node_features, 256))\n",
        "                self.convs.append(SAGEConv(256, dataset.num_classes))\n",
        "\n",
        "            elif model_type == \"SGC\":\n",
        "                # SGC\n",
        "                self.conv1 = SGConv(dataset.num_node_features, 256, K=2, cached=False)\n",
        "                self.conv2 = SGConv(256, dataset.num_classes, K=2, cached=False)\n",
        "\n",
        "            elif model_type == \"GAT\":\n",
        "                # GAT\n",
        "                self.conv1 = GATConv(dataset.num_features, 8, heads=8, dropout=0.1)\n",
        "                # On the Pubmed dataset, use heads=8 in conv2.\n",
        "                if data_type == \"PubMed\":\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False)\n",
        "                    # self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False, dropout=0.1)\n",
        "                else:\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=False)\n",
        "            else:\n",
        "                print(\"Error: No model selected\")\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "            # print(\"xxxxxxx\", x.size())\n",
        "\n",
        "            if model_type == \"SAGE\":\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                # the edge_index here is quite different (it is a list cos we will be passing train_loader). edge index is different based on batch data.\n",
        "                # Note edge_index here is a bipartite graph. meaning all the edges retured are connected\n",
        "                for i, (edge_ind, _, size) in enumerate(edge_index):\n",
        "                    # print(\"iiiiiiiiiiiiiiii\", i)\n",
        "\n",
        "                    edges_raw = edge_ind.cpu().numpy()\n",
        "                    # print(\"edges_raw\", edges_raw)\n",
        "                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                    G = nx.Graph()\n",
        "                    G.add_nodes_from(list(range(\n",
        "                        data.num_nodes)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Target\n",
        "                    G.add_edges_from(edges)\n",
        "\n",
        "                    # print(\"x.size(0) forward\", x.size(0))\n",
        "                    # print(\"x.size forward\", x.size())\n",
        "                    # getting the neigbors of a particular node\n",
        "                    for n in range(0, x.size(\n",
        "                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                        all_nodes.append(n)  # get all nodes\n",
        "                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                        # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "                    # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "                    x_target = x[:size[1]]  # Target nodes are always placed first.\n",
        "                    x = self.convs[i]((x, x_target), edge_ind)\n",
        "                    if i != self.num_layers - 1:\n",
        "                        x = F.relu(x)\n",
        "                        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "                # print(\"Final all nodes and neighbors\", all_node_and_neigbors)\n",
        "                return x.log_softmax(dim=-1), all_node_and_neigbors\n",
        "\n",
        "            else:\n",
        "                # torch.set_printoptions(threshold=10000)\n",
        "                # print(\"x0\", x[0])\n",
        "                # print(\"x1\", x[1])\n",
        "                # print(\"x\", x.shape)\n",
        "                # print(\"edge_index\", edge_index.shape)\n",
        "\n",
        "                # Begin edit data for passing node n it's neghbprs\n",
        "                edges_raw = edge_index.cpu().numpy()\n",
        "                # print(\"edges_raw\", edges_raw)\n",
        "                edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                G = nx.Graph()\n",
        "                G.add_nodes_from(list(range(\n",
        "                    data.num_nodes)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Target\n",
        "                G.add_edges_from(edges)\n",
        "\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                # print(\"x.size(0) shadow forward\", x.size(0))\n",
        "                for n in range(0, x.size(\n",
        "                        0)):  # x.size(0) caters for when u input the full graph for for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                    all_nodes.append(n)  # get all nodes\n",
        "                    # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                    all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                    # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "\n",
        "                # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "                x, edge_index = x, edge_index\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                # x = F.dropout(x, p=0.5, training=self.training)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "                x = self.conv2(x, edge_index)\n",
        "                # x = F.relu(x)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "                return F.log_softmax(x, dim=1), all_node_and_neigbors\n",
        "\n",
        "        def inference(self, x_all):\n",
        "            # Compute representations of nodes layer by layer, using *all*\n",
        "            # available edges. This leads to faster computation in contrast to\n",
        "            # immediately computing the final representations of each batch.\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                xs = []\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                for batch_size, n_id, adj in all_graph_loader:\n",
        "                    edge_index, _, size = adj.to(device)\n",
        "\n",
        "                    x = x_all[n_id].to(device)\n",
        "\n",
        "                    edges_raw = edge_index.cpu().numpy()\n",
        "                    # print(\"edges_raw inference\", edges_raw)\n",
        "                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                    G = nx.Graph()\n",
        "                    G.add_nodes_from(list(range(\n",
        "                        data.num_nodes)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Target\n",
        "                    G.add_edges_from(edges)\n",
        "\n",
        "                    # print(\"x.size(0)\", x.size(0))\n",
        "                    # print(\"x.size inference\", x.size())\n",
        "                    for n in range(0, x.size(\n",
        "                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                        all_nodes.append(n)  # get all nodes\n",
        "                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                        # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "\n",
        "                    # print(\"all_node_and_neigbors inference\", all_node_and_neigbors)\n",
        "\n",
        "                    x_target = x[:size[1]]\n",
        "                    x = self.convs[i]((x, x_target), edge_index)\n",
        "                    if i != self.num_layers - 1:\n",
        "                        x = F.relu(x)\n",
        "                    xs.append(x.cpu())\n",
        "\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "\n",
        "            return F.log_softmax(x_all, dim=1), all_node_and_neigbors\n",
        "\n",
        "\n",
        "    class ShadowModel(torch.nn.Module):\n",
        "        def __init__(self, dataset):\n",
        "            super(ShadowModel, self).__init__()\n",
        "\n",
        "            if model_type == \"GCN\":\n",
        "                # GCN\n",
        "                self.conv1 = GCNConv(dataset.num_node_features, 256)\n",
        "                self.conv2 = GCNConv(256, dataset.num_classes)\n",
        "            elif model_type == \"SAGE\":\n",
        "                # GraphSage\n",
        "                # self.conv1 = SAGEConv(dataset.num_node_features, 256)\n",
        "                # self.conv2 = SAGEConv(256, dataset.num_classes)\n",
        "\n",
        "                self.num_layers = 2\n",
        "\n",
        "                self.convs = torch.nn.ModuleList()\n",
        "                self.convs.append(SAGEConv(dataset.num_node_features, 256))\n",
        "                self.convs.append(SAGEConv(256, dataset.num_classes))\n",
        "\n",
        "            elif model_type == \"SGC\":\n",
        "                # SGC\n",
        "                self.conv1 = SGConv(dataset.num_node_features, 256, K=2, cached=False)\n",
        "                self.conv2 = SGConv(256, dataset.num_classes, K=2, cached=False)\n",
        "\n",
        "            elif model_type == \"GAT\":\n",
        "                # GAT\n",
        "                self.conv1 = GATConv(dataset.num_features, 8, heads=8, dropout=0.1)\n",
        "                # On the Pubmed dataset, use heads=8 in conv2.\n",
        "                if data_type == \"PubMed\":\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False)\n",
        "                    # self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=8, concat=False, dropout=0.1)\n",
        "                else:\n",
        "                    self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=False)\n",
        "            else:\n",
        "                print(\"Error: No model selected\")\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "            # print(\"xxxxxxx\", x.size())\n",
        "\n",
        "            if model_type == \"SAGE\":\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                # the edge_index here is quite different (it is a list cos we will be passing train_loader). edge index is different based on batch data.\n",
        "                # Note edge_index here is a bipartite graph. meaning all the edges retured are connected\n",
        "                for i, (edge_ind, _, size) in enumerate(edge_index):\n",
        "                    # print(\"iiiiiiiiiiiiiiii\", i)\n",
        "\n",
        "                    edges_raw = edge_ind.cpu().numpy()\n",
        "                    # print(\"edges_raw\", edges_raw)\n",
        "                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                    G = nx.Graph()\n",
        "                    G.add_nodes_from(list(range(\n",
        "                        data.num_nodes)))  # this is set to num_test_Shadow cos that's the max no of nodes since num_test_Shadow = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Shadow\n",
        "                    G.add_edges_from(edges)\n",
        "\n",
        "                    # print(\"x.size(0) shadow forward\", x.size(0))\n",
        "                    # print(\"x.size\", x.size())\n",
        "                    for n in range(0, x.size(\n",
        "                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                        all_nodes.append(n)  # get all nodes\n",
        "                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                        # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "                    # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "                    x_shadow = x[:size[1]]  # Shadow nodes are always placed first.\n",
        "                    x = self.convs[i]((x, x_shadow), edge_ind)\n",
        "                    if i != self.num_layers - 1:\n",
        "                        x = F.relu(x)\n",
        "                        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "                # print(\"Final all nodes and neighbors\", all_node_and_neigbors)\n",
        "                return x.log_softmax(dim=-1), all_node_and_neigbors\n",
        "\n",
        "            else:\n",
        "                # Begin edit data for passing node n it's neghbprs\n",
        "                edges_raw = edge_index.cpu().numpy()\n",
        "                # print(\"edges_raw\", edges_raw)\n",
        "                edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                G = nx.Graph()\n",
        "                G.add_nodes_from(list(range(\n",
        "                    data.num_nodes)))  # this is set to num_test_Shadow cos that's the max no of nodes since num_test_Shadow = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Shadow\n",
        "                G.add_edges_from(edges)\n",
        "\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                # print(\"x.size(0)\", x.size(0))\n",
        "                for n in range(0, x.size(\n",
        "                        0)):  # x.size(0) caters for when u input the full graph for for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                    all_nodes.append(n)  # get all nodes\n",
        "                    # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                    all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                    # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "                # Therefore no need to save nodes. Just loop through\n",
        "\n",
        "                # print(\"all_node_and_neigbors\", all_node_and_neigbors)\n",
        "\n",
        "\n",
        "                x, edge_index = x, edge_index\n",
        "                x = self.conv1(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                # x = F.dropout(x, p=0.5, training=self.training)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "                x = self.conv2(x, edge_index)\n",
        "                # x = F.relu(x)\n",
        "                # x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "                return F.log_softmax(x, dim=1), all_node_and_neigbors\n",
        "\n",
        "        def inference(self, x_all):\n",
        "            # Compute representations of nodes layer by layer, using *all*\n",
        "            # available edges. This leads to faster computation in contrast to\n",
        "            # immediately computing the final representations of each batch.\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                xs = []\n",
        "                all_node_and_neigbors = []\n",
        "                all_nodes = []\n",
        "\n",
        "                for batch_size, n_id, adj in all_graph_loader:\n",
        "                    edge_index, _, size = adj.to(device)\n",
        "\n",
        "                    x = x_all[n_id].to(device)\n",
        "\n",
        "                    edges_raw = edge_index.cpu().numpy()\n",
        "                    # print(\"edges_raw inference\", edges_raw)\n",
        "                    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "\n",
        "                    G = nx.Graph()\n",
        "                    G.add_nodes_from(list(range(\n",
        "                        data.num_nodes)))  # this is set to num_test_Shadow cos that's the max no of nodes since num_test_Shadow = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Shadow\n",
        "                    G.add_edges_from(edges)\n",
        "\n",
        "                    # print(\"x.size(0)\", x.size(0))\n",
        "                    # print(\"x.size inference\", x.size())\n",
        "                    for n in range(0, x.size(\n",
        "                            0)):  # x.size(0) caters for when u input the full graph. This is cos the only the edge_index for those nodes with connectivity will be returned. So this allows getting other nodes without connection.   for n in set(edges_raw[0,:]):  # take first row info of the adjacency matrix. This will give all nodes in the graph!\n",
        "                        all_nodes.append(n)  # get all nodes\n",
        "                        # all_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "                        all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "                        # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "\n",
        "                    # print(\"all_node_and_neigbors inference\", all_node_and_neigbors)\n",
        "\n",
        "                    x_shadow = x[:size[1]]\n",
        "                    x = self.convs[i]((x, x_shadow), edge_index)\n",
        "                    if i != self.num_layers - 1:\n",
        "                        x = F.relu(x)\n",
        "                    xs.append(x.cpu())\n",
        "\n",
        "                x_all = torch.cat(xs, dim=0)\n",
        "\n",
        "            return F.log_softmax(x_all, dim=1), all_node_and_neigbors\n",
        "\n",
        "\n",
        "    '''\n",
        "    ##################################### Data Processing Inductive Split ######################################\n",
        "    '''\n",
        "\n",
        "\n",
        "    def get_inductive_spilt(data, num_classes, num_train_Train_per_class, num_train_Shadow_per_class, num_test_Target,\n",
        "                            num_test_Shadow):\n",
        "        # -----------------------------------------------------------------------\n",
        "        # target_train, target_out\n",
        "        # shadow_train, shadow_out\n",
        "        '''\n",
        "        Randomly choose 'num_train_Train_per_class' and 'num_train_Shadow_per_class' per classes for training Target and shadow models respectively\n",
        "        Random choose 'num_test_Target' and 'num_test_Shadow' for testing (out data) Target and shadow models respectively\n",
        "\n",
        "        '''\n",
        "\n",
        "        # convert all label to list\n",
        "        label_idx = data.y.numpy().tolist()\n",
        "        print(\"label_idx\", len(label_idx))\n",
        "        target_train_idx = []\n",
        "        shadow_train_idx = []\n",
        "        # for i in range(num_classes):\n",
        "        #     c = [x for x in range(len(label_idx)) if label_idx[x] == i]\n",
        "        #     print(\"c\", len(c)) #the min is 180 which is 7th class\n",
        "        #     sample = random.sample(range(c),num_train_Train_per_class)\n",
        "        #     target_train_idx.extend(sample)\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            idx = (data.y == c).nonzero().view(-1)\n",
        "            sample_train_idx = idx[torch.randperm(idx.size(0))]\n",
        "            sample_target_train_idx = sample_train_idx[:num_train_Train_per_class]\n",
        "            target_train_idx.extend(sample_target_train_idx)\n",
        "\n",
        "            print(\"idx.size(0)\", idx.size(0))  # this is the total number of data in each class\n",
        "\n",
        "            # Ensure they don't over lap\n",
        "            # sample_shadow_train_idx = idx[torch.randperm(idx.size(0))[num_train_Train_per_class:num_train_Train_per_class + num_train_Shadow_per_class]]\n",
        "            sample_shadow_train_idx = sample_train_idx[\n",
        "                                      num_train_Train_per_class:num_train_Train_per_class + num_train_Shadow_per_class]\n",
        "            shadow_train_idx.extend(sample_shadow_train_idx)\n",
        "\n",
        "        print(\"shadow_train_idx\", len(shadow_train_idx))\n",
        "        print(\"Target_train_idx\", len(target_train_idx))\n",
        "\n",
        "        others = [x for x in range(len(label_idx)) if x not in set(target_train_idx) and x not in set(shadow_train_idx)]\n",
        "        # print(\"others\",others)\n",
        "        print(\"done others\")\n",
        "        target_test_idx = random.sample(others, num_test_Target)\n",
        "        print(\"done target test\")\n",
        "        shadow_test = [x for x in others if x not in set(target_test_idx)]\n",
        "        shadow_test_idx = random.sample(shadow_test, num_test_Shadow)\n",
        "        print(\"done shadow test\")\n",
        "\n",
        "        print(\"target_test_idx\", target_test_idx)\n",
        "        print(\"shadow_test_idx\", shadow_test_idx)\n",
        "\n",
        "        # ----------set values for mask--------------------------------\n",
        "\n",
        "        # This is done cos in SAGE, we use the mask to create loader. But in other GNN, we only use it to get the sum of elements\n",
        "        if model_type == \"SAGE\":\n",
        "            target_train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "            for i in target_train_idx:\n",
        "                target_train_mask[i] = 1\n",
        "\n",
        "            shadow_train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "            for i in shadow_train_idx:\n",
        "                shadow_train_mask[i] = 1\n",
        "\n",
        "        else:\n",
        "            # Other GNN\n",
        "            # Also changed this so as to conform with the shape of others. No, this uncommented one is better\n",
        "            target_train_mask = torch.ones(len(target_train_idx), dtype=torch.uint8)\n",
        "            shadow_train_mask = torch.ones(len(shadow_train_idx), dtype=torch.uint8)\n",
        "\n",
        "        # # Also changed this so as to conform with the shape of others. No, this uncommented one is better\n",
        "        # target_train_mask = torch.ones(len(target_train_idx), dtype=torch.uint8)\n",
        "        # shadow_train_mask = torch.ones(len(shadow_train_idx), dtype=torch.uint8)\n",
        "\n",
        "        # print(\"target_train_mask.shape\", target_train_mask.shape)\n",
        "\n",
        "        # ---test-mask---\n",
        "        target_test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "        for i in target_test_idx:\n",
        "            target_test_mask[i] = 1\n",
        "        # ---val-mask-----\n",
        "        shadow_test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
        "        for i in shadow_test_idx:\n",
        "            shadow_test_mask[i] = 1\n",
        "\n",
        "        '''\n",
        "        get all nodes and corresponding edge_index information\n",
        "        '''\n",
        "        # This is for creating subgraphs\n",
        "\n",
        "        # For target\n",
        "        target_x_inductive = data.x[target_train_idx]\n",
        "        target_y_inductive = data.y[target_train_idx]\n",
        "        target_edge_index_inductive, _ = subgraph(target_train_idx, data.edge_index)\n",
        "\n",
        "        # For shadow\n",
        "        shadow_x_inductive = data.x[shadow_train_idx]\n",
        "        shadow_y_inductive = data.y[shadow_train_idx]\n",
        "        shadow_edge_index_inductive, _ = subgraph(shadow_train_idx, data.edge_index)\n",
        "\n",
        "        '''\n",
        "        in this part use a vertex_map to get a correct target_edge_index_inductive\n",
        "        '''\n",
        "        # ---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*\n",
        "        '''\n",
        "        get new edge_index information, because some nodes were removed from orginal nodes set. so there\n",
        "        are some edge_index that will disappear. If we dont do that, it will cause error: out of index 193\n",
        "        '''\n",
        "\n",
        "        target_vertex_map = {}\n",
        "        ind = -1\n",
        "        for i in range(data.num_nodes):\n",
        "            if i in target_train_idx:\n",
        "                ind += 1\n",
        "                target_vertex_map[i] = ind\n",
        "        for i in range(target_edge_index_inductive.shape[1]):\n",
        "            target_edge_index_inductive[0, i] = target_vertex_map[target_edge_index_inductive[0, i].tolist()]\n",
        "            target_edge_index_inductive[1, i] = target_vertex_map[target_edge_index_inductive[1, i].tolist()]\n",
        "\n",
        "        shadow_vertex_map = {}\n",
        "        ind = -1\n",
        "        for i in range(data.num_nodes):\n",
        "            if i in shadow_train_idx:\n",
        "                ind += 1\n",
        "                shadow_vertex_map[i] = ind\n",
        "        for i in range(shadow_edge_index_inductive.shape[1]):\n",
        "            shadow_edge_index_inductive[0, i] = shadow_vertex_map[shadow_edge_index_inductive[0, i].tolist()]\n",
        "            shadow_edge_index_inductive[1, i] = shadow_vertex_map[shadow_edge_index_inductive[1, i].tolist()]\n",
        "\n",
        "        # ---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*\n",
        "\n",
        "        # All graph data\n",
        "        all_x = data.x\n",
        "        all_y = data.y\n",
        "        all_edge_index = data.edge_index\n",
        "\n",
        "        '''\n",
        "        now we create a New data instances for save the all data with that we do inductive learning tasks\n",
        "        '''\n",
        "\n",
        "        data = Data(target_x=target_x_inductive, target_edge_index=target_edge_index_inductive,\n",
        "                    target_y=target_y_inductive,\n",
        "                    shadow_x=shadow_x_inductive, shadow_edge_index=shadow_edge_index_inductive,\n",
        "                    shadow_y=shadow_y_inductive,\n",
        "                    target_train_mask=target_train_mask, shadow_train_mask=shadow_train_mask, all_x=all_x,\n",
        "                    all_edge_index=all_edge_index, all_y=all_y, target_test_mask=target_test_mask,\n",
        "                    shadow_test_mask=shadow_test_mask)\n",
        "\n",
        "        # # flipping the shadow to target and target to shadow\n",
        "        # data = Data(target_x=shadow_x_inductive,target_edge_index=shadow_edge_index_inductive,target_y=shadow_y_inductive,\n",
        "        #                 shadow_x=target_x_inductive, shadow_edge_index=target_edge_index_inductive,shadow_y=target_y_inductive,\n",
        "        #                 target_train_mask=shadow_train_mask,shadow_train_mask=target_train_mask,\n",
        "        #                 all_x=all_x,all_edge_index=all_edge_index,all_y=all_y,\n",
        "        #                 target_test_mask=shadow_test_mask, shadow_test_mask=target_test_mask)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def get_train_acc(data_new, pred, isTarget=True):\n",
        "        if isTarget:\n",
        "            # Removed train mask cos u r testing on the subgraph only tho\n",
        "            # train_acc = pred.eq(data_new.target_y[data_new.target_train_mask]).sum().item() / data_new.target_train_mask.sum().item()\n",
        "            train_acc = pred.eq(data_new.target_y).sum().item() / data_new.target_train_mask.sum().item()\n",
        "        else:\n",
        "            # train_acc = pred.eq(data_new.target_y[data_new.shadow_train_mask]).sum().item() / data_new.shadow_train_mask.sum().item()\n",
        "            train_acc = pred.eq(data_new.shadow_y).sum().item() / data_new.shadow_train_mask.sum().item()\n",
        "        # I changed this so as to get the prediction when u test on the train dataset cos all_y has all y. No this approach is not good. The apporach above is accurate\n",
        "        # train_acc = pred.eq(data_new.all_y[data_new.target_train_mask]).sum().item() / data_new.target_train_mask.sum().item()\n",
        "        return train_acc\n",
        "\n",
        "\n",
        "    def get_test_acc(data_new, pred, isTarget=True):\n",
        "        if isTarget:\n",
        "            test_acc = pred.eq(\n",
        "                data_new.all_y[data_new.target_test_mask]).sum().item() / data_new.target_test_mask.sum().item()\n",
        "        else:\n",
        "            test_acc = pred.eq(\n",
        "                data_new.all_y[data_new.shadow_test_mask]).sum().item() / data_new.shadow_test_mask.sum().item()\n",
        "        return test_acc\n",
        "\n",
        "\n",
        "    def get_marco_f1(data_new, pred_labels, true_labels, label_list):\n",
        "        # f1_marco = f1_score(true_labels,pred_labels,label_list,average='macro')\n",
        "        f1_marco = f1_score(true_labels, pred_labels, average='macro')\n",
        "        return f1_marco\n",
        "\n",
        "\n",
        "    def get_micro_f1(data_new, pred_labels, true_labels, label_list):\n",
        "        # f1_micro = f1_score(true_labels,pred_labels,label_list,average='micro')\n",
        "        f1_micro = f1_score(true_labels, pred_labels, average='micro')\n",
        "        return f1_micro\n",
        "\n",
        "\n",
        "    '''\n",
        "    ########################## End Data Processing Inductive Split ###########################\n",
        "    '''\n",
        "\n",
        "    ''' Data initalization '''\n",
        "\n",
        "    # --- create labels_list---\n",
        "    label_list = [x for x in range(dataset.num_classes)]\n",
        "\n",
        "    # convert all label to list\n",
        "    label_idx = data.y.numpy().tolist()\n",
        "\n",
        "    data_new = get_inductive_spilt(data, dataset.num_classes, num_train_Train_per_class, num_train_Shadow_per_class,\n",
        "                                   num_test_Target, num_test_Shadow)\n",
        "\n",
        "    print(\"data\", data)\n",
        "    print(\"data new\", data_new)\n",
        "    print(\"data_new.shadow_test_mask.sum()\", data_new.shadow_test_mask.sum())\n",
        "    print(\"data_new.target_test_mask.sum()\", data_new.target_test_mask.sum())\n",
        "\n",
        "    print(dataset.num_classes)\n",
        "\n",
        "    bool_tensor = torch.ones(num_test_Target, dtype=torch.bool)\n",
        "\n",
        "    target_train_loader = NeighborSampler(data_new.target_edge_index, node_idx=bool_tensor,\n",
        "                                          sizes=[25, 10], num_nodes=num_test_Target, batch_size=64,\n",
        "                                          shuffle=False)  # solution: node_idx to none since we wanna consider all nodes in the subgraph. Also num_test_Target is used cos its the total of nodes\n",
        "    # print(\"data_new.target_edge_index\", data_new.target_edge_index.shape)\n",
        "    # print(\"data_new.target_x\", data_new.target_x.shape)\n",
        "    # print(\"data_new.target_train_mask\", data_new.target_train_mask)\n",
        "\n",
        "    shadow_train_loader = NeighborSampler(data_new.shadow_edge_index, node_idx=bool_tensor,\n",
        "                                          sizes=[25, 10], num_nodes=num_test_Shadow, batch_size=64, shuffle=False)\n",
        "\n",
        "    # This is left cos we are training on the full graph. i.e TSTF\n",
        "    all_graph_loader = NeighborSampler(data_new.all_edge_index, node_idx=None, sizes=[-1],\n",
        "                                       batch_size=1024, num_nodes=data.num_nodes, shuffle=False)\n",
        "\n",
        "    ''' Model initialization '''\n",
        "\n",
        "    target_model = TargetModel(dataset)\n",
        "    shadow_model = ShadowModel(\n",
        "        dataset)  # Defining it as TargetModel(dataset) still produces the same result. I explicitly redefined each model for clarity\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    data_new = data_new.to(device)\n",
        "\n",
        "    target_model = target_model.to(device)\n",
        "\n",
        "    # # To reset the model to default\n",
        "    # for name, module in target_model.named_children():\n",
        "    #     print('resetting ', name)\n",
        "    #     module.reset_parameters()\n",
        "\n",
        "    shadow_model = shadow_model.to(device)\n",
        "\n",
        "    print(\"model\", target_model)\n",
        "    if model_type == \"SAGE\":\n",
        "        # better attack but slighly less test acc\n",
        "        if data_type == \"PubMed\":\n",
        "            target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "            shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.0001)  # 01\n",
        "        else:\n",
        "            target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "            shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.001)  # 01\n",
        "    else:\n",
        "        target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        shadow_optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.0001)\n",
        "\n",
        "    '''\n",
        "    Train and Test function for model\n",
        "    '''\n",
        "\n",
        "\n",
        "    # --*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*\n",
        "    # #----------------------------- TRAIN FUCNTION---------------------------\n",
        "    def train(model, optimizer, isTarget=True):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if isTarget:\n",
        "            out, nodes_and_neighbors = model(data_new.target_x,\n",
        "                                             data_new.target_edge_index)\n",
        "            loss = F.nll_loss(out, data_new.target_y)\n",
        "        else:\n",
        "            out, nodes_and_neighbors = model(data_new.shadow_x, data_new.shadow_edge_index)\n",
        "            loss = F.nll_loss(out, data_new.shadow_y)\n",
        "\n",
        "        pred = torch.exp(out)\n",
        "        # print(\"pred\", pred)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # approximate accuracy\n",
        "        if isTarget:\n",
        "            train_loss = loss.item() / int(data_new.target_train_mask.sum())\n",
        "            total_correct = int(pred.argmax(dim=-1).eq(data_new.target_y).sum()) / int(data_new.target_train_mask.sum())\n",
        "            # np.savetxt(\"save_Target_Train\", pred.cpu().detach().numpy())\n",
        "        else:\n",
        "            train_loss = loss.item() / int(data_new.shadow_train_mask.sum())\n",
        "            total_correct = int(pred.argmax(dim=-1).eq(data_new.shadow_y).sum()) / int(data_new.shadow_train_mask.sum())\n",
        "            # np.savetxt(\"save_Shadow_Train\", pred.cpu().detach().numpy())\n",
        "        return total_correct, train_loss\n",
        "\n",
        "\n",
        "    def train_SAGE(model, optimizer, isTarget=True):\n",
        "        # print(\"================ Begin SAGE Train ==================\")\n",
        "        model.train()\n",
        "\n",
        "        if isTarget:\n",
        "            total_loss = total_correct = 0\n",
        "            for batch_size, n_id, adjs in target_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs\", adjs)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                out, nodes_and_neighbors = model(data_new.target_x[n_id], adjs)\n",
        "                # print(\"out traaaaain\", out.shape)\n",
        "                loss = F.nll_loss(out, data_new.target_y[n_id[:batch_size]])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += float(loss)\n",
        "                total_correct += int(out.argmax(dim=-1).eq(data_new.target_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            loss = total_loss / len(target_train_loader)\n",
        "            approx_acc = total_correct / int(data_new.target_train_mask.sum())\n",
        "\n",
        "        else:\n",
        "            # Shadow training\n",
        "            total_loss = total_correct = 0\n",
        "            for batch_size, n_id, adjs in shadow_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs\", adjs)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                out, nodes_and_neighbors = model(data_new.shadow_x[n_id], adjs)\n",
        "                # print(\"out traaaaain shadow\", out.shape)\n",
        "                loss = F.nll_loss(out, data_new.shadow_y[n_id[:batch_size]])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += float(loss)\n",
        "                total_correct += int(out.argmax(dim=-1).eq(data_new.shadow_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            loss = total_loss / len(shadow_train_loader)\n",
        "            approx_acc = total_correct / int(data_new.shadow_train_mask.sum())\n",
        "\n",
        "        # print(\"================ End SAGE Train ==================\")\n",
        "        return approx_acc, loss\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------------------------------------------------\n",
        "    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*\n",
        "    # -------------------------------- TEST FUNCTION -----------------------------------------\n",
        "    def test(model, isTarget=True):\n",
        "        model.eval()\n",
        "        # Also changed this to give true test using full graph. This will give the true train result--No it wont. See comment below\n",
        "\n",
        "        # This is a better n accurate approach\n",
        "        if isTarget:\n",
        "\n",
        "            '''InTrain Target'''\n",
        "            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless\n",
        "            # pred_Intrain = model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask].max(1)[1]\n",
        "            pred, nodes_and_neighbors = model(data_new.target_x,\n",
        "                                              data_new.target_edge_index)\n",
        "            pred_Intrain = pred.max(1)[1].to(device)\n",
        "            # Actual probabilities\n",
        "            # pred_Intrain_ps = torch.exp(model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask])\n",
        "            pred_Intrain_ps = torch.exp(pred)\n",
        "            np.savetxt(save_target_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "\n",
        "            np.save(save_target_InTrain_nodes_neigbors, nodes_and_neighbors)\n",
        "            # print(\"nodes_and_neighborsnodes_and_neighbors\", nodes_and_neighbors) # 630\n",
        "\n",
        "            # print(\"End InTrain for target\")\n",
        "\n",
        "            '''OutTrain Target'''\n",
        "            # This is where the difference is\n",
        "\n",
        "            # preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)[data_new.target_test_mask]\n",
        "            preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)  # [data_new.target_test_mask]\n",
        "            nodes_and_neighbors = np.array(nodes_and_neighbors)\n",
        "            # print(\"type(preds)\", type(preds), \"type(nodes_and_neighbors)\", type(nodes_and_neighbors))\n",
        "            # print(\"preds.shape\", preds.shape)\n",
        "            # print(\"nodes_and_neighbors.shape\", nodes_and_neighbors) # 2708\n",
        "\n",
        "            preds = preds[data_new.target_test_mask]\n",
        "            # print(\"baba1\")\n",
        "            mask = data_new.target_test_mask.gt(\n",
        "                0)  # trick to convert to true false for masking nodes_and_neighbors cos its a numpy array\n",
        "            mask = mask.cpu().numpy()\n",
        "            # print(\"data_new.target_test_mask\", data_new.target_test_mask)\n",
        "            # print(\"mask\", mask)\n",
        "            # print(nodes_and_neighbors)\n",
        "            nodes_and_neighbors = nodes_and_neighbors[mask]\n",
        "            # print(\"baba2\")\n",
        "\n",
        "            pred_out = preds.max(1)[1].to(device)\n",
        "            pred_out_ps = torch.exp(preds)\n",
        "\n",
        "            # Increment each node in dict by (num_test_Target) e.g 630 as well as the values for ease of \"continuing\" the graph since each graph is recreated\n",
        "            # This will allow shuffling of posteriors in the attack model not to loose the node info\n",
        "            # print(\"newwwwwwww\", nodes_and_neighbors)\n",
        "\n",
        "            incremented_nodes_and_neighbors = []  # {}\n",
        "            # for i in range(len(nodes_and_neighbors)):\n",
        "            #     # print(nodes_and_neighbors[i])  # list\n",
        "            #     res = [x + num_test_Target for x in nodes_and_neighbors[i]]\n",
        "            #     incremented_nodes_and_neighbors[i+num_test_Target] = res\n",
        "\n",
        "            for i in range(len(nodes_and_neighbors)):  # range 630\n",
        "                # print(\"nodes_and_neighbors[i]\", nodes_and_neighbors[i])  # list with num_nodes. nodes_and_neighbors[i] [2694 list([431, 2695])]\n",
        "                # print(nodes_and_neighbors[i][0])\n",
        "                # print(nodes_and_neighbors[i][1])\n",
        "\n",
        "                # print(\"num_test_Target\", num_test_Target)\n",
        "\n",
        "                res = nodes_and_neighbors[i][\n",
        "                    1]  # [x + num_test_Target for x in nodes_and_neighbors[i][1]] #increment from 630 num_test_Target\n",
        "                res_0 = nodes_and_neighbors[i][0]  # nodes_and_neighbors[i][0] + data.num_nodes\n",
        "                incremented_nodes_and_neighbors.append((res_0, res))\n",
        "\n",
        "\n",
        "\n",
        "            # print(\"incremented_nodes_and_neighbors\", incremented_nodes_and_neighbors)\n",
        "\n",
        "            np.save(save_target_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)\n",
        "\n",
        "            # Simply the nodes are the last column of the \"posterior\"\n",
        "            nodes = []\n",
        "            for i in range(0, len(incremented_nodes_and_neighbors)):\n",
        "                # print(\"len(incremented_nodes_and_neighbors)\", len(incremented_nodes_and_neighbors)) # 630\n",
        "                # print(\"incremented_nodes_and_neighbors[i][0]\", incremented_nodes_and_neighbors[i][0])\n",
        "                nodes.append(incremented_nodes_and_neighbors[i][0])\n",
        "            nodes = np.array(nodes)\n",
        "            # print(\"nodesnodes\",nodes)\n",
        "            preds_and_nodes = np.column_stack((pred_out_ps.cpu().detach().numpy(), nodes))\n",
        "            # print(\"preds_and_nodes\", preds_and_nodes.shape)\n",
        "            np.savetxt(save_target_OutTrain, preds_and_nodes)  # pred_out_ps.cpu().detach().numpy()\n",
        "\n",
        "            # print(\"End OutTrain for target\")\n",
        "\n",
        "            pred_labels = pred_out.tolist()\n",
        "            true_labels = data_new.all_y[data_new.target_test_mask].tolist()\n",
        "\n",
        "            # The train accuracy is not on the full graph. It's similar to approx_train_acc\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out)\n",
        "\n",
        "        else:\n",
        "\n",
        "            '''InTrain Shadow'''\n",
        "            # pred_Intrain = model(data_new.shadow_x, data_new.shadow_edge_index)[data_new.shadow_train_mask].max(1)[1]\n",
        "            pred, nodes_and_neighbors = model(data_new.shadow_x, data_new.shadow_edge_index)\n",
        "            pred_Intrain = pred.max(1)[1].to(device)\n",
        "            # Actual probabilities\n",
        "            # pred_Intrain_ps = torch.exp(model(data_new.shadow_x, data_new.shadow_edge_index)[data_new.shadow_train_mask])\n",
        "            pred_Intrain_ps = torch.exp(pred)\n",
        "            np.savetxt(save_shadow_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "\n",
        "            '''OutTrain Shadow'''\n",
        "\n",
        "            # preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)[data_new.shadow_test_mask]\n",
        "            preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)  # [data_new.shadow_test_mask]\n",
        "            nodes_and_neighbors = np.array(nodes_and_neighbors)\n",
        "            # print(\"type(preds)\", type(preds), \"type(nodes_and_neighbors)\", type(nodes_and_neighbors))\n",
        "            # print(\"preds.shape\", preds.shape)\n",
        "            # print(\"nodes_and_neighbors.shape\", nodes_and_neighbors.shape)\n",
        "\n",
        "            preds = preds[data_new.shadow_test_mask]\n",
        "            # print(\"baba1\")\n",
        "            mask = data_new.shadow_test_mask.gt(0)  # trick to\n",
        "            mask = mask.cpu().numpy()\n",
        "            # print(\"data_new.shadow_test_mask\", data_new.shadow_test_mask)\n",
        "            # print(\"mask\", mask.shape)\n",
        "            # print(nodes_and_neighbors)\n",
        "            nodes_and_neighbors = nodes_and_neighbors[mask]\n",
        "            # print(\"baba2\")\n",
        "\n",
        "            # preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)[data_new.shadow_test_mask]\n",
        "            pred_out = preds.max(1)[1].to(device)\n",
        "            pred_out_ps = torch.exp(preds)\n",
        "            np.savetxt(save_shadow_OutTrain, pred_out_ps.cpu().detach().numpy())\n",
        "\n",
        "            pred_labels = pred_out.tolist()\n",
        "            true_labels = data_new.all_y[data_new.shadow_test_mask].tolist()\n",
        "\n",
        "            # The train accuracy is not on the full graph. It's similar to approx_train_acc\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain, False)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out, False)\n",
        "\n",
        "        # pred_Intrain = model(data_new.all_x,data_new.all_edge_index)[data_new.target_train_mask].max(1)[1]\n",
        "        # # Actual probabilities\n",
        "        # pred_Intrain_ps = torch.exp(model(data_new.all_x,data_new.all_edge_index)[data_new.target_train_mask])\n",
        "\n",
        "        # print(\"posteriors\", pred_Intrain)\n",
        "\n",
        "        # The f1 measures are on test dataset\n",
        "        f1_marco = get_marco_f1(data_new, pred_labels, true_labels, label_list)\n",
        "        f1_micro = get_micro_f1(data_new, pred_labels, true_labels, label_list)\n",
        "\n",
        "        return train_acc, test_acc, f1_marco, f1_micro\n",
        "\n",
        "\n",
        "    def test_SAGE(model, isTarget=True):\n",
        "        # print(\"****************** SAGE test begin *************************\")\n",
        "        model.eval()\n",
        "        # Also changed this to give true test using full graph. This will give the true train result--No it wont. See comment below\n",
        "\n",
        "        # This is a better n accurate approach\n",
        "        if isTarget:\n",
        "\n",
        "            '''InTrain Target'''\n",
        "            # Removed train mask cos u r training on the subgraph not the full graph. Therefore, train mask is useless\n",
        "            # pred_Intrain = model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask].max(1)[1]\n",
        "\n",
        "            pred = []\n",
        "            nodes_and_neighbors = []\n",
        "\n",
        "            total_target_train_correct = 0\n",
        "\n",
        "            for batch_size, n_id, adjs in target_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs test\", adjs)\n",
        "                # print(\"n_id\",len(n_id))\n",
        "\n",
        "                out, node_and_neigh = model(data_new.target_x[n_id], adjs)\n",
        "                out = torch.exp(out)\n",
        "                # print(\"out test\", out.shape)\n",
        "                pred.append(out.cpu())\n",
        "                nodes_and_neighbors.append(node_and_neigh)\n",
        "\n",
        "                total_target_train_correct += int(out.argmax(dim=-1).eq(data_new.target_y[n_id[:batch_size]]).sum())\n",
        "\n",
        "            target_train_acc = total_target_train_correct / int(\n",
        "                data_new.target_train_mask.sum())  # similar to get_train_acc()\n",
        "            # print(\"target_train_acc\", target_train_acc)\n",
        "\n",
        "            # Need to concat all preds cos it's per batch\n",
        "            pred_all_inTrain = torch.cat(pred, dim=0)\n",
        "\n",
        "            # pred, nodes_and_neighbors = model(data_new.target_x,data_new.target_edge_index)\n",
        "            pred_Intrain = pred_all_inTrain.max(1)[1].to(device)\n",
        "            # # Actual probabilities\n",
        "            # # pred_Intrain_ps = torch.exp(model(data_new.target_x,data_new.target_edge_index)[data_new.target_train_mask])\n",
        "            pred_Intrain_ps = pred_all_inTrain  # torch.exp(pred)\n",
        "            np.savetxt(save_target_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "            #\n",
        "            np.save(save_target_InTrain_nodes_neigbors, nodes_and_neighbors)\n",
        "            # print(\"nodes_and_neighborsnodes_and_neighbors\", nodes_and_neighbors) # 630\n",
        "\n",
        "            # print(\"End InTrain for target\")\n",
        "\n",
        "            '''OutTrain Target'''\n",
        "            # This is where the difference is\n",
        "            out, nodes_and_neighbors = model.inference(data_new.all_x)\n",
        "            # print(\"out OutTrain Target\", out.shape)\n",
        "\n",
        "            y_true = data_new.all_y.cpu().unsqueeze(-1)\n",
        "            y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "            # print(y_pred[data_new.target_train_mask])\n",
        "\n",
        "            # results = []\n",
        "            # for mask in [data_new.target_train_mask, data_new.target_test_mask]:\n",
        "            #     results += [int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
        "\n",
        "            # train_acc = int(y_pred[data_new.target_train_mask].eq(y_true[data_new.target_train_mask]).sum()) / int(data_new.target_train_mask.sum())\n",
        "            # test_acc = int(y_pred[data_new.target_test_mask].eq(y_true[data_new.target_test_mask]).sum()) / int(data_new.target_test_mask.sum())\n",
        "\n",
        "            # # preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)[data_new.target_test_mask]\n",
        "            # preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)#[data_new.target_test_mask]\n",
        "            nodes_and_neighbors = np.array(nodes_and_neighbors)\n",
        "            # print(\"type(preds)\", type(preds), \"type(nodes_and_neighbors)\", type(nodes_and_neighbors))\n",
        "            # print(\"preds.shape\", preds.shape)\n",
        "            # print(\"nodes_and_neighbors.shape\", nodes_and_neighbors.shape) # 2708\n",
        "            #\n",
        "            preds = out[data_new.target_test_mask]\n",
        "            # print(\"predsssssssssssssssss\", torch.exp(preds).shape)\n",
        "            mask = data_new.target_test_mask.gt(\n",
        "                0)  # trick to convert to true false for masking nodes_and_neighbors cos its a numpy array\n",
        "            mask = mask.cpu().numpy()\n",
        "            # print(\"data_new.target_test_mask\", data_new.target_test_mask)\n",
        "            # print(\"mask\", mask)\n",
        "            # print(\"nodes_and_neighborsnodes_and_neighbors\", nodes_and_neighbors)\n",
        "\n",
        "            nodes_and_neighbors = nodes_and_neighbors[\n",
        "                                  :data.num_nodes]\n",
        "\n",
        "            nodes_and_neighbors = nodes_and_neighbors[mask]\n",
        "            # print(\"baba2\")\n",
        "\n",
        "            pred_out = preds.max(1)[1].to(device)\n",
        "            pred_out_ps = torch.exp(preds)\n",
        "            #\n",
        "            #\n",
        "            # Increment each node in dict by (num_test_Target) e.g 630 as well as the values for ease of \"continuing\" the graph since each graph is recreated\n",
        "            # # This will allow shuffling of posteriors in the attack model not to loose the node info\n",
        "            # print(\"newwwwwwww test\", nodes_and_neighbors)\n",
        "\n",
        "            incremented_nodes_and_neighbors = []  # {}\n",
        "            # # for i in range(len(nodes_and_neighbors)):\n",
        "            # #     # print(nodes_and_neighbors[i])  # list\n",
        "            # #     res = [x + num_test_Target for x in nodes_and_neighbors[i]]\n",
        "            # #     incremented_nodes_and_neighbors[i+num_test_Target] = res\n",
        "            #\n",
        "            for i in range(len(nodes_and_neighbors)):\n",
        "                # print(nodes_and_neighbors[i])  # list\n",
        "                # print(nodes_and_neighbors[i][0])\n",
        "                # print(nodes_and_neighbors[i][1])\n",
        "\n",
        "                res = [x + num_test_Target for x in nodes_and_neighbors[i][1]]  # increment from 630 num_test_Target\n",
        "                res_0 = nodes_and_neighbors[i][0] + data.num_nodes\n",
        "                incremented_nodes_and_neighbors.append((res_0, res))\n",
        "\n",
        "\n",
        "            # print(\"incremented_nodes_and_neighbors\", incremented_nodes_and_neighbors)\n",
        "\n",
        "            np.save(save_target_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)\n",
        "\n",
        "            # Need to save the node info along side for this one cos this is no more sequential cos of the masking. To be used in the target_data_for_testing_outTrain in attack\n",
        "            # Simply the nodes are the last column of the \"posterior\"\n",
        "            nodes = []\n",
        "            for i in range(0, len(incremented_nodes_and_neighbors)):\n",
        "                nodes.append(incremented_nodes_and_neighbors[i][0])\n",
        "            nodes = np.array(nodes)\n",
        "            # print(\"nodesnodes\",nodes)\n",
        "            preds_and_nodes = np.column_stack((pred_out_ps.cpu().detach().numpy(), nodes))\n",
        "            # print(\"preds_and_nodes test\", preds_and_nodes.shape)\n",
        "            np.savetxt(save_target_OutTrain, preds_and_nodes)  # pred_out_ps.cpu().detach().numpy()\n",
        "\n",
        "            # print(\"End OutTrain for target\")\n",
        "            #\n",
        "            #\n",
        "            #\n",
        "            pred_labels = pred_out.tolist()\n",
        "            true_labels = data_new.all_y[data_new.target_test_mask].tolist()\n",
        "\n",
        "            # The train accuracy is not on the full graph. It's similar to approx_train_acc\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out)\n",
        "\n",
        "        else:\n",
        "\n",
        "            '''InTrain Shadow'''\n",
        "            # pred_Intrain = model(data_new.shadow_x, data_new.shadow_edge_index)[data_new.shadow_train_mask].max(1)[1]\n",
        "            pred = []\n",
        "            nodes_and_neighbors = []\n",
        "            for batch_size, n_id, adjs in shadow_train_loader:\n",
        "                # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "                adjs = [adj.to(device) for adj in adjs]\n",
        "                # print(\"adjs test\", adjs)\n",
        "                # print(\"n_id\", len(n_id))\n",
        "\n",
        "                out, node_and_neigh = model(data_new.shadow_x[n_id], adjs)\n",
        "                out = torch.exp(out)\n",
        "                # print(\"out test\", out.shape)\n",
        "                pred.append(out.cpu())\n",
        "                nodes_and_neighbors.append(node_and_neigh)\n",
        "\n",
        "            # Need to concat all preds cos it's per batch\n",
        "            pred_all_inTrain = torch.cat(pred, dim=0)\n",
        "\n",
        "\n",
        "            # pred, nodes_and_neighbors = model(data_new.shadow_x,data_new.shadow_edge_index)\n",
        "            pred_Intrain = pred_all_inTrain.max(1)[1].to(device)\n",
        "            # # Actual probabilities\n",
        "            # # pred_Intrain_ps = torch.exp(model(data_new.shadow_x,data_new.shadow_edge_index)[data_new.shadow_train_mask])\n",
        "            pred_Intrain_ps = pred_all_inTrain  # torch.exp(pred)\n",
        "            # print(\"pred_Intrain_ps.cpu().detach().numpy()\", pred_Intrain_ps.cpu().detach().numpy())\n",
        "            np.savetxt(save_shadow_InTrain, pred_Intrain_ps.cpu().detach().numpy())\n",
        "            #\n",
        "            # np.save(save_shadow_InTrain_nodes_neigbors, nodes_and_neighbors)  # No need to save cos its shadow. We dont need neighbor info\n",
        "            # print(\"nodes_and_neighborsnodes_and_neighbors\", nodes_and_neighbors) # 630\n",
        "\n",
        "            # print(\"End InTrain for shadow\")\n",
        "\n",
        "            '''OutTrain Shadow'''\n",
        "            out, nodes_and_neighbors = model.inference(data_new.all_x)\n",
        "            # print(\"out OutTrain Shadow\", out.shape)\n",
        "\n",
        "            y_true = data_new.all_y.cpu().unsqueeze(-1)\n",
        "            y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "            # print(y_pred[data_new.shadow_train_mask])\n",
        "\n",
        "            # results = []\n",
        "            # for mask in [data_new.shadow_train_mask, data_new.shadow_test_mask]:\n",
        "            #     results += [int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\n",
        "\n",
        "            # train_acc = int(y_pred[data_new.shadow_train_mask].eq(y_true[data_new.shadow_train_mask]).sum()) / int(\n",
        "            #     data_new.shadow_train_mask.sum())\n",
        "            # test_acc = int(y_pred[data_new.shadow_test_mask].eq(y_true[data_new.shadow_test_mask]).sum()) / int(\n",
        "            #     data_new.shadow_test_mask.sum())\n",
        "\n",
        "            # # preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)[data_new.shadow_test_mask]\n",
        "            # preds, nodes_and_neighbors = model(data_new.all_x, data_new.all_edge_index)#[data_new.shadow_test_mask]\n",
        "            nodes_and_neighbors = np.array(nodes_and_neighbors)\n",
        "            # print(\"type(preds)\", type(preds), \"type(nodes_and_neighbors)\", type(nodes_and_neighbors))\n",
        "            # print(\"preds.shape\", preds.shape)\n",
        "            # print(\"nodes_and_neighbors.shape\", nodes_and_neighbors.shape)  # 2708\n",
        "            #\n",
        "            preds = out[data_new.shadow_test_mask]\n",
        "            # print(\"predsssssssssssssssss\", torch.exp(preds).shape)\n",
        "            mask = data_new.shadow_test_mask.gt(\n",
        "                0)  # trick to convert to true false for masking nodes_and_neighbors cos its a numpy array\n",
        "            mask = mask.cpu().numpy()\n",
        "            # print(\"data_new.shadow_test_mask\", data_new.shadow_test_mask)\n",
        "            # print(\"mask\", mask)\n",
        "            # print(\"nodes_and_neighborsnodes_and_neighbors\", nodes_and_neighbors)\n",
        "\n",
        "            nodes_and_neighbors = nodes_and_neighbors[\n",
        "                                  :data.num_nodes]\n",
        "\n",
        "            nodes_and_neighbors = nodes_and_neighbors[mask]\n",
        "            # print(\"baba2\")\n",
        "\n",
        "            pred_out = preds.max(1)[1].to(device)\n",
        "            pred_out_ps = torch.exp(preds)\n",
        "            #\n",
        "            #\n",
        "            incremented_nodes_and_neighbors = []  # {}\n",
        "            # # for i in range(len(nodes_and_neighbors)):\n",
        "            # #     # print(nodes_and_neighbors[i])  # list\n",
        "            # #     res = [x + num_test_Shadow for x in nodes_and_neighbors[i]]\n",
        "            # #     incremented_nodes_and_neighbors[i+num_test_Shadow] = res\n",
        "            #\n",
        "            for i in range(len(nodes_and_neighbors)):\n",
        "                # print(nodes_and_neighbors[i])  # list\n",
        "                # print(nodes_and_neighbors[i][0])\n",
        "                # print(nodes_and_neighbors[i][1])\n",
        "\n",
        "                res = [x + num_test_Shadow for x in nodes_and_neighbors[i][1]]  # increment from 630 num_test_Shadow\n",
        "                res_0 = nodes_and_neighbors[i][0] + data.num_nodes\n",
        "                incremented_nodes_and_neighbors.append((res_0, res))\n",
        "\n",
        "            # print(\"incremented_nodes_and_neighbors\", incremented_nodes_and_neighbors)\n",
        "\n",
        "            # np.save(save_shadow_OutTrain_nodes_neigbors, incremented_nodes_and_neighbors)\n",
        "\n",
        "            # Need to save the node info along side for this one cos this is no more sequential cos of the masking. To be used in the shadow_data_for_testing_outTrain in attack\n",
        "            # Simply the nodes are the last column of the \"posterior\"\n",
        "            nodes = []\n",
        "            for i in range(0, len(incremented_nodes_and_neighbors)):\n",
        "                nodes.append(incremented_nodes_and_neighbors[i][0])\n",
        "            nodes = np.array(nodes)\n",
        "            # print(\"nodesnodes\", nodes)\n",
        "            preds_and_nodes = np.column_stack((pred_out_ps.cpu().detach().numpy(), nodes))\n",
        "            # print(\"preds_and_nodes test\", preds_and_nodes.shape)\n",
        "            np.savetxt(save_shadow_OutTrain,\n",
        "                       pred_out_ps.cpu().detach().numpy())  # pred_out_ps.cpu().detach().numpy(). No need for preds_and_nodes here. It;s not needed. Only applicable in inTrain\n",
        "\n",
        "            # print(\"End OutTrain for shadow\")\n",
        "            #\n",
        "            #\n",
        "            #\n",
        "            pred_labels = pred_out.tolist()\n",
        "            true_labels = data_new.all_y[data_new.shadow_test_mask].tolist()\n",
        "            train_acc = get_train_acc(data_new, pred_Intrain, False)\n",
        "            # Test n val are on full graph\n",
        "            test_acc = get_test_acc(data_new, pred_out, False)\n",
        "\n",
        "        # # Actual probabilities\n",
        "        # pred_Intrain_ps = torch.exp(model(data_new.all_x,data_new.all_edge_index)[data_new.target_train_mask])\n",
        "\n",
        "        # print(\"posteriors\", pred_Intrain)\n",
        "\n",
        "        # The f1 measures are on test dataset\n",
        "        f1_marco = get_marco_f1(data_new, pred_labels, true_labels, label_list)\n",
        "        f1_micro = get_micro_f1(data_new, pred_labels, true_labels, label_list)\n",
        "\n",
        "        # print(\"****************** SAGE test End *************************\")\n",
        "        return train_acc, test_acc, f1_marco, f1_micro\n",
        "\n",
        "\n",
        "    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*\n",
        "    # -----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*-----*\n",
        "    # --------------------------TRAIN PROCESS-----------------------------------------------\n",
        "    best_val_acc = best_val_acc = 0\n",
        "\n",
        "    '''\n",
        "    Training and testing target and shadow models\n",
        "    '''\n",
        "\n",
        "    if model_type == \"SAGE\":\n",
        "        if data_type == \"CiteSeer\" or data_type == \"Cora\":\n",
        "            model_training_epoch = 16  # 301 #16 for CiteSeer n Cora, 101 for PubMed, 301 for Flickr n Reddit\n",
        "        elif data_type == \"PubMed\":\n",
        "            model_training_epoch = 101\n",
        "        else:\n",
        "            model_training_epoch = 301\n",
        "    else:\n",
        "        model_training_epoch = 301  # 301\n",
        "\n",
        "    # Target train\n",
        "    for epoch in range(1, model_training_epoch):\n",
        "        if model_type == \"SAGE\":\n",
        "            approx_train_acc, train_loss = train_SAGE(target_model, target_optimizer)\n",
        "            train_acc, test_acc, marco, micro = test_SAGE(target_model)\n",
        "        else:\n",
        "            approx_train_acc, train_loss = train(target_model, target_optimizer)\n",
        "            train_acc, test_acc, marco, micro = test(target_model)\n",
        "        # print(\"approx train acc\", approx_train_acc, \"train_loss\", train_loss)\n",
        "\n",
        "        # train_acc, test_acc,marco,micro = test(target_model)\n",
        "        # if val_acc > best_val_acc:\n",
        "        #     best_val_acc = val_acc\n",
        "        #     test_acc = tmp_test_acc\n",
        "        #     marco = tmp_marco\n",
        "        #     micro = tmp_micro\n",
        "\n",
        "        # log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        # print(log.format(epoch, train_acc, best_val_acc, test_acc,marco,micro))\n",
        "        log = 'TargetModel Epoch: {:03d}, Approx Train: {:.4f}, Train: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        print(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro))\n",
        "        if epoch == model_training_epoch - 1:\n",
        "            result_file.write(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro) + \"\\n\")\n",
        "\n",
        "    print()\n",
        "    print(\"=========================================================End Target Train ==============================\")\n",
        "\n",
        "    # Shadow train\n",
        "    for epoch in range(1, model_training_epoch):\n",
        "        if model_type == \"SAGE\":\n",
        "            approx_train_acc, train_loss = train_SAGE(shadow_model, shadow_optimizer, False)\n",
        "            train_acc, test_acc, marco, micro = test_SAGE(shadow_model, False)\n",
        "        else:\n",
        "            approx_train_acc, train_loss = train(shadow_model, shadow_optimizer, False)\n",
        "            train_acc, test_acc, marco, micro = test(shadow_model, False)\n",
        "        # print(\"approx train acc\", approx_train_acc, \"train_loss\", train_loss)\n",
        "\n",
        "        # train_acc, test_acc,marco,micro = test(shadow_model)\n",
        "        # if val_acc > best_val_acc:\n",
        "        #     best_val_acc = val_acc\n",
        "        #     test_acc = tmp_test_acc\n",
        "        #     marco = tmp_marco\n",
        "        #     micro = tmp_micro\n",
        "\n",
        "        # log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        # print(log.format(epoch, train_acc, best_val_acc, test_acc,marco,micro))\n",
        "        log = 'ShadowModel Epoch: {:03d}, Approx Train: {:.4f}, Train: {:.4f}, Test: {:.4f},marco: {:.4f},micro: {:.4f}'\n",
        "        print(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro))\n",
        "        if epoch == model_training_epoch - 1:\n",
        "            result_file.write(log.format(epoch, approx_train_acc, train_acc, test_acc, marco, micro) + \"\\n\")\n",
        "\n",
        "    ''' =========================================== ATTACK ============================================== '''\n",
        "\n",
        "    # Use normal python to import posterior files.\n",
        "    # Split posterior data into 80 / 20 (train, test)\n",
        "\n",
        "    # add 2 dataset 2 gether pandas\n",
        "    # Add 0 to 629 (num_test_Target) to train posterior {target_data_for_testing_Intrain} and 630 to 1259 (num_test_Target x 2) to test posterior in attack data\n",
        "\n",
        "    positive_attack_data = pd.read_csv(save_shadow_InTrain, header=None,\n",
        "                                       sep=\" \")  # dataframe #\"posteriorsShadowTrain.txt\"\n",
        "\n",
        "    # target in and out data\n",
        "\n",
        "    target_data_for_testing_Intrain = pd.read_csv(save_target_InTrain, header=None,\n",
        "                                                  sep=\" \")  # dataframe \"posteriorsTargetTrain.txt\"\n",
        "    # Assign 1 to indata\n",
        "    target_data_for_testing_Intrain[\"labels\"] = 1\n",
        "\n",
        "    target_data_for_testing_Intrain['nodeID'] = range(0,\n",
        "                                                      num_test_Target)  # num_test_Target 630 first save\n",
        "\n",
        "    # print(\"target_data_for_testing_Intrain.head(10)\", target_data_for_testing_Intrain.head(10))\n",
        "    # print(\"target_data_for_testing_Intrain.tail(10)\", target_data_for_testing_Intrain.tail(10))\n",
        "\n",
        "    # # randomly select 500\n",
        "    # chosen_idx = np.random.choice(153431, replace=False, size=50000)\n",
        "    # print(chosen_idx)\n",
        "    # positive_attack_data = positive_attack_data.iloc[chosen_idx]\n",
        "\n",
        "    target_data_for_testing_Outtrain_data = pd.read_csv(save_target_OutTrain, header=None,\n",
        "                                                        sep=\" \")  # dataframe \"posteriorsTargetOut.txt\"\n",
        "\n",
        "    target_data_for_testing_Outtrain = target_data_for_testing_Outtrain_data.iloc[:,\n",
        "                                       :-1]  # drop last. To be assigned later as nodeID\n",
        "\n",
        "    target_data_for_testing_Outtrain['nodeID'] = target_data_for_testing_Outtrain_data.iloc[:, -1:].astype(\n",
        "        float).astype(\n",
        "        int)  # selects last column #range(num_test_Target, data.num_nodes+data.num_nodes) #num_test_Target, num_test_Target+num_test_Target\n",
        "    # print(\"target_data_for_testing_Outtrain\", target_data_for_testing_Outtrain)\n",
        "    # print(positive_attack_data.head())\n",
        "\n",
        "    # Assign 0 to outdata\n",
        "    target_data_for_testing_Outtrain[\"labels\"] = 0\n",
        "\n",
        "    # Assign 1 to training data\n",
        "    positive_attack_data[\"labels\"] = 1\n",
        "\n",
        "    print(\"positive_attack_data.shape\", positive_attack_data.shape)\n",
        "\n",
        "    negative_attack_data = pd.read_csv(save_shadow_OutTrain, header=None, sep=\" \")  # \"posteriorsShadowOut.txt\"\n",
        "\n",
        "    # # randomly select 140\n",
        "    # chosen_idx = np.random.choice(55703, replace=False, size=50000)\n",
        "    # print(chosen_idx)\n",
        "    # negative_attack_data = negative_attack_data.iloc[chosen_idx]\n",
        "\n",
        "    # print(negative_attack_data.head())\n",
        "\n",
        "    # Assign 0 to out data\n",
        "    negative_attack_data[\"labels\"] = 0\n",
        "    print(\"negative_attack_data.shape\", negative_attack_data.shape)\n",
        "\n",
        "    # Combine to single dataframe\n",
        "\n",
        "    # combine them together\n",
        "    attack_data_combo = [positive_attack_data, negative_attack_data]\n",
        "    attack_data = pd.concat(attack_data_combo)\n",
        "\n",
        "    target_data_for_testing_InAndOutTrain_combo = [target_data_for_testing_Intrain, target_data_for_testing_Outtrain]\n",
        "    target_data_for_testing_InAndOutTrain = pd.concat(target_data_for_testing_InAndOutTrain_combo, sort=False)\n",
        "\n",
        "    print(\"target_data_for_testing_InAndOutTrain\", target_data_for_testing_InAndOutTrain.shape)\n",
        "\n",
        "    print(\"attack_data.shape\", attack_data.shape)\n",
        "    # print(attack_data.head())\n",
        "\n",
        "    # # sample randomly\n",
        "    # # returns all but in a random fashion\n",
        "    # attack_data = attack_data.sample(frac=1)\n",
        "    # print(attack_data.head())\n",
        "\n",
        "    X_attack = attack_data.drop(\"labels\", axis=1)\n",
        "    print(\"X_attack.shape\", X_attack.shape)\n",
        "\n",
        "    y_attack = attack_data[\"labels\"]\n",
        "\n",
        "    # let's do in and out for attack data (shadow)\n",
        "    X_attack_InTrain = positive_attack_data.drop(\"labels\", axis=1)\n",
        "    y_attack_InTrain = positive_attack_data[\"labels\"]\n",
        "\n",
        "    X_attack_OutTrain = negative_attack_data.drop(\"labels\", axis=1)\n",
        "    y_attack_OutTrain = negative_attack_data[\"labels\"]\n",
        "\n",
        "    print(\"X_attack_InTrain\", X_attack_InTrain.shape)\n",
        "    print(\"X_attack_OutTrain\", X_attack_OutTrain.shape)\n",
        "\n",
        "    # For in train data (target)\n",
        "    X_InTrain = target_data_for_testing_Intrain.drop([\"labels\", \"nodeID\"], axis=1)\n",
        "    y_InTrain = target_data_for_testing_Intrain[\"labels\"]\n",
        "    nodeID_InTrain = target_data_for_testing_Intrain[\"nodeID\"]\n",
        "\n",
        "    # For Out train data\n",
        "    X_OutTrain = target_data_for_testing_Outtrain.drop([\"labels\", \"nodeID\"], axis=1)\n",
        "    y_OutTrain = target_data_for_testing_Outtrain[\"labels\"]\n",
        "    nodeID_OutTrain = target_data_for_testing_Outtrain[\"nodeID\"]\n",
        "\n",
        "    # For in out data\n",
        "    X_InOutTrain = target_data_for_testing_InAndOutTrain.drop([\"labels\", \"nodeID\"], axis=1)\n",
        "    print(\"X_InTrain.shape\", X_InOutTrain.shape)\n",
        "\n",
        "    y_InOutTrain = target_data_for_testing_InAndOutTrain[\"labels\"]\n",
        "    nodeID_InOutTrain = target_data_for_testing_InAndOutTrain[\"nodeID\"]\n",
        "\n",
        "    # convert to numpy\n",
        "    # for shadow\n",
        "    X_attack_InOut, y_attack_InOut = X_attack.to_numpy(), y_attack.to_numpy()\n",
        "\n",
        "    X_attack_InTrain, X_attack_OutTrain = X_attack_InTrain.to_numpy(), X_attack_OutTrain.to_numpy()\n",
        "    y_attack_InTrain, y_attack_OutTrain = y_attack_InTrain.to_numpy(), y_attack_OutTrain.to_numpy()\n",
        "\n",
        "    # for target\n",
        "    X_InTrain, y_InTrain, nodeID_InTrain = X_InTrain.to_numpy(), y_InTrain.to_numpy(), nodeID_InTrain.to_numpy()\n",
        "    X_OutTrain, y_OutTrain, nodeID_OutTrain = X_OutTrain.to_numpy(), y_OutTrain.to_numpy(), nodeID_OutTrain.to_numpy()\n",
        "\n",
        "    # for target\n",
        "    X_InOutTrain, y_InOutTrain, nodeID_InOutTrain = X_InOutTrain.to_numpy(), y_InOutTrain.to_numpy(), nodeID_InOutTrain.to_numpy()\n",
        "\n",
        "    # # Plot graphs\n",
        "    #\n",
        "    # plt.imshow(X_attack_InTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Positive: In Train Posteriors')\n",
        "    # plt.show()\n",
        "    #\n",
        "    # plt.imshow(X_attack_OutTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Negative: Out Train Posteriors')\n",
        "    # plt.show()\n",
        "    #\n",
        "    #\n",
        "    # plt.imshow(X_InTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Positive: Target Posteriors')\n",
        "    # plt.show()\n",
        "    #\n",
        "    # plt.imshow(X_OutTrain, interpolation='nearest', aspect='auto')\n",
        "    # plt.colorbar()\n",
        "    # plt.tight_layout()\n",
        "    # plt.title('Negative: Target Posteriors')\n",
        "    # plt.show()\n",
        "\n",
        "    attack_train_data_X, attack_test_data_X, attack_train_data_y, attack_test_data_y = train_test_split(X_attack,\n",
        "                                                                                                        y_attack,\n",
        "                                                                                                        test_size=50,\n",
        "                                                                                                        stratify=y_attack,\n",
        "                                                                                                        random_state=rand_state)\n",
        "    print(\"baba\")\n",
        "\n",
        "    # convert series data to numpy array\n",
        "    attack_train_data_X, attack_test_data_X, attack_train_data_y, attack_test_data_y = attack_train_data_X.to_numpy(), attack_test_data_X.to_numpy(), attack_train_data_y.to_numpy(), attack_test_data_y.to_numpy()\n",
        "\n",
        "    # print(\"Attack data printing...\")\n",
        "    # print(attack_test_data_X, attack_test_data_y)\n",
        "\n",
        "    # Attack_train\n",
        "    attack_train_data = torch.utils.data.TensorDataset(torch.from_numpy(attack_train_data_X).float(), torch.from_numpy(\n",
        "        attack_train_data_y))  # convert to float to fix  uint8_t overflow error\n",
        "    attack_train_data_loader = torch.utils.data.DataLoader(attack_train_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Attack_test = combo of targettrain and targetOut\n",
        "    attack_test_data = torch.utils.data.TensorDataset(torch.from_numpy(attack_test_data_X).float(), torch.from_numpy(\n",
        "        attack_test_data_y))  # convert to float to fix  uint8_t overflow error\n",
        "    attack_test_data_loader = torch.utils.data.DataLoader(attack_test_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Training InData\n",
        "    target_data_for_testing_InTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_InTrain).float(),\n",
        "                                                                          torch.from_numpy(y_InTrain),\n",
        "                                                                          torch.from_numpy(nodeID_InTrain))\n",
        "    target_data_for_testing_InTrain_data_loader = torch.utils.data.DataLoader(target_data_for_testing_InTrain_data,\n",
        "                                                                              batch_size=64, shuffle=False)\n",
        "\n",
        "    # Training OutData\n",
        "    target_data_for_testing_OutTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_OutTrain).float(),\n",
        "                                                                           torch.from_numpy(y_OutTrain),\n",
        "                                                                           torch.from_numpy(nodeID_OutTrain))\n",
        "    target_data_for_testing_OutTrain_data_loader = torch.utils.data.DataLoader(target_data_for_testing_OutTrain_data,\n",
        "                                                                               batch_size=64, shuffle=False)\n",
        "\n",
        "    # Training InOut Data\n",
        "    target_data_for_testing_InOutTrain_data = torch.utils.data.TensorDataset(torch.from_numpy(X_InOutTrain).float(),\n",
        "                                                                             torch.from_numpy(y_InOutTrain),\n",
        "                                                                             torch.from_numpy(nodeID_InOutTrain))\n",
        "    target_data_for_testing_InOutTrain_data_loader = torch.utils.data.DataLoader(\n",
        "        target_data_for_testing_InOutTrain_data,\n",
        "        batch_size=64, shuffle=True)\n",
        "\n",
        "    # features, labels = next(iter(attack_test_data_loader))\n",
        "    # print(features, labels)\n",
        "\n",
        "    class AttackModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            # inputs to hidden layer linear transformation\n",
        "            # Note, when using Linear, weight and biases are randomly initialized for you\n",
        "            self.hidden = nn.Linear(dataset.num_classes, 100)\n",
        "            self.hidden2 = nn.Linear(100, 50)\n",
        "            # output layer, 10 units - one for each digits\n",
        "            self.output = nn.Linear(50, 2)\n",
        "\n",
        "            # # Define sigmoid activation and softmax output\n",
        "            # # comment this cos u can just define directly if u are using functional\n",
        "            # self.sigmoid = nn.Sigmoid()\n",
        "            # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # # pass the input tensor through each of our operations\n",
        "            # # comment to use functional\n",
        "            # x = self.hidden(x)\n",
        "            # x = self.sigmoid(x)\n",
        "            # x = self.output(x)\n",
        "            # x = self.softmax(x)\n",
        "\n",
        "            # Hidden layer with sigmoid activation\n",
        "            x = F.sigmoid(self.hidden(x))\n",
        "            x = F.sigmoid(self.hidden2(x))\n",
        "            # output layer with softmax activation\n",
        "            x = F.softmax(self.output(x), dim=1)\n",
        "            # print(\"xxxxxxxx\", x)\n",
        "\n",
        "            return x\n",
        "\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        # define nn\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            self.fc1 = nn.Linear(dataset.num_classes, 100)\n",
        "            self.fc2 = nn.Linear(100, 50)\n",
        "            self.fc3 = nn.Linear(50, 2)\n",
        "            self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        def forward(self, X):\n",
        "            # print(\"attack X\",X)\n",
        "            X = F.relu(self.fc1(X))\n",
        "            X = F.relu(self.fc2(X))\n",
        "            X = self.fc3(X)\n",
        "            X = self.softmax(X)\n",
        "\n",
        "            return X\n",
        "\n",
        "\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "    # create the ntwk\n",
        "    attack_model = Net()  # AttackModel()\n",
        "    attack_model = attack_model.to(device)\n",
        "    attack_model.apply(init_weights)  # initialize weight rather than randomly\n",
        "    print(attack_model)\n",
        "\n",
        "\n",
        "    def attack_train(model, trainloader, testloader, criterion, optimizer, epochs, steps=0):\n",
        "        # train ntwk\n",
        "\n",
        "        # Decay LR by a factor of 0.1 every 7 epochs\n",
        "        # scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "        final_train_loss = 0\n",
        "        train_losses, test_losses = [], []\n",
        "        posteriors = []\n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            train_accuracy = 0\n",
        "\n",
        "            # This is features, labels cos we dont care about nodeID during training! only during test\n",
        "            for features, labels in trainloader:\n",
        "                model.train()\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "                # print(\"post shape\", features.shape)\n",
        "                # print(\"labels\",labels)\n",
        "                optimizer.zero_grad()\n",
        "                # print(\"features\", features.shape)\n",
        "\n",
        "                # features = features.unsqueeze(1) #unsqueeze\n",
        "                # flatten features\n",
        "                features = features.view(features.shape[0], -1)\n",
        "\n",
        "                logps = model(features)  # log probabilities\n",
        "                # print(\"labelsssss\", labels.shape)\n",
        "                loss = criterion(logps, labels)\n",
        "\n",
        "                # Actual probabilities\n",
        "                ps = logps  # torch.exp(logps) #Only use this if the loss is nlloss\n",
        "                # print(\"ppppp\",ps)\n",
        "\n",
        "                top_p, top_class = ps.topk(1,\n",
        "                                           dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                # print(top_p)\n",
        "                equals = top_class == labels.view(\n",
        "                    *top_class.shape)  # making the shape of the label and top class the same\n",
        "                train_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "            else:\n",
        "                # Everything in this else block executes after every epock\n",
        "                # print(f\"training loss: {running_loss}\")\n",
        "\n",
        "                # test_loss = 0\n",
        "                # test_accuracy = 0\n",
        "                #\n",
        "                # # Turn off gradients for validation, saves memory and computations\n",
        "                # with torch.no_grad():\n",
        "                #     # Doing validation\n",
        "                #\n",
        "                #     # set model to evaluation mode\n",
        "                #     model.eval()\n",
        "                #\n",
        "                #     if e == epochs - 1:\n",
        "                #         print(\"Doing attack validation===========\")\n",
        "                #     # validation pass\n",
        "                #\n",
        "                #     for features, labels in testloader:\n",
        "                #         # features = features.unsqueeze(1)  # unsqueeze\n",
        "                #         features = features.view(features.shape[0], -1)\n",
        "                #         logps = model(features)\n",
        "                #         test_loss += criterion(logps, labels)\n",
        "                #\n",
        "                #         # Actual probabilities\n",
        "                #         ps = torch.exp(logps)\n",
        "                #\n",
        "                #         top_p, top_class = ps.topk(1,\n",
        "                #                                    dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                #         # print(top_p)\n",
        "                #         equals = top_class == labels.view(\n",
        "                #             *top_class.shape)  # making the shape of the label and top class the same\n",
        "                #         test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                test_loss, test_accuracy, _, _, _, _, _, _, _ = attack_test(model, testloader, trainTest=True)\n",
        "\n",
        "                # set model back yo train model\n",
        "                model.train()\n",
        "                # scheduler.step()\n",
        "\n",
        "                train_losses.append(running_loss / len(trainloader))\n",
        "                test_losses.append(test_loss)\n",
        "\n",
        "                # get final train loss. To be returned at the end of the training loop\n",
        "                final_train_loss = running_loss / len(trainloader)\n",
        "\n",
        "                print(\"Epoch: {}/{}..\".format(e + 1, epochs),\n",
        "                      \"Training loss: {:.5f}..\".format(running_loss / len(trainloader)),\n",
        "                      \"Test Loss: {:.5f}..\".format(test_loss),\n",
        "                      \"Train Accuracy: {:.3f}\".format(train_accuracy / len(trainloader)),\n",
        "                      \"Test Accuracy: {:.3f}\".format(test_accuracy)\n",
        "                      )\n",
        "\n",
        "        # # plot train and test loss\n",
        "        # plt.show()\n",
        "        # plt.plot(train_losses)\n",
        "        # plt.plot(test_losses)\n",
        "        # plt.title('Model Losses')\n",
        "        # plt.ylabel('loss')\n",
        "        # plt.xlabel('epoch')\n",
        "        # plt.legend(['train', 'val'], loc='upper left')\n",
        "        # plt.show()\n",
        "\n",
        "        return final_train_loss\n",
        "\n",
        "\n",
        "    def attack_test(model, testloader, singleClass=False, trainTest=False):\n",
        "        test_loss = 0\n",
        "        test_accuracy = 0\n",
        "        auroc = 0\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f_score = 0\n",
        "\n",
        "        posteriors = []\n",
        "        all_nodeIDs = []\n",
        "        true_predicted_nodeIDs_and_class = {}\n",
        "        false_predicted_nodeIDs_and_class = {}\n",
        "\n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            # Doing validation\n",
        "\n",
        "            # set model to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            if trainTest:\n",
        "                for features, labels in testloader:\n",
        "                    features, labels = features.to(device), labels.to(device)\n",
        "                    # features = features.unsqueeze(1)  # unsqueeze\n",
        "                    features = features.view(features.shape[0], -1)\n",
        "                    logps = model(features)\n",
        "                    test_loss += criterion(logps, labels)\n",
        "\n",
        "                    # Actual probabilities\n",
        "                    ps = logps  # torch.exp(logps)\n",
        "                    posteriors.append(ps)\n",
        "\n",
        "                    # if singleclass=false\n",
        "                    if not singleClass:\n",
        "                        y_true = labels.cpu().unsqueeze(-1)\n",
        "                        # print(\"y_true\", y_true)\n",
        "                        y_pred = ps.argmax(dim=-1, keepdim=True)\n",
        "                        # print(\"y_pred\", y_pred)\n",
        "\n",
        "                        # uncomment this to show AUROC\n",
        "                        auroc += roc_auc_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
        "                        # print(\"auroc\", auroc)\n",
        "\n",
        "                        precision += precision_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        recall += recall_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        f_score += f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                    top_p, top_class = ps.topk(1,\n",
        "                                               dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                    # print(top_p)\n",
        "                    equals = top_class == labels.view(\n",
        "                        *top_class.shape)  # making the shape of the label and top class the same\n",
        "                    test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            else:\n",
        "                # print(\"len(testloader.dataset)\", len(testloader.dataset))\n",
        "                for features, labels, nodeIDs in testloader:\n",
        "                    # print(\"nodeIDs\", nodeIDs)\n",
        "                    features, labels = features.to(device), labels.to(device)\n",
        "                    # features = features.unsqueeze(1)  # unsqueeze\n",
        "                    features = features.view(features.shape[0], -1)\n",
        "                    logps = model(features)\n",
        "                    test_loss += criterion(logps, labels)\n",
        "\n",
        "                    # Actual probabilities\n",
        "                    ps = logps  # torch.exp(logps)\n",
        "                    posteriors.append(ps)\n",
        "\n",
        "                    # print(\"ps\", ps)\n",
        "                    # print(\"nodeIDs\", nodeIDs)\n",
        "\n",
        "                    all_nodeIDs.append(nodeIDs)\n",
        "\n",
        "                    # if singleclass=false\n",
        "                    if not singleClass:\n",
        "                        y_true = labels.cpu().unsqueeze(-1)\n",
        "                        # print(\"y_true\", y_true)\n",
        "                        y_pred = ps.argmax(dim=-1, keepdim=True)\n",
        "                        # print(\"y_pred\", y_pred)\n",
        "\n",
        "                        # uncomment this to show AUROC\n",
        "                        auroc += roc_auc_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
        "                        # print(\"auroc\", auroc)\n",
        "\n",
        "                        precision += precision_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        recall += recall_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                        f_score += f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n",
        "\n",
        "                    top_p, top_class = ps.topk(1,\n",
        "                                               dim=1)  # top_p gives the probabilities while top_class gives the predicted classes\n",
        "                    # print(\"top_p\", top_p)\n",
        "                    # print(\"top_class\", top_class)\n",
        "\n",
        "                    equals = top_class == labels.view(\n",
        "                        *top_class.shape)  # making the shape of the label and top class the same\n",
        "\n",
        "                    # print(\"equals\", len(equals))\n",
        "                    for i in range(len(equals)):\n",
        "                        if equals[i]:  # if element is true {meaning both member n non-member}, get the nodeID\n",
        "                            # print(\"baba\")\n",
        "                            # print(\"true pred nodeIDs\", nodeIDs[i])\n",
        "                            true_predicted_nodeIDs_and_class[nodeIDs[i].item()] = top_class[i].item()\n",
        "                            # print(\"len(true_predicted_nodeIDs_and_class)\", len(true_predicted_nodeIDs_and_class),\"nodeID--\",nodeIDs[i].item(), \"class--\",  top_class[i].item())\n",
        "                        else:\n",
        "                            false_predicted_nodeIDs_and_class[nodeIDs[i].item()] = top_class[i].item()\n",
        "                            # print(\"len(false_predicted_nodeIDs_and_class)\", len(false_predicted_nodeIDs_and_class))\n",
        "\n",
        "                    test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "        test_accuracy = test_accuracy / len(testloader)\n",
        "        test_loss = test_loss / len(testloader)\n",
        "        final_auroc = auroc / len(testloader)\n",
        "        final_precision = precision / len(testloader)\n",
        "        final_recall = recall / len(testloader)\n",
        "        final_f_score = f_score / len(testloader)\n",
        "\n",
        "        # print('final precision', final_precision)\n",
        "        # print('final micro precision', final_recall)\n",
        "\n",
        "        # print(\"final auroc\", final_auroc)\n",
        "        # if all_nodeIDs:\n",
        "        #     print(\"all_nodeIDs\", torch.cat(all_nodeIDs, 0), \"shape Cat\", torch.cat(all_nodeIDs, 0).shape)\n",
        "        #     # true_predicted_nodeIDs_and_class = torch.cat(true_predicted_nodeIDs_and_class)\n",
        "\n",
        "        return test_loss, test_accuracy, posteriors, final_auroc, final_precision, final_recall, final_f_score, true_predicted_nodeIDs_and_class, false_predicted_nodeIDs_and_class\n",
        "\n",
        "\n",
        "    '''Initialization / params for attack model'''\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # nn.NLLLoss() # cross entropy loss\n",
        "\n",
        "    optimizer = torch.optim.Adam(attack_model.parameters(), lr=0.01)  # 0.01 #0.00001\n",
        "\n",
        "    epochs = 100  # 1000\n",
        "\n",
        "    '''==============Train and test Attack model ========== '''\n",
        "\n",
        "    attack_train(attack_model, attack_train_data_loader, attack_test_data_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    # test to confirm using attack_test_data_loader\n",
        "    _, test_accuracyConfirmTest, posteriors, auroc, precision, recall, f_score, _, _ = attack_test(attack_model,\n",
        "                                                                                                   attack_test_data_loader,\n",
        "                                                                                                   trainTest=True)\n",
        "    # print(posteriors)\n",
        "\n",
        "    # This is d result on the test set we used i.e split the attack data into train and test.\n",
        "    # Size of the test = 50\n",
        "    print(\"To confirm using attack_test_data_loader (50 test samples): {:.3f}\".format(test_accuracyConfirmTest),\n",
        "          \"AUROC: {:.3f}\".format(auroc), \"precision: {:.3f}\".format(precision), \"recall {:.3f}\".format(recall))\n",
        "\n",
        "    # test for InOut train target data\n",
        "    # This is the one we are interested in\n",
        "    _, test_accuracyInOut, posteriors, auroc, precision, recall, f_score, true_predicted_nodeIDs_and_class, false_predicted_nodeIDs_and_class = attack_test(\n",
        "        attack_model,\n",
        "        target_data_for_testing_InOutTrain_data_loader)\n",
        "    # print(posteriors)\n",
        "    # print(\"true_predicted_nodeIDs_and_class\", len(true_predicted_nodeIDs_and_class))\n",
        "    # print(\"false_predicted_nodeIDs_and_class\", len(false_predicted_nodeIDs_and_class))\n",
        "    print(\"Test accuracy with Target Train InOut: {:.3f}\".format(test_accuracyInOut), \"AUROC: {:.3f}\".format(auroc),\n",
        "          \"precision: {:.3f}\".format(precision), \"recall {:.3f}\".format(recall), \"F1 score {:.3f}\".format(f_score),\n",
        "          \"===> Attack Performance!\")\n",
        "\n",
        "    result_file.write(\n",
        "        \"Test accuracy with Target Train InOut: {:.3f} \".format(test_accuracyInOut) + \" AUROC: {:.3f}\".format(auroc) +\n",
        "        \" precision: {:.3f}\".format(precision) + \" recall {:.3f}\".format(recall) + \" F1 score {:.3f}\".format(f_score) +\n",
        "        \"===> Attack Performance! \\n\")\n",
        "\n",
        "    # test for Only In train target data\n",
        "    _, test_accuracyIn, posteriors, _, precision, recall, f_score, _, _ = attack_test(attack_model,\n",
        "                                                                                      target_data_for_testing_InTrain_data_loader,\n",
        "                                                                                      True)\n",
        "    # print(\"Test accuracy with Target Train In: {:.3f}\".format(test_accuracyIn), \"precision: {:.3f}\".format(precision),\n",
        "    #       \"recall {:.3f}\".format(recall), \"F1 score {:.3f}\".format(f_score))\n",
        "\n",
        "    # test for Only Out train target data\n",
        "    _, test_accuracyOut, posteriors, _, precision, recall, f_score, _, _ = attack_test(attack_model,\n",
        "                                                                                       target_data_for_testing_OutTrain_data_loader,\n",
        "                                                                                       True)\n",
        "    # print(\"Test accuracy with Target Train Out: {:.3f}\".format(test_accuracyOut), \"precision: {:.3f}\".format(precision),\n",
        "    #       \"recall {:.3f}\".format(recall), \"F1 score {:.3f}\".format(f_score))\n",
        "\n",
        "    result_file.write(\n",
        "        \"Test accuracy with Target Train In: {:.3f} \".format(\n",
        "            test_accuracyIn) + \" |=====| Test accuracy with Target Train Out: {:.3f}\".format(test_accuracyOut) + \"\\n\")\n",
        "\n",
        "    print(\"data_type\", data_type)\n",
        "    print(\"model_type\", model_type)\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_time = round(end_time - start_time, 3)\n",
        "    print(\"WhichRun\", which_run, \" Total time\", total_time)\n",
        "\n",
        "    # result_file.write(\"Data:\"+data_type +\" Model:\"+ model_type+\"\\n\\n\\n\")\n",
        "    result_file.write(\" ================ WhichRun: \" + str(\n",
        "        which_run) + \" || Data: \" + data_type + \" || Model: \" + model_type + \" || Time: \" + str(\n",
        "        total_time) + \" || rand_state: \" + str(rand_state) + \" ================== \\n\\n\\n\")\n",
        "\n",
        "    result_file.close()\n",
        "\n",
        "    # sys.exit()\n",
        "\n",
        "    # # Uncomment unless SAGE. TODO merge SAGE\n",
        "    # print(\"==================== Begin Quick analysis===============================\")\n",
        "    # # Analyzing lookup Quick analysis\n",
        "    # # Look at the look up table, merge the files of both n then print out the connectivity / values of the lookup table wrt true_predicted_nodeIDs_and_class\n",
        "    #\n",
        "    # # Load\n",
        "    # read_target_InTrain_nodes_neigbors_lookup = np.load(save_target_InTrain_nodes_neigbors,\n",
        "    #                                                     allow_pickle='TRUE')\n",
        "    #\n",
        "    # read_target_OutTrain_nodes_neigbors_lookup = np.load(save_target_OutTrain_nodes_neigbors,\n",
        "    #                                                      allow_pickle='TRUE')\n",
        "    #\n",
        "    # print(\"read_target_InTrain_nodes_neigbors_lookup\", len(read_target_InTrain_nodes_neigbors_lookup))\n",
        "    # print(\"read_target_OutTrain_nodes_neigbors_lookup\", len(read_target_OutTrain_nodes_neigbors_lookup))\n",
        "    # # Merge the 2 list #dictionary\n",
        "    # # all_nodes_lookup = {**read_target_InTrain_nodes_neigbors_lookup, **read_target_OutTrain_nodes_neigbors_lookup}\n",
        "    # all_nodes_lookup = np.concatenate(\n",
        "    #     (read_target_InTrain_nodes_neigbors_lookup, read_target_OutTrain_nodes_neigbors_lookup), axis=0)\n",
        "    # print(len(all_nodes_lookup))\n",
        "    #\n",
        "    # # for only train nodes {for stat}\n",
        "    # train_nodes_lookup = read_target_InTrain_nodes_neigbors_lookup\n",
        "    #\n",
        "    # print(\"all_nodes_lookup\", all_nodes_lookup)\n",
        "    #\n",
        "    # # convert list to dict\n",
        "    # all_nodes_lookup_dict = {all_nodes_lookup[nodeID][0]: all_nodes_lookup[nodeID][1] for nodeID in\n",
        "    #                          range(0, len(all_nodes_lookup))}\n",
        "    # print(\"all_nodes_lookup_dict\", all_nodes_lookup_dict)\n",
        "    #\n",
        "    # train_nodes_lookup_dict = {train_nodes_lookup[nodeID][0]: train_nodes_lookup[nodeID][1] for nodeID in\n",
        "    #                            range(0, len(train_nodes_lookup))}\n",
        "    #\n",
        "    # print(\"train_nodes_lookup_dict\", train_nodes_lookup_dict)\n",
        "    #\n",
        "    # # Problem: Reconstructed node train_nodes_lookup_dict and all_target_node_and_neigbors dont match?\n",
        "    # # Trick: Need not use the info of the train_nodes_lookup_dict. Just compare all_target_node_and_neigbors against all_all_node_and_neigbors using the nodeID of the train_nodes_lookup_dict\n",
        "    #\n",
        "    # # this is done for the display of the graph\n",
        "    # # all_correct_classified_nodes = []\n",
        "    # all_correct_classified_nodes_as1 = []\n",
        "    # all_correct_classified_nodes_as0 = []\n",
        "    #\n",
        "    # all_incorrect_classified_nodes_as1 = []\n",
        "    # all_incorrect_classified_nodes_as0 = []\n",
        "    # correct_edge_index = []\n",
        "    # incorrect_edge_index = []\n",
        "    #\n",
        "    # # Commented out. This test for node neighbor fraction not needed 17.10.2020\n",
        "    # # # for stat\n",
        "    # # # The all node look up here (all_nodes_lookup_dict) has to be picked from the original graph? Else the train_nodes_lookup_dict == train_nodes_lookup_dict\n",
        "    # #\n",
        "    # #\n",
        "    # # # PositiveInTrain distribution. To cater for negative, do the same thing, but downgrade the from 630 back to 0 so as to mach n change the edge index to target_test_edge_index\n",
        "    # #\n",
        "    # # # For the 1st (target graph)\n",
        "    # # edges_raw = data_new.target_edge_index.cpu().numpy()\n",
        "    # # edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "    # #\n",
        "    # #\n",
        "    # # G = nx.Graph()\n",
        "    # # G.add_nodes_from(list(range(\n",
        "    # #     data.num_nodes)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Target\n",
        "    # # G.add_edges_from(edges)\n",
        "    # #\n",
        "    # # all_target_node_and_neigbors = {} #[]\n",
        "    # # all_target_nodes = []\n",
        "    # #\n",
        "    # # # print(\"x.size(0) shadow forward\", x.size(0))\n",
        "    # # for n in range(0, len(train_nodes_lookup)):  #630 for cora This will give all nodes in the graph!\n",
        "    # #     all_target_nodes.append(n)  # get all nodes\n",
        "    # #     all_target_node_and_neigbors[n] = [node for node in G.neighbors(n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "    # #     # all_target_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "    # #     # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "    # #\n",
        "    # # print(\"all_target_node_and_neigbors\", all_target_node_and_neigbors)\n",
        "    # #\n",
        "    # #\n",
        "    # #\n",
        "    # # # For the 2nd (all graph)\n",
        "    # # edges_raw = data_new.all_edge_index.cpu().numpy()\n",
        "    # # edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "    # #\n",
        "    # # G = nx.Graph()\n",
        "    # # G.add_nodes_from(list(range(\n",
        "    # #     data.num_nodes)))  # this is set to num_test_Target cos that's the max no of nodes since num_test_Target = num_nodes_in_each_class x num_class. Changed to data.num_nodes instead of num_test_Target\n",
        "    # # G.add_edges_from(edges)\n",
        "    # #\n",
        "    # # all_all_node_and_neigbors = {}  # []  # {}\n",
        "    # # all_all_nodes = []\n",
        "    # #\n",
        "    # # # print(\"x.size(0) shadow forward\", x.size(0))\n",
        "    # # # getting the neigbors of a particular node\n",
        "    # # for n in range(0, data.num_nodes):  # 1260 for cora This will give all nodes in the graph!\n",
        "    # #     all_all_nodes.append(n)  # get all nodes\n",
        "    # #     all_all_node_and_neigbors[n] = [node for node in G.neighbors(\n",
        "    # #         n)]  # set the value of the dict to the corresponding value if it has a neighbor else put empty list\n",
        "    # #     # all_all_node_and_neigbors.append((n, [node for node in G.neighbors(n)]))\n",
        "    # #     # print(\"n:\", n, \"neighbors:\", [n for n in G.neighbors(n)])  # G.adj[n] # gets the neighbors of a particular n\n",
        "    # #\n",
        "    # # print(\"all_all_node_and_neigbors\", all_all_node_and_neigbors)\n",
        "    # #\n",
        "    # # InTrainMemberCorrectPredDictionary = {}\n",
        "    # # InTrainNonMemberCorrectPredDictionary = {}\n",
        "    # # # Trick: Need not use the info of the train_nodes_lookup_dict. Just compare all_target_node_and_neigbors against all_all_node_and_neigbors using the nodeID of the train_nodes_lookup_dict\n",
        "    # #\n",
        "    # # # PositiveInTrain distribution. To cater for negative, do the same thing, but downgrade the from 630 back to 0 so as to mach n change the edge index to target_test_edge_index\n",
        "    # # # for positive and predicted\n",
        "    # # # Meaning the max should be 630\n",
        "    # # for nodeID in true_predicted_nodeIDs_and_class.keys(): #884\n",
        "    # #     # print(\"all_nodes_lookup_dict[nodeID]\", all_all_node_and_neigbors[nodeID])\n",
        "    # #     # print(\"true_predicted_nodeIDs_and_class[nodeID]\", true_predicted_nodeIDs_and_class[nodeID])\n",
        "    # #\n",
        "    # #     if nodeID in range(0, len(train_nodes_lookup)): #max of this section will be 630 but we have 448. This is good\n",
        "    # #\n",
        "    # #         print(\"nodeID\", nodeID,\"train_nodes_lookup_dict[nodeID]\", all_target_node_and_neigbors[nodeID]) #448\n",
        "    # #         print(\"all_nodes_lookup_dict[nodeID]\", all_all_node_and_neigbors[nodeID])\n",
        "    # #\n",
        "    # #         # Correctly predicted as 1\n",
        "    # #         # if len(all_nodes_lookup_dict[nodeID]) !=0 and true_predicted_nodeIDs_and_class[nodeID] == 1: # avoid division by 0  and cater for correctly predicted member\n",
        "    # #         if true_predicted_nodeIDs_and_class[nodeID] == 1:\n",
        "    # #             dist = len(all_target_node_and_neigbors[nodeID]) / len(all_all_node_and_neigbors[nodeID])\n",
        "    # #             if dist > 1:\n",
        "    # #                 dist = 1 # this is done to peg to 1\n",
        "    # #\n",
        "    # #             # dist = 1-dist\n",
        "    # #             # print(\"Node\", nodeID, \"==>\", len(all_target_node_and_neigbors[nodeID])/len(all_all_node_and_neigbors[nodeID]), \"True Predicted class ==>\",\n",
        "    # #             #       true_predicted_nodeIDs_and_class[nodeID])\n",
        "    # #             # print(\"Bababababa\")\n",
        "    # #             if dist >0: # removes those that have no connection with outside graph\n",
        "    # #                 InTrainMemberCorrectPredDictionary[nodeID] = dist\n",
        "    # #\n",
        "    # #         # Correctly predicted as 0\n",
        "    # #         elif true_predicted_nodeIDs_and_class[nodeID] == 0: # avoid division by 0  and cater for correctly predicted member\n",
        "    # #             # if len(all_target_node_and_neigbors[nodeID]) / len(all_all_node_and_neigbors[nodeID]) <=1:\n",
        "    # #             dist = len(all_target_node_and_neigbors[nodeID]) / len(all_all_node_and_neigbors[nodeID])\n",
        "    # #             if dist > 1:\n",
        "    # #                 dist = 1 # this is done\n",
        "    # #             if dist > 0: # removes those that have no connection with outside graph\n",
        "    # #                 InTrainNonMemberCorrectPredDictionary[nodeID] = dist\n",
        "    # #\n",
        "    # # print(\"InTrainMemberCorrectPredDictionary.values()\", len(InTrainMemberCorrectPredDictionary.values()))\n",
        "    # # print(\"InTrainNonMemberCorrectPredDictionary.values()\", len(InTrainNonMemberCorrectPredDictionary.values()))\n",
        "    # # # width = 5.0 #histogram width\n",
        "    # # # plt.bar(list(InTrainMemberCorrectPredDictionary.keys()), InTrainMemberCorrectPredDictionary.values(), width) #, color='g'\n",
        "    # # # plt.show()\n",
        "    #\n",
        "    # #\n",
        "    # # import seaborn as sns\n",
        "    # # # fraction of all target node's neighbors correctly classified as members and neighbors in entire graph\n",
        "    # # sns.displot(list(InTrainMemberCorrectPredDictionary.values()))\n",
        "    # # plt.xlabel('Values', size=10)\n",
        "    # # plt.ylabel('Counts', size=10)\n",
        "    # # plt.show()\n",
        "    # #\n",
        "    # # # fraction of all target node's neighbors correctly classified as non-members and neighbors in entire graph\n",
        "    # # sns.displot(list(InTrainNonMemberCorrectPredDictionary.values()))\n",
        "    # # plt.xlabel('Values', size=10)\n",
        "    # # plt.ylabel('Counts', size=10)\n",
        "    # # plt.show()\n",
        "    #\n",
        "    # # # bins = []\n",
        "    # # # for i in range(0, 650, 20): # hardcoded for cora having 630 nodes\n",
        "    # # #     bins.append(i)\n",
        "    # # #\n",
        "    # # # plt.hist(InTrainMemberCorrectPredDictionary.values(), bins, histtype=\"bar\", rwidth=0.8)\n",
        "    # # # plt.show()\n",
        "    # # #\n",
        "    # # # for nodeID in true_predicted_nodeIDs_and_class.keys():\n",
        "    # # #     if nodeID in range(0, len(train_nodes_lookup)):\n",
        "    # # #         print(\"train_nodes_lookup_dict[nodeID]\", train_nodes_lookup_dict[nodeID])\n",
        "    # # #         print(\"all_nodes_lookup_dict[nodeID]\", all_nodes_lookup_dict[nodeID])\n",
        "    # # #         if len(all_nodes_lookup_dict[nodeID]) !=0: # avoid division by 0\n",
        "    # # #             print(\"Node\", nodeID, \"==>\", len(train_nodes_lookup_dict[nodeID])/len(all_nodes_lookup_dict[nodeID]), \"True Predicted class ==>\",\n",
        "    # # #                   true_predicted_nodeIDs_and_class[nodeID])\n",
        "    #\n",
        "    # print(\"============== Begin Homophily===========\")\n",
        "    # # Homophily Experiment:\n",
        "    # # Select a node.\n",
        "    # # if the neighbors of the selected nodes are not empty:\n",
        "    # # For each of the neighbors of the connected nodes:\n",
        "    # # Check the true labels. {if such nodes exist}\n",
        "    # # if true_label_node == true_label_neigbor, input 1 else 0. Then Add up the labels n get the \"average\"\n",
        "    # # if node does not exist, only return average for nodes that exist! {or simply give it the same label as the node!}\n",
        "    # # if node exist in the false_predicted_node, assign 1-the value as the truly predicted node!\n",
        "    # # That is the homophily for the selected node\n",
        "    #\n",
        "    # # for nodesID in range(0, data.num_nodes):\n",
        "    # #     if node true_predicted_nodeIDs_and_class[nodeID]\n",
        "    #\n",
        "    # # this stores the fractions already\n",
        "    # global_true_homophily = []\n",
        "    # global_pred_homophily = []\n",
        "    # correct_incorrect_homophily_prediction = []  # 1=correct, 0 = incorrect!\n",
        "    #\n",
        "    # for nodeID in range(0, data.num_nodes):  # true_predicted_nodeIDs_and_class.keys():\n",
        "    #     if nodeID in true_predicted_nodeIDs_and_class or nodeID in false_predicted_nodeIDs_and_class:\n",
        "    #\n",
        "    #         # get value not index for all_nodes_lookup cos its now a list?\n",
        "    #         if nodeID in true_predicted_nodeIDs_and_class:\n",
        "    #             print(\"Node\", nodeID, \"==>\", all_nodes_lookup_dict[nodeID], \"True Predicted class ==>\",\n",
        "    #                   true_predicted_nodeIDs_and_class[nodeID])\n",
        "    #\n",
        "    #         if nodeID in false_predicted_nodeIDs_and_class:\n",
        "    #             print(\"Node\", nodeID, \"==>\", all_nodes_lookup_dict[nodeID], \"False Predicted class ==>\",\n",
        "    #                   false_predicted_nodeIDs_and_class[nodeID])\n",
        "    #\n",
        "    #         true_homophily = []\n",
        "    #         pred_homophily = []\n",
        "    #\n",
        "    #         selected_node = all_nodes_lookup_dict[nodeID]\n",
        "    #\n",
        "    #         if nodeID in true_predicted_nodeIDs_and_class:\n",
        "    #             true_class_of_selected_node = true_predicted_nodeIDs_and_class[nodeID]\n",
        "    #             predicted_class_of_selected_node = true_predicted_nodeIDs_and_class[nodeID]\n",
        "    #         else:\n",
        "    #             true_class_of_selected_node = 1 - false_predicted_nodeIDs_and_class[nodeID]  # inverse\n",
        "    #             predicted_class_of_selected_node = false_predicted_nodeIDs_and_class[nodeID]\n",
        "    #\n",
        "    #         print(\"true_class_of_selected_node\", true_class_of_selected_node)\n",
        "    #         print(\"predicted_class_of_selected_node\", predicted_class_of_selected_node)\n",
        "    #\n",
        "    #         # Homiphily begins\n",
        "    #         # deal with true predicted first\n",
        "    #         if selected_node:  # if neghbors of selected node are not empty\n",
        "    #\n",
        "    #             # get correct n incorrect pred info\n",
        "    #             if nodeID in true_predicted_nodeIDs_and_class:\n",
        "    #                 correct_incorrect_homophily_prediction.append(1)  # correct pred homophily\n",
        "    #             else:\n",
        "    #                 correct_incorrect_homophily_prediction.append(0)  # incorrect pred homophily\n",
        "    #\n",
        "    #             # print(\"baba\")\n",
        "    #             for i in range(0, len(selected_node)):  # get all neighbors\n",
        "    #\n",
        "    #                 # if i in true_predicted_nodeIDs_and_class: #get true class of the node\n",
        "    #                 #     true_membership = true_predicted_nodeIDs_and_class[nodeID]\n",
        "    #                 # elif i in false_predicted_nodeIDs_and_class:\n",
        "    #                 #     true_membership = 1- false_predicted_nodeIDs_and_class[nodeID]\n",
        "    #                 # else:\n",
        "    #                 #     print(\"No membership. Stopping program now\")\n",
        "    #                 #     sys.exit() #exit the program!\n",
        "    #\n",
        "    #                 neigh = selected_node[i]  # get the neighbor(s) of the node\n",
        "    #                 print(\"neigh\", neigh)\n",
        "    #                 membership = 100  # intial value\n",
        "    #\n",
        "    #                 # This gives true homophily\n",
        "    #                 if neigh in true_predicted_nodeIDs_and_class:\n",
        "    #                     membership = true_predicted_nodeIDs_and_class[neigh]\n",
        "    #                 elif neigh in false_predicted_nodeIDs_and_class:  # if in the false pred # inverse\n",
        "    #                     membership = 1 - false_predicted_nodeIDs_and_class[neigh]\n",
        "    #                 else:\n",
        "    #                     membership = true_class_of_selected_node  # true_predicted_nodeIDs_and_class[nodeID] # becomes membership of the initial node if key not in true_predicted_nodeIDs_and_class\n",
        "    #\n",
        "    #                 true_homophily.append(membership)\n",
        "    #\n",
        "    #                 print(\"membership true\", membership)\n",
        "    #                 print(\"End =========\")\n",
        "    #\n",
        "    #                 # Predicted homophily: Whatever the data predicts is true\n",
        "    #                 if neigh in true_predicted_nodeIDs_and_class:\n",
        "    #                     membership = true_predicted_nodeIDs_and_class[neigh]\n",
        "    #                 elif neigh in false_predicted_nodeIDs_and_class:\n",
        "    #                     membership = false_predicted_nodeIDs_and_class[\n",
        "    #                         neigh]  # no need for inverse since this is the predicted\n",
        "    #                 else:\n",
        "    #                     membership = predicted_class_of_selected_node  # true_predicted_nodeIDs_and_class[nodeID] # becomes membership of the initial node if key not in true_predicted_nodeIDs_and_class\n",
        "    #\n",
        "    #                 pred_homophily.append(membership)\n",
        "    #\n",
        "    #                 print(\"membership pred\", membership)\n",
        "    #\n",
        "    #             print(\"true_homophily\", true_homophily)\n",
        "    #             print(\"pred_homophily\", pred_homophily)\n",
        "    #\n",
        "    #             # look thru each true and pred homophily, compare each element with their true n pred membership. Sum them and get average.\n",
        "    #             # Append to global_true_homophily and global_pred_homophily\n",
        "    #\n",
        "    #             frac_true_homophily = 0\n",
        "    #             frac_pred_homophily = 0\n",
        "    #\n",
        "    #             for i in range(0, len(true_homophily)):  # cos len of true_homophily n pred_homophily are the same\n",
        "    #                 if true_class_of_selected_node == true_homophily[i]:\n",
        "    #                     frac_true_homophily += 1\n",
        "    #                 if predicted_class_of_selected_node == pred_homophily[i]:\n",
        "    #                     frac_pred_homophily += 1\n",
        "    #\n",
        "    #             frac_true_homophily = frac_true_homophily / len(true_homophily)\n",
        "    #             frac_pred_homophily = frac_pred_homophily / len(pred_homophily)\n",
        "    #\n",
        "    #             print(\"frac_true_homophily = \", frac_true_homophily)\n",
        "    #             print(\"frac_pred_homophily = \", frac_pred_homophily)\n",
        "    #\n",
        "    #             global_true_homophily.append(frac_true_homophily)\n",
        "    #             global_pred_homophily.append(frac_pred_homophily)\n",
        "    #\n",
        "    #         # Homophily Ends\n",
        "    #\n",
        "    # print(\"global_true_homophily\", global_true_homophily)\n",
        "    # print(\"global_pred_homophily\", global_pred_homophily)\n",
        "    # print(\"correct_incorrect_homophily_prediction\", correct_incorrect_homophily_prediction)\n",
        "    #\n",
        "    # np.savetxt(save_global_pred_homophily, global_pred_homophily)\n",
        "    # np.savetxt(save_global_true_homophily, global_true_homophily)\n",
        "    # np.savetxt(save_correct_incorrect_homophily_prediction, correct_incorrect_homophily_prediction)\n",
        "    #\n",
        "    # print(\"lenghts\", len(correct_incorrect_homophily_prediction), len(global_true_homophily),\n",
        "    #       len(global_pred_homophily))\n",
        "    #\n",
        "    # # # Plot graph!\n",
        "    # # sns.displot(global_true_homophily)\n",
        "    # # plt.xlabel('Values', size=10)\n",
        "    # # plt.ylabel('Counts', size=10)\n",
        "    # # plt.show()\n",
        "    # #\n",
        "    # # sns.displot(global_pred_homophily)\n",
        "    # # plt.xlabel('Values', size=10)\n",
        "    # # plt.ylabel('Counts', size=10)\n",
        "    # # plt.show()\n",
        "    #\n",
        "    # # sys.exit()\n",
        "    #\n",
        "    # # # this stores the fractions already\n",
        "    # # global_true_homophily = []\n",
        "    # # global_pred_homophily = []\n",
        "    #\n",
        "    # for nodeID in true_predicted_nodeIDs_and_class.keys():\n",
        "    #     # get value not index for all_nodes_lookup cos its now a list?\n",
        "    #     print(\"Node\", nodeID, \"==>\", all_nodes_lookup_dict[nodeID], \"True Predicted class ==>\",\n",
        "    #           true_predicted_nodeIDs_and_class[nodeID])\n",
        "    #\n",
        "    #     # true_homophily = []\n",
        "    #     # pred_homophily = []\n",
        "    #     #\n",
        "    #     # selected_node = all_nodes_lookup_dict[nodeID]\n",
        "    #\n",
        "    #     # if nodeID in true_predicted_nodeIDs_and_class:\n",
        "    #     #     true_class_of_selected_node = true_predicted_nodeIDs_and_class[nodeID]\n",
        "    #     #     predicted_class_of_selected_node = true_predicted_nodeIDs_and_class[nodeID]\n",
        "    #     # else:\n",
        "    #     #     true_class_of_selected_node = 1- false_predicted_nodeIDs_and_class[nodeID] #inverse\n",
        "    #     #     predicted_class_of_selected_node = false_predicted_nodeIDs_and_class[nodeID]\n",
        "    #     #\n",
        "    #     # print(\"true_class_of_selected_node\", true_class_of_selected_node)\n",
        "    #     # print(\"predicted_class_of_selected_node\", predicted_class_of_selected_node)\n",
        "    #\n",
        "    #     # # Homiphily begins\n",
        "    #     # # deal with true predicted first\n",
        "    #     # if selected_node: # if neghbors of selected node are not empty\n",
        "    #     #     # print(\"baba\")\n",
        "    #     #     for i in range(0,len(selected_node)): #get all neighbors\n",
        "    #     #\n",
        "    #     #         # if i in true_predicted_nodeIDs_and_class: #get true class of the node\n",
        "    #     #         #     true_membership = true_predicted_nodeIDs_and_class[nodeID]\n",
        "    #     #         # elif i in false_predicted_nodeIDs_and_class:\n",
        "    #     #         #     true_membership = 1- false_predicted_nodeIDs_and_class[nodeID]\n",
        "    #     #         # else:\n",
        "    #     #         #     print(\"No membership. Stopping program now\")\n",
        "    #     #         #     sys.exit() #exit the program!\n",
        "    #     #\n",
        "    #     #         neigh = selected_node[i] #get the neighbor(s) of the node\n",
        "    #     #         print(\"neigh\", neigh)\n",
        "    #     #         membership = 100 # intial value\n",
        "    #     #\n",
        "    #     #         # This gives true homophily\n",
        "    #     #         if neigh in true_predicted_nodeIDs_and_class:\n",
        "    #     #             membership = true_predicted_nodeIDs_and_class[neigh]\n",
        "    #     #         elif neigh in false_predicted_nodeIDs_and_class: # if in the false pred # inverse\n",
        "    #     #             membership = 1-false_predicted_nodeIDs_and_class[neigh]\n",
        "    #     #         else:\n",
        "    #     #             membership = true_class_of_selected_node #true_predicted_nodeIDs_and_class[nodeID] # becomes membership of the initial node if key not in true_predicted_nodeIDs_and_class\n",
        "    #     #\n",
        "    #     #         true_homophily.append(membership)\n",
        "    #     #\n",
        "    #     #         print(\"membership true\", membership)\n",
        "    #     #         print(\"End =========\")\n",
        "    #     #\n",
        "    #     #         # Predicted homophily: Whatever the data predicts is true\n",
        "    #     #         if neigh in true_predicted_nodeIDs_and_class:\n",
        "    #     #             membership = true_predicted_nodeIDs_and_class[neigh]\n",
        "    #     #         elif neigh in false_predicted_nodeIDs_and_class:\n",
        "    #     #             membership = false_predicted_nodeIDs_and_class[neigh] # no need for inverse since this is the predicted\n",
        "    #     #         else:\n",
        "    #     #             membership = predicted_class_of_selected_node #true_predicted_nodeIDs_and_class[nodeID] # becomes membership of the initial node if key not in true_predicted_nodeIDs_and_class\n",
        "    #     #\n",
        "    #     #         pred_homophily.append(membership)\n",
        "    #     #\n",
        "    #     #         print(\"membership pred\", membership)\n",
        "    #     #\n",
        "    #     #     print(\"true_homophily\", true_homophily)\n",
        "    #     #     print(\"pred_homophily\", pred_homophily)\n",
        "    #     #\n",
        "    #     #     # look thru each true and pred homophily, compare each element with their true n pred membership. Sum them and get average.\n",
        "    #     #     # Append to global_true_homophily and global_pred_homophily\n",
        "    #     #\n",
        "    #     #     frac_true_homophily = 0\n",
        "    #     #     frac_pred_homophily = 0\n",
        "    #     #\n",
        "    #     #     for i in range(0, len(true_homophily)):  # cos len of true_homophily n pred_homophily are the same\n",
        "    #     #         if true_class_of_selected_node == true_homophily[i]:\n",
        "    #     #             frac_true_homophily += 1\n",
        "    #     #         if predicted_class_of_selected_node == pred_homophily[i]:\n",
        "    #     #             frac_pred_homophily += 1\n",
        "    #     #\n",
        "    #     #     frac_true_homophily = frac_true_homophily / len(true_homophily)\n",
        "    #     #     frac_pred_homophily = frac_pred_homophily / len(pred_homophily)\n",
        "    #     #\n",
        "    #     #     print(\"frac_true_homophily = \", frac_true_homophily)\n",
        "    #     #     print(\"frac_pred_homophily = \", frac_pred_homophily)\n",
        "    #     #\n",
        "    #     #     global_true_homophily.append(frac_true_homophily)\n",
        "    #     #     global_pred_homophily.append(frac_pred_homophily)\n",
        "    #     #\n",
        "    #     #\n",
        "    #     # # Homophily Ends\n",
        "    #\n",
        "    #     # all_correct_classified_nodes.append(nodeID)\n",
        "    #\n",
        "    #     # cater for 1 and 0\n",
        "    #     if true_predicted_nodeIDs_and_class[nodeID] == 1:\n",
        "    #         all_correct_classified_nodes_as1.append(nodeID)\n",
        "    #     else:\n",
        "    #         all_correct_classified_nodes_as0.append(nodeID)\n",
        "    #\n",
        "    #     for i in range(0, len(all_nodes_lookup_dict[nodeID])):\n",
        "    #         edge_ind = (nodeID, all_nodes_lookup_dict[nodeID][i])\n",
        "    #         # print(\"edge_ind\", edge_ind)\n",
        "    #         correct_edge_index.append((edge_ind))\n",
        "    #\n",
        "    # # print(\"global_true_homophily\", global_true_homophily)\n",
        "    # # print(\"global_pred_homophily\", global_pred_homophily)\n",
        "    # #\n",
        "    # # # Plot graph!\n",
        "    # # sns.displot(global_true_homophily)\n",
        "    # # plt.xlabel('Values', size=10)\n",
        "    # # plt.ylabel('Counts', size=10)\n",
        "    # # plt.show()\n",
        "    # #\n",
        "    # # sns.displot(global_pred_homophily)\n",
        "    # # plt.xlabel('Values', size=10)\n",
        "    # # plt.ylabel('Counts', size=10)\n",
        "    # # plt.show()\n",
        "    #\n",
        "    # print(\"======================End True predicted=============================\")\n",
        "    #\n",
        "    # for nodeID in false_predicted_nodeIDs_and_class.keys():\n",
        "    #     # get value not index for all_nodes_lookup cos its now a list?\n",
        "    #     print(\"Node\", nodeID, \"==>\", all_nodes_lookup_dict[nodeID], \"False Predicted class ==>\",\n",
        "    #           false_predicted_nodeIDs_and_class[nodeID])\n",
        "    #\n",
        "    #     # cater for different colors for wrongly predicted as 1 or 0\n",
        "    #     if false_predicted_nodeIDs_and_class[nodeID] == 1:\n",
        "    #         all_incorrect_classified_nodes_as1.append(nodeID)\n",
        "    #     else:\n",
        "    #         all_incorrect_classified_nodes_as0.append(nodeID)\n",
        "    #\n",
        "    #     for i in range(0, len(all_nodes_lookup_dict[nodeID])):\n",
        "    #         edge_ind = (nodeID, all_nodes_lookup_dict[nodeID][i])\n",
        "    #         # print(\"edge_ind\", edge_ind)\n",
        "    #         incorrect_edge_index.append((edge_ind))\n",
        "    #\n",
        "    #\n",
        "    # def plot_graph_result(edges_correct, nodes0, nodes1, edges_incorrect, nodes2, nodes3):\n",
        "    #     print(\"edges\", edges_correct)\n",
        "    #     # labels = labels.numpy() #dataset.data.y.numpy()\n",
        "    #\n",
        "    #     # correct 0\n",
        "    #     G0 = nx.Graph()\n",
        "    #     G0.add_nodes_from(nodes0)\n",
        "    #     G0.add_edges_from(edges_correct)\n",
        "    #\n",
        "    #     # correct 1\n",
        "    #     G1 = nx.Graph()\n",
        "    #     G1.add_nodes_from(nodes1)\n",
        "    #     G1.add_edges_from(edges_correct)\n",
        "    #\n",
        "    #     # wrongly pred as 0\n",
        "    #     G2 = nx.Graph()\n",
        "    #     G2.add_nodes_from(nodes2)\n",
        "    #     G2.add_edges_from(edges_incorrect)\n",
        "    #\n",
        "    #     # wrongly pred as 1\n",
        "    #     G3 = nx.Graph()\n",
        "    #     G3.add_nodes_from(nodes3)  # only change nodes. The edges is same\n",
        "    #     G3.add_edges_from(edges_incorrect)\n",
        "    #\n",
        "    #     # plt.subplot(111)\n",
        "    #     options = {\n",
        "    #         'node_size': 30,\n",
        "    #         'width': 0.2,\n",
        "    #     }\n",
        "    #\n",
        "    #     nx.draw(G0, with_labels=False, node_color=\"#8E44AD\", cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    #     nx.draw(G1, with_labels=False, node_color=\"#D2B4DE\", cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    #     nx.draw(G2, with_labels=False, node_color=\"#D35400\", cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    #     nx.draw(G3, with_labels=False, node_color=\"#EDBB99\", cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    #     plt.legend([\"Correctly Predicted as 0\", \"\", \"Correctly Predicted as 1\", \"\", \"Wrongly predicted as 0\", \"\",\n",
        "    #                 \"Wrongly predicted as 1\"])  # Empty cos of the link?\n",
        "    #     # plt.savefig(save_pics_filename)\n",
        "    #     plt.show()\n",
        "    #\n",
        "    #\n",
        "    # # plot_graph_result(correct_edge_index, all_correct_classified_nodes_as0, all_correct_classified_nodes_as1, incorrect_edge_index, all_incorrect_classified_nodes_as0, all_incorrect_classified_nodes_as1)\n",
        "    #\n",
        "    # # # plot_graph_result(incorrect_edge_index, all_incorrect_classified_nodes, color=\"pink\")\n",
        "    #\n",
        "    # print(\"==================== End Quick analysis===============================\")\n",
        "    #\n",
        "    # # from sklearn.svm import SVC\n",
        "    # # from sklearn.ensemble import VotingClassifier\n",
        "    # # from sklearn.linear_model import LogisticRegression\n",
        "    # # from sklearn.tree import DecisionTreeClassifier\n",
        "    # #\n",
        "    # # # Ensemble method using Logistic regression & Decision trees\n",
        "    # # lr_clf = LogisticRegression(random_state=0)\n",
        "    # #\n",
        "    # # dec_clf = DecisionTreeClassifier()\n",
        "    # #\n",
        "    # # voting_clf2 = VotingClassifier(\n",
        "    # #     estimators=[('lr', lr_clf), ('decision', dec_clf)],\n",
        "    # #     voting='hard')\n",
        "    # # voting_clf2.fit(X_attack_InOut, y_attack_InOut) # for shadow\n",
        "    # #\n",
        "    # # # performance\n",
        "    # # print(\"Ensemble accuracy performance! Interested\", voting_clf2.score(X_InOutTrain, y_InOutTrain)) # for target"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "rand_state 731702194\n",
            "data Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n",
            "label_idx 2708\n",
            "idx.size(0) 351\n",
            "idx.size(0) 217\n",
            "idx.size(0) 418\n",
            "idx.size(0) 818\n",
            "idx.size(0) 426\n",
            "idx.size(0) 298\n",
            "idx.size(0) 180\n",
            "shadow_train_idx 630\n",
            "Target_train_idx 630\n",
            "done others\n",
            "done target test\n",
            "done shadow test\n",
            "target_test_idx [2294, 189, 1971, 1320, 68, 1002, 2477, 2569, 1517, 990, 1946, 2566, 463, 2350, 1616, 2523, 2700, 1997, 1459, 2553, 1087, 1103, 1958, 1781, 1966, 1414, 2033, 2319, 2422, 2276, 392, 2618, 1749, 3, 634, 1129, 2702, 733, 1274, 1656, 566, 1614, 1798, 914, 2134, 2065, 316, 581, 2243, 2091, 1812, 730, 2380, 2299, 677, 172, 269, 2397, 2072, 1804, 470, 1992, 812, 696, 1431, 437, 1669, 1334, 1714, 1171, 1275, 350, 1872, 2303, 774, 599, 834, 541, 217, 764, 964, 2430, 1907, 1626, 1918, 803, 1383, 828, 679, 1483, 2070, 1374, 871, 1715, 1807, 1775, 54, 122, 69, 465, 1229, 958, 1422, 391, 1228, 2220, 7, 1221, 476, 1322, 831, 1491, 1646, 2037, 947, 208, 1628, 1327, 1769, 1507, 633, 1929, 993, 583, 160, 1977, 1664, 1460, 133, 1303, 279, 120, 1643, 1012, 190, 1635, 1821, 2223, 1013, 404, 81, 2151, 303, 411, 2444, 1983, 767, 1309, 2136, 1244, 1796, 2120, 691, 2116, 312, 2552, 2126, 2257, 1398, 1794, 1347, 1443, 2579, 2359, 262, 1960, 2311, 873, 429, 705, 2649, 1501, 2005, 1742, 2425, 1382, 484, 2509, 33, 2184, 2590, 2218, 1259, 2254, 1344, 1593, 1490, 963, 2469, 241, 2457, 479, 1833, 643, 532, 1805, 2467, 757, 1160, 2578, 2386, 1462, 414, 2310, 2684, 1613, 449, 2633, 469, 321, 2105, 2403, 203, 2557, 500, 440, 1925, 2537, 1223, 2631, 141, 2614, 1570, 2683, 2682, 1521, 2385, 1044, 970, 1673, 1543, 1448, 2343, 2017, 518, 1297, 2016, 636, 1894, 1599, 1137, 1604, 504, 265, 368, 1069, 1074, 204, 1726, 277, 1375, 93, 1588, 2155, 1123, 37, 587, 366, 1800, 1487, 117, 997, 2252, 1522, 913, 346, 861, 1029, 205, 1954, 2259, 735, 1011, 1565, 1406, 644, 2513, 998, 889, 2014, 1021, 2577, 2215, 8, 1225, 1788, 1718, 838, 2147, 1124, 1038, 105, 2399, 932, 2610, 896, 196, 1301, 544, 2378, 2248, 2176, 1899, 1610, 2193, 1386, 327, 6, 360, 2374, 85, 614, 2408, 673, 1534, 431, 2643, 319, 1497, 2227, 111, 1337, 1265, 1931, 2666, 1155, 1864, 926, 2040, 2562, 174, 1792, 323, 192, 797, 1846, 353, 474, 2371, 1624, 515, 1717, 1668, 2295, 1077, 2536, 973, 2368, 1758, 2208, 2458, 1288, 197, 1353, 1999, 642, 1131, 113, 1555, 1423, 140, 337, 1447, 2199, 2372, 2600, 761, 2642, 2144, 2224, 1252, 288, 2329, 629, 1582, 1060, 2624, 1486, 2502, 2390, 1531, 1454, 2149, 880, 1466, 1024, 1558, 1728, 972, 604, 2438, 2152, 2165, 740, 433, 148, 524, 1633, 2196, 2041, 2289, 741, 1603, 1944, 611, 819, 1914, 1085, 1114, 1179, 1331, 2138, 1362, 256, 83, 974, 1492, 1473, 2653, 1148, 1090, 868, 202, 1338, 996, 2270, 2419, 1793, 1777, 2550, 2394, 108, 398, 2593, 886, 1183, 40, 2634, 2102, 2335, 2357, 2515, 1354, 1196, 862, 1687, 2507, 215, 1887, 1953, 451, 1884, 184, 680, 2604, 610, 1922, 1791, 139, 1547, 684, 1289, 2379, 648, 2648, 830, 2496, 1858, 1184, 2364, 229, 2118, 420, 597, 1524, 1393, 2171, 1665, 1063, 1233, 671, 2115, 1043, 245, 579, 1429, 2237, 672, 2690, 2482, 2542, 1441, 405, 1452, 1463, 1410, 736, 654, 954, 2011, 1875, 2699, 1539, 340, 2589, 362, 10, 119, 97, 143, 1358, 897, 128, 804, 959, 2287, 876, 956, 450, 1159, 2283, 1595, 477, 232, 159, 261, 937, 1190, 1439, 646, 1477, 1277, 39, 2697, 1575, 2282, 1818, 1440, 1722, 150, 1097, 144, 707, 1498, 247, 1955, 444, 459, 1062, 2145, 2337, 493, 1660, 489, 442, 1485, 885, 91, 652, 832, 517, 1045, 1111, 1985, 843, 1212, 2225, 448, 1589, 1725, 855, 223, 840, 438, 2366, 2121, 1678, 70, 1237, 2599, 496, 1900, 1634, 1987, 2654, 2143, 759, 818, 689, 538, 523, 1102, 2628, 2051, 230, 620, 1213, 1870, 347, 543, 2050, 273, 2459, 1118, 1065, 2342, 2226, 2200, 1957, 1133, 146, 315, 938, 2221, 944, 43, 1503, 879, 112, 214, 788, 1594, 2101, 1729, 55, 2325, 675, 2455, 2574, 2096]\n",
            "shadow_test_idx [2305, 1862, 2692, 116, 1052, 533, 239, 2341, 2090, 1142, 829, 1254, 655, 2680, 1552, 1841, 1488, 304, 975, 75, 939, 2447, 1707, 467, 1433, 1161, 233, 1711, 2013, 1516, 1839, 1948, 2522, 647, 1249, 619, 1537, 2527, 38, 1394, 2514, 1691, 2316, 1990, 2204, 2346, 1465, 2156, 727, 2636, 1934, 2635, 2594, 2241, 1525, 2019, 890, 1754, 1299, 329, 794, 2672, 1438, 436, 1164, 80, 1396, 2657, 783, 1242, 114, 1561, 980, 2082, 2461, 121, 649, 1636, 201, 1324, 2641, 373, 290, 1950, 157, 2280, 860, 292, 1541, 74, 99, 1585, 156, 909, 921, 1241, 441, 1808, 1389, 1721, 1968, 1236, 2099, 1762, 2465, 1214, 1982, 1686, 1748, 2179, 1295, 410, 770, 267, 2309, 2660, 1824, 1323, 2053, 151, 494, 678, 556, 2344, 511, 1767, 2216, 1402, 286, 2168, 1312, 266, 2177, 12, 2110, 891, 2331, 1688, 900, 231, 2267, 246, 2705, 1731, 1752, 2452, 1740, 2698, 1417, 1527, 658, 2111, 577, 36, 423, 601, 417, 2228, 1676, 1920, 2172, 1333, 1395, 1848, 385, 1710, 2296, 1986, 1418, 243, 15, 2274, 1681, 1744, 251, 2362, 753, 929, 0, 1771, 296, 1545, 852, 218, 792, 1222, 1838, 1723, 2365, 1360, 435, 2351, 227, 32, 179, 1150, 2373, 2640, 721, 1020, 1302, 2393, 2234, 713, 1261, 2696, 1163, 2424, 462, 2232, 1250, 2264, 1880, 536, 1298, 1685, 1893, 2401, 2504, 2214, 2049, 530, 1140, 2012, 2035, 418, 1937, 1472, 719, 2181, 445, 2517, 923, 1902, 817, 725, 1243, 1915, 2048, 2230, 2291, 2321, 1618, 805, 1912, 210, 1967, 2284, 857, 1369, 1319, 86, 1520, 2073, 1445, 1632, 965, 962, 1738, 17, 1508, 1768, 1147, 2038, 2312, 1305, 989, 1509, 2212, 1939, 2353, 2, 1641, 1336, 2161, 1446, 686, 427, 1963, 1831, 2555, 1480, 1733, 1430, 503, 363, 516, 580, 87, 2568, 625, 1774, 605, 1577, 334, 2535, 1471, 1837, 351, 793, 2067, 1450, 1757, 750, 749, 124, 2506, 1583, 5, 1071, 1014, 2463, 1620, 25, 2377, 472, 1027, 2153, 2330, 1263, 506, 1219, 739, 166, 732, 73, 167, 2652, 452, 685, 2388, 2596, 307, 2288, 2546, 716, 621, 1413, 837, 968, 558, 2173, 528, 1652, 2290, 2396, 170, 2646, 138, 1786, 922, 371, 1832, 1310, 2114, 2089, 1139, 945, 16, 2285, 723, 2687, 2434, 537, 2109, 280, 1654, 295, 1993, 2429, 519, 608, 555, 1695, 4, 592, 65, 253, 1364, 226, 2532, 1152, 1058, 950, 547, 1553, 706, 322, 1128, 2260, 2518, 2464, 801, 1290, 1025, 1840, 1007, 991, 1116, 571, 1847, 737, 676, 2369, 586, 1843, 1496, 1086, 782, 1075, 2123, 407, 687, 1378, 2612, 1108, 181, 82, 815, 791, 289, 276, 2244, 2315, 2298, 902, 1049, 787, 2505, 2192, 478, 406, 1962, 52, 1512, 1145, 2206, 1073, 905, 2229, 615, 1361, 103, 2219, 60, 2632, 2598, 908, 1094, 212, 1476, 51, 2057, 380, 2203, 570, 1397, 2685, 2007, 2381, 1896, 2622, 2693, 1267, 2400, 402, 1936, 1535, 1905, 1617, 35, 1704, 2406, 453, 259, 182, 1271, 2348, 101, 419, 955, 1387, 559, 1415, 1672, 1281, 320, 2580, 100, 2112, 2491, 1806, 1101, 632, 126, 953, 1928, 1734, 1548, 1416, 693, 1779, 744, 2585, 1882, 275, 2521, 1540, 2093, 1756, 2549, 1916, 2247, 147, 2166, 1095, 816, 521, 1186, 90, 1855, 2675, 2615, 755, 2376, 2327, 458, 1724, 291, 213, 1003, 2603, 59, 2607, 495, 1559, 1084, 1627, 1844, 2472, 460, 1154, 933, 622, 550, 2187, 1974, 1325, 1661, 155, 903, 845, 260, 2148, 746, 1730, 969, 1280, 994, 2191, 645, 1245, 28, 2531, 2630, 1248, 27, 898, 255, 1342, 1455, 396, 1037, 2194, 2492, 2154, 1039, 1001, 1819, 1533, 1083, 924, 1737, 1205, 602, 2142, 1587, 663, 1365, 2431, 2433, 670, 1461, 2322, 206, 2024, 992, 520, 2286, 1739, 1647, 1464, 2129, 2658, 554, 2601, 1136, 1351, 1873, 2503, 1300, 1033, 375, 2083, 188, 758, 681, 408, 178, 1823, 690, 318, 1284, 61, 388, 46, 209, 1706, 700, 1795]\n",
            "data Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n",
            "data new Data(all_edge_index=[2, 10556], all_x=[2708, 1433], all_y=[2708], shadow_edge_index=[2, 730], shadow_test_mask=[2708], shadow_train_mask=[630], shadow_x=[630, 1433], shadow_y=[630], target_edge_index=[2, 682], target_test_mask=[2708], target_train_mask=[630], target_x=[630, 1433], target_y=[630])\n",
            "data_new.shadow_test_mask.sum() tensor(630)\n",
            "data_new.target_test_mask.sum() tensor(630)\n",
            "7\n",
            "model TargetModel(\n",
            "  (conv1): GCNConv(1433, 256)\n",
            "  (conv2): GCNConv(256, 7)\n",
            ")\n",
            "TargetModel Epoch: 001, Approx Train: 0.1381, Train: 0.1508, Test: 0.1349,marco: 0.0939,micro: 0.1349\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:879: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:936: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:670: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TargetModel Epoch: 002, Approx Train: 0.1508, Train: 0.1571, Test: 0.1413,marco: 0.0999,micro: 0.1413\n",
            "TargetModel Epoch: 003, Approx Train: 0.1571, Train: 0.1587, Test: 0.1444,marco: 0.1028,micro: 0.1444\n",
            "TargetModel Epoch: 004, Approx Train: 0.1587, Train: 0.1651, Test: 0.1540,marco: 0.1100,micro: 0.1540\n",
            "TargetModel Epoch: 005, Approx Train: 0.1651, Train: 0.1762, Test: 0.1667,marco: 0.1265,micro: 0.1667\n",
            "TargetModel Epoch: 006, Approx Train: 0.1762, Train: 0.1889, Test: 0.1778,marco: 0.1362,micro: 0.1778\n",
            "TargetModel Epoch: 007, Approx Train: 0.1889, Train: 0.2032, Test: 0.1810,marco: 0.1379,micro: 0.1810\n",
            "TargetModel Epoch: 008, Approx Train: 0.2032, Train: 0.2175, Test: 0.1937,marco: 0.1504,micro: 0.1937\n",
            "TargetModel Epoch: 009, Approx Train: 0.2175, Train: 0.2238, Test: 0.1952,marco: 0.1512,micro: 0.1952\n",
            "TargetModel Epoch: 010, Approx Train: 0.2238, Train: 0.2349, Test: 0.2079,marco: 0.1649,micro: 0.2079\n",
            "TargetModel Epoch: 011, Approx Train: 0.2349, Train: 0.2524, Test: 0.2175,marco: 0.1746,micro: 0.2175\n",
            "TargetModel Epoch: 012, Approx Train: 0.2524, Train: 0.2571, Test: 0.2270,marco: 0.1854,micro: 0.2270\n",
            "TargetModel Epoch: 013, Approx Train: 0.2571, Train: 0.2698, Test: 0.2349,marco: 0.1948,micro: 0.2349\n",
            "TargetModel Epoch: 014, Approx Train: 0.2698, Train: 0.2873, Test: 0.2444,marco: 0.2038,micro: 0.2444\n",
            "TargetModel Epoch: 015, Approx Train: 0.2873, Train: 0.3111, Test: 0.2476,marco: 0.2072,micro: 0.2476\n",
            "TargetModel Epoch: 016, Approx Train: 0.3111, Train: 0.3302, Test: 0.2683,marco: 0.2379,micro: 0.2683\n",
            "TargetModel Epoch: 017, Approx Train: 0.3302, Train: 0.3460, Test: 0.2873,marco: 0.2629,micro: 0.2873\n",
            "TargetModel Epoch: 018, Approx Train: 0.3460, Train: 0.3635, Test: 0.3063,marco: 0.2850,micro: 0.3063\n",
            "TargetModel Epoch: 019, Approx Train: 0.3635, Train: 0.3794, Test: 0.3254,marco: 0.3162,micro: 0.3254\n",
            "TargetModel Epoch: 020, Approx Train: 0.3794, Train: 0.3968, Test: 0.3349,marco: 0.3253,micro: 0.3349\n",
            "TargetModel Epoch: 021, Approx Train: 0.3968, Train: 0.4079, Test: 0.3460,marco: 0.3338,micro: 0.3460\n",
            "TargetModel Epoch: 022, Approx Train: 0.4079, Train: 0.4222, Test: 0.3651,marco: 0.3621,micro: 0.3651\n",
            "TargetModel Epoch: 023, Approx Train: 0.4222, Train: 0.4444, Test: 0.3841,marco: 0.3885,micro: 0.3841\n",
            "TargetModel Epoch: 024, Approx Train: 0.4444, Train: 0.4571, Test: 0.3952,marco: 0.3965,micro: 0.3952\n",
            "TargetModel Epoch: 025, Approx Train: 0.4571, Train: 0.4714, Test: 0.4095,marco: 0.4097,micro: 0.4095\n",
            "TargetModel Epoch: 026, Approx Train: 0.4714, Train: 0.4921, Test: 0.4302,marco: 0.4359,micro: 0.4302\n",
            "TargetModel Epoch: 027, Approx Train: 0.4921, Train: 0.5079, Test: 0.4381,marco: 0.4469,micro: 0.4381\n",
            "TargetModel Epoch: 028, Approx Train: 0.5079, Train: 0.5254, Test: 0.4524,marco: 0.4601,micro: 0.4524\n",
            "TargetModel Epoch: 029, Approx Train: 0.5254, Train: 0.5397, Test: 0.4746,marco: 0.4848,micro: 0.4746\n",
            "TargetModel Epoch: 030, Approx Train: 0.5397, Train: 0.5492, Test: 0.4905,marco: 0.5074,micro: 0.4905\n",
            "TargetModel Epoch: 031, Approx Train: 0.5492, Train: 0.5603, Test: 0.5063,marco: 0.5239,micro: 0.5063\n",
            "TargetModel Epoch: 032, Approx Train: 0.5603, Train: 0.5619, Test: 0.5190,marco: 0.5323,micro: 0.5190\n",
            "TargetModel Epoch: 033, Approx Train: 0.5619, Train: 0.5730, Test: 0.5317,marco: 0.5454,micro: 0.5317\n",
            "TargetModel Epoch: 034, Approx Train: 0.5730, Train: 0.5810, Test: 0.5540,marco: 0.5690,micro: 0.5540\n",
            "TargetModel Epoch: 035, Approx Train: 0.5810, Train: 0.5921, Test: 0.5683,marco: 0.5843,micro: 0.5683\n",
            "TargetModel Epoch: 036, Approx Train: 0.5921, Train: 0.6063, Test: 0.5841,marco: 0.5975,micro: 0.5841\n",
            "TargetModel Epoch: 037, Approx Train: 0.6063, Train: 0.6222, Test: 0.6000,marco: 0.6142,micro: 0.6000\n",
            "TargetModel Epoch: 038, Approx Train: 0.6222, Train: 0.6302, Test: 0.6175,marco: 0.6332,micro: 0.6175\n",
            "TargetModel Epoch: 039, Approx Train: 0.6302, Train: 0.6413, Test: 0.6222,marco: 0.6374,micro: 0.6222\n",
            "TargetModel Epoch: 040, Approx Train: 0.6413, Train: 0.6492, Test: 0.6302,marco: 0.6452,micro: 0.6302\n",
            "TargetModel Epoch: 041, Approx Train: 0.6492, Train: 0.6556, Test: 0.6429,marco: 0.6575,micro: 0.6429\n",
            "TargetModel Epoch: 042, Approx Train: 0.6556, Train: 0.6635, Test: 0.6556,marco: 0.6717,micro: 0.6556\n",
            "TargetModel Epoch: 043, Approx Train: 0.6635, Train: 0.6714, Test: 0.6698,marco: 0.6837,micro: 0.6698\n",
            "TargetModel Epoch: 044, Approx Train: 0.6714, Train: 0.6746, Test: 0.6746,marco: 0.6879,micro: 0.6746\n",
            "TargetModel Epoch: 045, Approx Train: 0.6746, Train: 0.6810, Test: 0.6810,marco: 0.6936,micro: 0.6810\n",
            "TargetModel Epoch: 046, Approx Train: 0.6810, Train: 0.6857, Test: 0.6873,marco: 0.6993,micro: 0.6873\n",
            "TargetModel Epoch: 047, Approx Train: 0.6857, Train: 0.6889, Test: 0.6873,marco: 0.6976,micro: 0.6873\n",
            "TargetModel Epoch: 048, Approx Train: 0.6889, Train: 0.6905, Test: 0.6937,marco: 0.7045,micro: 0.6937\n",
            "TargetModel Epoch: 049, Approx Train: 0.6905, Train: 0.6921, Test: 0.6968,marco: 0.7069,micro: 0.6968\n",
            "TargetModel Epoch: 050, Approx Train: 0.6921, Train: 0.6968, Test: 0.7079,marco: 0.7207,micro: 0.7079\n",
            "TargetModel Epoch: 051, Approx Train: 0.6968, Train: 0.7000, Test: 0.7095,marco: 0.7213,micro: 0.7095\n",
            "TargetModel Epoch: 052, Approx Train: 0.7000, Train: 0.7032, Test: 0.7095,marco: 0.7210,micro: 0.7095\n",
            "TargetModel Epoch: 053, Approx Train: 0.7032, Train: 0.7063, Test: 0.7206,marco: 0.7316,micro: 0.7206\n",
            "TargetModel Epoch: 054, Approx Train: 0.7063, Train: 0.7127, Test: 0.7270,marco: 0.7375,micro: 0.7270\n",
            "TargetModel Epoch: 055, Approx Train: 0.7127, Train: 0.7143, Test: 0.7270,marco: 0.7367,micro: 0.7270\n",
            "TargetModel Epoch: 056, Approx Train: 0.7143, Train: 0.7206, Test: 0.7286,marco: 0.7377,micro: 0.7286\n",
            "TargetModel Epoch: 057, Approx Train: 0.7206, Train: 0.7222, Test: 0.7286,marco: 0.7378,micro: 0.7286\n",
            "TargetModel Epoch: 058, Approx Train: 0.7222, Train: 0.7238, Test: 0.7286,marco: 0.7387,micro: 0.7286\n",
            "TargetModel Epoch: 059, Approx Train: 0.7238, Train: 0.7238, Test: 0.7302,marco: 0.7398,micro: 0.7302\n",
            "TargetModel Epoch: 060, Approx Train: 0.7238, Train: 0.7254, Test: 0.7349,marco: 0.7447,micro: 0.7349\n",
            "TargetModel Epoch: 061, Approx Train: 0.7254, Train: 0.7270, Test: 0.7413,marco: 0.7498,micro: 0.7413\n",
            "TargetModel Epoch: 062, Approx Train: 0.7270, Train: 0.7270, Test: 0.7460,marco: 0.7546,micro: 0.7460\n",
            "TargetModel Epoch: 063, Approx Train: 0.7270, Train: 0.7317, Test: 0.7508,marco: 0.7587,micro: 0.7508\n",
            "TargetModel Epoch: 064, Approx Train: 0.7317, Train: 0.7333, Test: 0.7556,marco: 0.7641,micro: 0.7556\n",
            "TargetModel Epoch: 065, Approx Train: 0.7333, Train: 0.7349, Test: 0.7587,marco: 0.7670,micro: 0.7587\n",
            "TargetModel Epoch: 066, Approx Train: 0.7349, Train: 0.7381, Test: 0.7619,marco: 0.7704,micro: 0.7619\n",
            "TargetModel Epoch: 067, Approx Train: 0.7381, Train: 0.7381, Test: 0.7635,marco: 0.7715,micro: 0.7635\n",
            "TargetModel Epoch: 068, Approx Train: 0.7381, Train: 0.7381, Test: 0.7651,marco: 0.7728,micro: 0.7651\n",
            "TargetModel Epoch: 069, Approx Train: 0.7381, Train: 0.7413, Test: 0.7667,marco: 0.7739,micro: 0.7667\n",
            "TargetModel Epoch: 070, Approx Train: 0.7413, Train: 0.7429, Test: 0.7683,marco: 0.7752,micro: 0.7683\n",
            "TargetModel Epoch: 071, Approx Train: 0.7429, Train: 0.7444, Test: 0.7683,marco: 0.7752,micro: 0.7683\n",
            "TargetModel Epoch: 072, Approx Train: 0.7444, Train: 0.7444, Test: 0.7698,marco: 0.7775,micro: 0.7698\n",
            "TargetModel Epoch: 073, Approx Train: 0.7444, Train: 0.7460, Test: 0.7698,marco: 0.7775,micro: 0.7698\n",
            "TargetModel Epoch: 074, Approx Train: 0.7460, Train: 0.7476, Test: 0.7698,marco: 0.7768,micro: 0.7698\n",
            "TargetModel Epoch: 075, Approx Train: 0.7476, Train: 0.7492, Test: 0.7698,marco: 0.7777,micro: 0.7698\n",
            "TargetModel Epoch: 076, Approx Train: 0.7492, Train: 0.7508, Test: 0.7762,marco: 0.7844,micro: 0.7762\n",
            "TargetModel Epoch: 077, Approx Train: 0.7508, Train: 0.7540, Test: 0.7762,marco: 0.7844,micro: 0.7762\n",
            "TargetModel Epoch: 078, Approx Train: 0.7540, Train: 0.7540, Test: 0.7810,marco: 0.7885,micro: 0.7810\n",
            "TargetModel Epoch: 079, Approx Train: 0.7540, Train: 0.7556, Test: 0.7825,marco: 0.7896,micro: 0.7825\n",
            "TargetModel Epoch: 080, Approx Train: 0.7556, Train: 0.7587, Test: 0.7825,marco: 0.7896,micro: 0.7825\n",
            "TargetModel Epoch: 081, Approx Train: 0.7587, Train: 0.7603, Test: 0.7825,marco: 0.7896,micro: 0.7825\n",
            "TargetModel Epoch: 082, Approx Train: 0.7603, Train: 0.7603, Test: 0.7825,marco: 0.7896,micro: 0.7825\n",
            "TargetModel Epoch: 083, Approx Train: 0.7603, Train: 0.7603, Test: 0.7841,marco: 0.7900,micro: 0.7841\n",
            "TargetModel Epoch: 084, Approx Train: 0.7603, Train: 0.7603, Test: 0.7841,marco: 0.7901,micro: 0.7841\n",
            "TargetModel Epoch: 085, Approx Train: 0.7603, Train: 0.7603, Test: 0.7841,marco: 0.7901,micro: 0.7841\n",
            "TargetModel Epoch: 086, Approx Train: 0.7603, Train: 0.7603, Test: 0.7841,marco: 0.7899,micro: 0.7841\n",
            "TargetModel Epoch: 087, Approx Train: 0.7603, Train: 0.7587, Test: 0.7841,marco: 0.7899,micro: 0.7841\n",
            "TargetModel Epoch: 088, Approx Train: 0.7587, Train: 0.7587, Test: 0.7841,marco: 0.7899,micro: 0.7841\n",
            "TargetModel Epoch: 089, Approx Train: 0.7587, Train: 0.7571, Test: 0.7825,marco: 0.7888,micro: 0.7825\n",
            "TargetModel Epoch: 090, Approx Train: 0.7571, Train: 0.7571, Test: 0.7825,marco: 0.7889,micro: 0.7825\n",
            "TargetModel Epoch: 091, Approx Train: 0.7571, Train: 0.7571, Test: 0.7825,marco: 0.7889,micro: 0.7825\n",
            "TargetModel Epoch: 092, Approx Train: 0.7571, Train: 0.7571, Test: 0.7825,marco: 0.7889,micro: 0.7825\n",
            "TargetModel Epoch: 093, Approx Train: 0.7571, Train: 0.7571, Test: 0.7825,marco: 0.7889,micro: 0.7825\n",
            "TargetModel Epoch: 094, Approx Train: 0.7571, Train: 0.7556, Test: 0.7825,marco: 0.7889,micro: 0.7825\n",
            "TargetModel Epoch: 095, Approx Train: 0.7556, Train: 0.7587, Test: 0.7825,marco: 0.7886,micro: 0.7825\n",
            "TargetModel Epoch: 096, Approx Train: 0.7587, Train: 0.7587, Test: 0.7873,marco: 0.7941,micro: 0.7873\n",
            "TargetModel Epoch: 097, Approx Train: 0.7587, Train: 0.7587, Test: 0.7873,marco: 0.7950,micro: 0.7873\n",
            "TargetModel Epoch: 098, Approx Train: 0.7587, Train: 0.7587, Test: 0.7873,marco: 0.7950,micro: 0.7873\n",
            "TargetModel Epoch: 099, Approx Train: 0.7587, Train: 0.7587, Test: 0.7921,marco: 0.7996,micro: 0.7921\n",
            "TargetModel Epoch: 100, Approx Train: 0.7587, Train: 0.7587, Test: 0.7937,marco: 0.8010,micro: 0.7937\n",
            "TargetModel Epoch: 101, Approx Train: 0.7587, Train: 0.7587, Test: 0.7937,marco: 0.8010,micro: 0.7937\n",
            "TargetModel Epoch: 102, Approx Train: 0.7587, Train: 0.7587, Test: 0.7952,marco: 0.8030,micro: 0.7952\n",
            "TargetModel Epoch: 103, Approx Train: 0.7587, Train: 0.7571, Test: 0.7952,marco: 0.8030,micro: 0.7952\n",
            "TargetModel Epoch: 104, Approx Train: 0.7571, Train: 0.7587, Test: 0.7952,marco: 0.8034,micro: 0.7952\n",
            "TargetModel Epoch: 105, Approx Train: 0.7587, Train: 0.7587, Test: 0.7952,marco: 0.8034,micro: 0.7952\n",
            "TargetModel Epoch: 106, Approx Train: 0.7587, Train: 0.7603, Test: 0.7968,marco: 0.8047,micro: 0.7968\n",
            "TargetModel Epoch: 107, Approx Train: 0.7603, Train: 0.7603, Test: 0.7968,marco: 0.8047,micro: 0.7968\n",
            "TargetModel Epoch: 108, Approx Train: 0.7603, Train: 0.7603, Test: 0.7952,marco: 0.8025,micro: 0.7952\n",
            "TargetModel Epoch: 109, Approx Train: 0.7603, Train: 0.7603, Test: 0.7952,marco: 0.8025,micro: 0.7952\n",
            "TargetModel Epoch: 110, Approx Train: 0.7603, Train: 0.7619, Test: 0.7968,marco: 0.8038,micro: 0.7968\n",
            "TargetModel Epoch: 111, Approx Train: 0.7619, Train: 0.7635, Test: 0.7968,marco: 0.8038,micro: 0.7968\n",
            "TargetModel Epoch: 112, Approx Train: 0.7635, Train: 0.7635, Test: 0.7968,marco: 0.8038,micro: 0.7968\n",
            "TargetModel Epoch: 113, Approx Train: 0.7635, Train: 0.7635, Test: 0.7984,marco: 0.8055,micro: 0.7984\n",
            "TargetModel Epoch: 114, Approx Train: 0.7635, Train: 0.7635, Test: 0.7984,marco: 0.8045,micro: 0.7984\n",
            "TargetModel Epoch: 115, Approx Train: 0.7635, Train: 0.7651, Test: 0.8000,marco: 0.8055,micro: 0.8000\n",
            "TargetModel Epoch: 116, Approx Train: 0.7651, Train: 0.7651, Test: 0.8032,marco: 0.8080,micro: 0.8032\n",
            "TargetModel Epoch: 117, Approx Train: 0.7651, Train: 0.7651, Test: 0.8032,marco: 0.8080,micro: 0.8032\n",
            "TargetModel Epoch: 118, Approx Train: 0.7651, Train: 0.7667, Test: 0.8063,marco: 0.8104,micro: 0.8063\n",
            "TargetModel Epoch: 119, Approx Train: 0.7667, Train: 0.7651, Test: 0.8095,marco: 0.8132,micro: 0.8095\n",
            "TargetModel Epoch: 120, Approx Train: 0.7651, Train: 0.7667, Test: 0.8111,marco: 0.8143,micro: 0.8111\n",
            "TargetModel Epoch: 121, Approx Train: 0.7667, Train: 0.7667, Test: 0.8127,marco: 0.8157,micro: 0.8127\n",
            "TargetModel Epoch: 122, Approx Train: 0.7667, Train: 0.7667, Test: 0.8143,marco: 0.8168,micro: 0.8143\n",
            "TargetModel Epoch: 123, Approx Train: 0.7667, Train: 0.7651, Test: 0.8143,marco: 0.8168,micro: 0.8143\n",
            "TargetModel Epoch: 124, Approx Train: 0.7651, Train: 0.7651, Test: 0.8159,marco: 0.8186,micro: 0.8159\n",
            "TargetModel Epoch: 125, Approx Train: 0.7651, Train: 0.7635, Test: 0.8175,marco: 0.8204,micro: 0.8175\n",
            "TargetModel Epoch: 126, Approx Train: 0.7635, Train: 0.7635, Test: 0.8175,marco: 0.8204,micro: 0.8175\n",
            "TargetModel Epoch: 127, Approx Train: 0.7635, Train: 0.7635, Test: 0.8206,marco: 0.8240,micro: 0.8206\n",
            "TargetModel Epoch: 128, Approx Train: 0.7635, Train: 0.7635, Test: 0.8222,marco: 0.8251,micro: 0.8222\n",
            "TargetModel Epoch: 129, Approx Train: 0.7635, Train: 0.7635, Test: 0.8206,marco: 0.8242,micro: 0.8206\n",
            "TargetModel Epoch: 130, Approx Train: 0.7635, Train: 0.7635, Test: 0.8206,marco: 0.8242,micro: 0.8206\n",
            "TargetModel Epoch: 131, Approx Train: 0.7635, Train: 0.7635, Test: 0.8222,marco: 0.8253,micro: 0.8222\n",
            "TargetModel Epoch: 132, Approx Train: 0.7635, Train: 0.7651, Test: 0.8238,marco: 0.8266,micro: 0.8238\n",
            "TargetModel Epoch: 133, Approx Train: 0.7651, Train: 0.7651, Test: 0.8254,marco: 0.8288,micro: 0.8254\n",
            "TargetModel Epoch: 134, Approx Train: 0.7651, Train: 0.7651, Test: 0.8254,marco: 0.8288,micro: 0.8254\n",
            "TargetModel Epoch: 135, Approx Train: 0.7651, Train: 0.7651, Test: 0.8270,marco: 0.8299,micro: 0.8270\n",
            "TargetModel Epoch: 136, Approx Train: 0.7651, Train: 0.7667, Test: 0.8270,marco: 0.8299,micro: 0.8270\n",
            "TargetModel Epoch: 137, Approx Train: 0.7667, Train: 0.7667, Test: 0.8270,marco: 0.8299,micro: 0.8270\n",
            "TargetModel Epoch: 138, Approx Train: 0.7667, Train: 0.7667, Test: 0.8270,marco: 0.8299,micro: 0.8270\n",
            "TargetModel Epoch: 139, Approx Train: 0.7667, Train: 0.7667, Test: 0.8270,marco: 0.8299,micro: 0.8270\n",
            "TargetModel Epoch: 140, Approx Train: 0.7667, Train: 0.7667, Test: 0.8270,marco: 0.8299,micro: 0.8270\n",
            "TargetModel Epoch: 141, Approx Train: 0.7667, Train: 0.7683, Test: 0.8270,marco: 0.8295,micro: 0.8270\n",
            "TargetModel Epoch: 142, Approx Train: 0.7683, Train: 0.7698, Test: 0.8270,marco: 0.8295,micro: 0.8270\n",
            "TargetModel Epoch: 143, Approx Train: 0.7698, Train: 0.7683, Test: 0.8270,marco: 0.8289,micro: 0.8270\n",
            "TargetModel Epoch: 144, Approx Train: 0.7683, Train: 0.7683, Test: 0.8270,marco: 0.8289,micro: 0.8270\n",
            "TargetModel Epoch: 145, Approx Train: 0.7683, Train: 0.7683, Test: 0.8254,marco: 0.8278,micro: 0.8254\n",
            "TargetModel Epoch: 146, Approx Train: 0.7683, Train: 0.7683, Test: 0.8254,marco: 0.8278,micro: 0.8254\n",
            "TargetModel Epoch: 147, Approx Train: 0.7683, Train: 0.7683, Test: 0.8254,marco: 0.8278,micro: 0.8254\n",
            "TargetModel Epoch: 148, Approx Train: 0.7683, Train: 0.7683, Test: 0.8270,marco: 0.8301,micro: 0.8270\n",
            "TargetModel Epoch: 149, Approx Train: 0.7683, Train: 0.7683, Test: 0.8270,marco: 0.8301,micro: 0.8270\n",
            "TargetModel Epoch: 150, Approx Train: 0.7683, Train: 0.7683, Test: 0.8286,marco: 0.8312,micro: 0.8286\n",
            "TargetModel Epoch: 151, Approx Train: 0.7683, Train: 0.7683, Test: 0.8286,marco: 0.8312,micro: 0.8286\n",
            "TargetModel Epoch: 152, Approx Train: 0.7683, Train: 0.7683, Test: 0.8286,marco: 0.8312,micro: 0.8286\n",
            "TargetModel Epoch: 153, Approx Train: 0.7683, Train: 0.7714, Test: 0.8286,marco: 0.8312,micro: 0.8286\n",
            "TargetModel Epoch: 154, Approx Train: 0.7714, Train: 0.7714, Test: 0.8286,marco: 0.8316,micro: 0.8286\n",
            "TargetModel Epoch: 155, Approx Train: 0.7714, Train: 0.7714, Test: 0.8286,marco: 0.8316,micro: 0.8286\n",
            "TargetModel Epoch: 156, Approx Train: 0.7714, Train: 0.7714, Test: 0.8286,marco: 0.8316,micro: 0.8286\n",
            "TargetModel Epoch: 157, Approx Train: 0.7714, Train: 0.7714, Test: 0.8302,marco: 0.8330,micro: 0.8302\n",
            "TargetModel Epoch: 158, Approx Train: 0.7714, Train: 0.7714, Test: 0.8302,marco: 0.8330,micro: 0.8302\n",
            "TargetModel Epoch: 159, Approx Train: 0.7714, Train: 0.7714, Test: 0.8302,marco: 0.8330,micro: 0.8302\n",
            "TargetModel Epoch: 160, Approx Train: 0.7714, Train: 0.7714, Test: 0.8302,marco: 0.8330,micro: 0.8302\n",
            "TargetModel Epoch: 161, Approx Train: 0.7714, Train: 0.7714, Test: 0.8302,marco: 0.8330,micro: 0.8302\n",
            "TargetModel Epoch: 162, Approx Train: 0.7714, Train: 0.7714, Test: 0.8302,marco: 0.8330,micro: 0.8302\n",
            "TargetModel Epoch: 163, Approx Train: 0.7714, Train: 0.7714, Test: 0.8317,marco: 0.8341,micro: 0.8317\n",
            "TargetModel Epoch: 164, Approx Train: 0.7714, Train: 0.7746, Test: 0.8317,marco: 0.8341,micro: 0.8317\n",
            "TargetModel Epoch: 165, Approx Train: 0.7746, Train: 0.7746, Test: 0.8317,marco: 0.8341,micro: 0.8317\n",
            "TargetModel Epoch: 166, Approx Train: 0.7746, Train: 0.7762, Test: 0.8317,marco: 0.8341,micro: 0.8317\n",
            "TargetModel Epoch: 167, Approx Train: 0.7762, Train: 0.7778, Test: 0.8317,marco: 0.8341,micro: 0.8317\n",
            "TargetModel Epoch: 168, Approx Train: 0.7778, Train: 0.7778, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 169, Approx Train: 0.7778, Train: 0.7778, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 170, Approx Train: 0.7778, Train: 0.7778, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 171, Approx Train: 0.7778, Train: 0.7778, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 172, Approx Train: 0.7778, Train: 0.7794, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 173, Approx Train: 0.7794, Train: 0.7794, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 174, Approx Train: 0.7794, Train: 0.7794, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 175, Approx Train: 0.7794, Train: 0.7794, Test: 0.8349,marco: 0.8362,micro: 0.8349\n",
            "TargetModel Epoch: 176, Approx Train: 0.7794, Train: 0.7794, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 177, Approx Train: 0.7794, Train: 0.7794, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 178, Approx Train: 0.7794, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 179, Approx Train: 0.7810, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 180, Approx Train: 0.7810, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 181, Approx Train: 0.7810, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 182, Approx Train: 0.7810, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 183, Approx Train: 0.7810, Train: 0.7794, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 184, Approx Train: 0.7794, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 185, Approx Train: 0.7810, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 186, Approx Train: 0.7810, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 187, Approx Train: 0.7810, Train: 0.7810, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 188, Approx Train: 0.7810, Train: 0.7841, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 189, Approx Train: 0.7841, Train: 0.7841, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 190, Approx Train: 0.7841, Train: 0.7841, Test: 0.8365,marco: 0.8381,micro: 0.8365\n",
            "TargetModel Epoch: 191, Approx Train: 0.7841, Train: 0.7873, Test: 0.8381,marco: 0.8395,micro: 0.8381\n",
            "TargetModel Epoch: 192, Approx Train: 0.7873, Train: 0.7873, Test: 0.8381,marco: 0.8395,micro: 0.8381\n",
            "TargetModel Epoch: 193, Approx Train: 0.7873, Train: 0.7873, Test: 0.8381,marco: 0.8395,micro: 0.8381\n",
            "TargetModel Epoch: 194, Approx Train: 0.7873, Train: 0.7873, Test: 0.8381,marco: 0.8395,micro: 0.8381\n",
            "TargetModel Epoch: 195, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 196, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 197, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 198, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 199, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 200, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 201, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 202, Approx Train: 0.7873, Train: 0.7857, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 203, Approx Train: 0.7857, Train: 0.7857, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 204, Approx Train: 0.7857, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 205, Approx Train: 0.7873, Train: 0.7857, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 206, Approx Train: 0.7857, Train: 0.7857, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 207, Approx Train: 0.7857, Train: 0.7857, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 208, Approx Train: 0.7857, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 209, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 210, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 211, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 212, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 213, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 214, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8410,micro: 0.8397\n",
            "TargetModel Epoch: 215, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8407,micro: 0.8397\n",
            "TargetModel Epoch: 216, Approx Train: 0.7873, Train: 0.7873, Test: 0.8397,marco: 0.8407,micro: 0.8397\n",
            "TargetModel Epoch: 217, Approx Train: 0.7873, Train: 0.7889, Test: 0.8397,marco: 0.8407,micro: 0.8397\n",
            "TargetModel Epoch: 218, Approx Train: 0.7889, Train: 0.7889, Test: 0.8397,marco: 0.8407,micro: 0.8397\n",
            "TargetModel Epoch: 219, Approx Train: 0.7889, Train: 0.7889, Test: 0.8397,marco: 0.8407,micro: 0.8397\n",
            "TargetModel Epoch: 220, Approx Train: 0.7889, Train: 0.7905, Test: 0.8397,marco: 0.8407,micro: 0.8397\n",
            "TargetModel Epoch: 221, Approx Train: 0.7905, Train: 0.7905, Test: 0.8413,marco: 0.8429,micro: 0.8413\n",
            "TargetModel Epoch: 222, Approx Train: 0.7905, Train: 0.7889, Test: 0.8413,marco: 0.8429,micro: 0.8413\n",
            "TargetModel Epoch: 223, Approx Train: 0.7889, Train: 0.7905, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 224, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 225, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 226, Approx Train: 0.7905, Train: 0.7889, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 227, Approx Train: 0.7889, Train: 0.7905, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 228, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 229, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 230, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8444,micro: 0.8429\n",
            "TargetModel Epoch: 231, Approx Train: 0.7905, Train: 0.7905, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 232, Approx Train: 0.7905, Train: 0.7905, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 233, Approx Train: 0.7905, Train: 0.7905, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 234, Approx Train: 0.7905, Train: 0.7905, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 235, Approx Train: 0.7905, Train: 0.7905, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 236, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8449,micro: 0.8429\n",
            "TargetModel Epoch: 237, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8449,micro: 0.8429\n",
            "TargetModel Epoch: 238, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8449,micro: 0.8429\n",
            "TargetModel Epoch: 239, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8449,micro: 0.8429\n",
            "TargetModel Epoch: 240, Approx Train: 0.7905, Train: 0.7889, Test: 0.8429,marco: 0.8449,micro: 0.8429\n",
            "TargetModel Epoch: 241, Approx Train: 0.7889, Train: 0.7905, Test: 0.8429,marco: 0.8449,micro: 0.8429\n",
            "TargetModel Epoch: 242, Approx Train: 0.7905, Train: 0.7905, Test: 0.8429,marco: 0.8449,micro: 0.8429\n",
            "TargetModel Epoch: 243, Approx Train: 0.7905, Train: 0.7921, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 244, Approx Train: 0.7921, Train: 0.7921, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 245, Approx Train: 0.7921, Train: 0.7937, Test: 0.8444,marco: 0.8459,micro: 0.8444\n",
            "TargetModel Epoch: 246, Approx Train: 0.7937, Train: 0.7937, Test: 0.8460,marco: 0.8470,micro: 0.8460\n",
            "TargetModel Epoch: 247, Approx Train: 0.7937, Train: 0.7937, Test: 0.8460,marco: 0.8470,micro: 0.8460\n",
            "TargetModel Epoch: 248, Approx Train: 0.7937, Train: 0.7952, Test: 0.8460,marco: 0.8470,micro: 0.8460\n",
            "TargetModel Epoch: 249, Approx Train: 0.7952, Train: 0.7968, Test: 0.8460,marco: 0.8470,micro: 0.8460\n",
            "TargetModel Epoch: 250, Approx Train: 0.7968, Train: 0.7968, Test: 0.8460,marco: 0.8470,micro: 0.8460\n",
            "TargetModel Epoch: 251, Approx Train: 0.7968, Train: 0.7968, Test: 0.8476,marco: 0.8480,micro: 0.8476\n",
            "TargetModel Epoch: 252, Approx Train: 0.7968, Train: 0.8000, Test: 0.8492,marco: 0.8496,micro: 0.8492\n",
            "TargetModel Epoch: 253, Approx Train: 0.8000, Train: 0.8000, Test: 0.8492,marco: 0.8496,micro: 0.8492\n",
            "TargetModel Epoch: 254, Approx Train: 0.8000, Train: 0.8000, Test: 0.8492,marco: 0.8496,micro: 0.8492\n",
            "TargetModel Epoch: 255, Approx Train: 0.8000, Train: 0.8000, Test: 0.8492,marco: 0.8496,micro: 0.8492\n",
            "TargetModel Epoch: 256, Approx Train: 0.8000, Train: 0.8000, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 257, Approx Train: 0.8000, Train: 0.8016, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 258, Approx Train: 0.8016, Train: 0.8016, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 259, Approx Train: 0.8016, Train: 0.8032, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 260, Approx Train: 0.8032, Train: 0.8032, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 261, Approx Train: 0.8032, Train: 0.8032, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 262, Approx Train: 0.8032, Train: 0.8032, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 263, Approx Train: 0.8032, Train: 0.8032, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 264, Approx Train: 0.8032, Train: 0.8032, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 265, Approx Train: 0.8032, Train: 0.8048, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 266, Approx Train: 0.8048, Train: 0.8048, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 267, Approx Train: 0.8048, Train: 0.8048, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 268, Approx Train: 0.8048, Train: 0.8063, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 269, Approx Train: 0.8063, Train: 0.8063, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 270, Approx Train: 0.8063, Train: 0.8063, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 271, Approx Train: 0.8063, Train: 0.8063, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 272, Approx Train: 0.8063, Train: 0.8063, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 273, Approx Train: 0.8063, Train: 0.8079, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 274, Approx Train: 0.8079, Train: 0.8079, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 275, Approx Train: 0.8079, Train: 0.8079, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 276, Approx Train: 0.8079, Train: 0.8079, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 277, Approx Train: 0.8079, Train: 0.8079, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 278, Approx Train: 0.8079, Train: 0.8079, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 279, Approx Train: 0.8079, Train: 0.8095, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 280, Approx Train: 0.8095, Train: 0.8095, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 281, Approx Train: 0.8095, Train: 0.8095, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 282, Approx Train: 0.8095, Train: 0.8095, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 283, Approx Train: 0.8095, Train: 0.8095, Test: 0.8476,marco: 0.8485,micro: 0.8476\n",
            "TargetModel Epoch: 284, Approx Train: 0.8095, Train: 0.8095, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 285, Approx Train: 0.8095, Train: 0.8111, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 286, Approx Train: 0.8111, Train: 0.8111, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 287, Approx Train: 0.8111, Train: 0.8127, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 288, Approx Train: 0.8127, Train: 0.8159, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 289, Approx Train: 0.8159, Train: 0.8175, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 290, Approx Train: 0.8175, Train: 0.8175, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 291, Approx Train: 0.8175, Train: 0.8175, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 292, Approx Train: 0.8175, Train: 0.8190, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 293, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8482,micro: 0.8476\n",
            "TargetModel Epoch: 294, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8478,micro: 0.8476\n",
            "TargetModel Epoch: 295, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8478,micro: 0.8476\n",
            "TargetModel Epoch: 296, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8478,micro: 0.8476\n",
            "TargetModel Epoch: 297, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8478,micro: 0.8476\n",
            "TargetModel Epoch: 298, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8478,micro: 0.8476\n",
            "TargetModel Epoch: 299, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8478,micro: 0.8476\n",
            "TargetModel Epoch: 300, Approx Train: 0.8190, Train: 0.8190, Test: 0.8476,marco: 0.8478,micro: 0.8476\n",
            "\n",
            "=========================================================End Target Train ==============================\n",
            "ShadowModel Epoch: 001, Approx Train: 0.1921, Train: 0.2016, Test: 0.1556,marco: 0.1353,micro: 0.1556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:963: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:979: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:673: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ShadowModel Epoch: 002, Approx Train: 0.2016, Train: 0.2079, Test: 0.1651,marco: 0.1426,micro: 0.1651\n",
            "ShadowModel Epoch: 003, Approx Train: 0.2079, Train: 0.2159, Test: 0.1762,marco: 0.1504,micro: 0.1762\n",
            "ShadowModel Epoch: 004, Approx Train: 0.2159, Train: 0.2254, Test: 0.1857,marco: 0.1637,micro: 0.1857\n",
            "ShadowModel Epoch: 005, Approx Train: 0.2254, Train: 0.2333, Test: 0.1905,marco: 0.1685,micro: 0.1905\n",
            "ShadowModel Epoch: 006, Approx Train: 0.2333, Train: 0.2476, Test: 0.2016,marco: 0.1778,micro: 0.2016\n",
            "ShadowModel Epoch: 007, Approx Train: 0.2476, Train: 0.2603, Test: 0.2190,marco: 0.1992,micro: 0.2190\n",
            "ShadowModel Epoch: 008, Approx Train: 0.2603, Train: 0.2683, Test: 0.2222,marco: 0.2005,micro: 0.2222\n",
            "ShadowModel Epoch: 009, Approx Train: 0.2683, Train: 0.2825, Test: 0.2397,marco: 0.2215,micro: 0.2397\n",
            "ShadowModel Epoch: 010, Approx Train: 0.2825, Train: 0.2984, Test: 0.2492,marco: 0.2293,micro: 0.2492\n",
            "ShadowModel Epoch: 011, Approx Train: 0.2984, Train: 0.3111, Test: 0.2571,marco: 0.2397,micro: 0.2571\n",
            "ShadowModel Epoch: 012, Approx Train: 0.3111, Train: 0.3270, Test: 0.2746,marco: 0.2655,micro: 0.2746\n",
            "ShadowModel Epoch: 013, Approx Train: 0.3270, Train: 0.3492, Test: 0.2968,marco: 0.2945,micro: 0.2968\n",
            "ShadowModel Epoch: 014, Approx Train: 0.3492, Train: 0.3587, Test: 0.3016,marco: 0.2995,micro: 0.3016\n",
            "ShadowModel Epoch: 015, Approx Train: 0.3587, Train: 0.3698, Test: 0.3206,marco: 0.3159,micro: 0.3206\n",
            "ShadowModel Epoch: 016, Approx Train: 0.3698, Train: 0.3857, Test: 0.3302,marco: 0.3273,micro: 0.3302\n",
            "ShadowModel Epoch: 017, Approx Train: 0.3857, Train: 0.3968, Test: 0.3476,marco: 0.3494,micro: 0.3476\n",
            "ShadowModel Epoch: 018, Approx Train: 0.3968, Train: 0.4143, Test: 0.3556,marco: 0.3586,micro: 0.3556\n",
            "ShadowModel Epoch: 019, Approx Train: 0.4143, Train: 0.4286, Test: 0.3714,marco: 0.3794,micro: 0.3714\n",
            "ShadowModel Epoch: 020, Approx Train: 0.4286, Train: 0.4444, Test: 0.3825,marco: 0.3927,micro: 0.3825\n",
            "ShadowModel Epoch: 021, Approx Train: 0.4444, Train: 0.4619, Test: 0.3905,marco: 0.4012,micro: 0.3905\n",
            "ShadowModel Epoch: 022, Approx Train: 0.4619, Train: 0.4714, Test: 0.4048,marco: 0.4174,micro: 0.4048\n",
            "ShadowModel Epoch: 023, Approx Train: 0.4714, Train: 0.4905, Test: 0.4159,marco: 0.4300,micro: 0.4159\n",
            "ShadowModel Epoch: 024, Approx Train: 0.4905, Train: 0.5143, Test: 0.4206,marco: 0.4336,micro: 0.4206\n",
            "ShadowModel Epoch: 025, Approx Train: 0.5143, Train: 0.5317, Test: 0.4333,marco: 0.4464,micro: 0.4333\n",
            "ShadowModel Epoch: 026, Approx Train: 0.5317, Train: 0.5381, Test: 0.4444,marco: 0.4602,micro: 0.4444\n",
            "ShadowModel Epoch: 027, Approx Train: 0.5381, Train: 0.5429, Test: 0.4571,marco: 0.4722,micro: 0.4571\n",
            "ShadowModel Epoch: 028, Approx Train: 0.5429, Train: 0.5508, Test: 0.4730,marco: 0.4882,micro: 0.4730\n",
            "ShadowModel Epoch: 029, Approx Train: 0.5508, Train: 0.5635, Test: 0.4825,marco: 0.4981,micro: 0.4825\n",
            "ShadowModel Epoch: 030, Approx Train: 0.5635, Train: 0.5683, Test: 0.4921,marco: 0.5077,micro: 0.4921\n",
            "ShadowModel Epoch: 031, Approx Train: 0.5683, Train: 0.5762, Test: 0.5016,marco: 0.5168,micro: 0.5016\n",
            "ShadowModel Epoch: 032, Approx Train: 0.5762, Train: 0.5937, Test: 0.5143,marco: 0.5292,micro: 0.5143\n",
            "ShadowModel Epoch: 033, Approx Train: 0.5937, Train: 0.5984, Test: 0.5254,marco: 0.5384,micro: 0.5254\n",
            "ShadowModel Epoch: 034, Approx Train: 0.5984, Train: 0.5984, Test: 0.5317,marco: 0.5442,micro: 0.5317\n",
            "ShadowModel Epoch: 035, Approx Train: 0.5984, Train: 0.6143, Test: 0.5429,marco: 0.5526,micro: 0.5429\n",
            "ShadowModel Epoch: 036, Approx Train: 0.6143, Train: 0.6222, Test: 0.5524,marco: 0.5638,micro: 0.5524\n",
            "ShadowModel Epoch: 037, Approx Train: 0.6222, Train: 0.6365, Test: 0.5619,marco: 0.5745,micro: 0.5619\n",
            "ShadowModel Epoch: 038, Approx Train: 0.6365, Train: 0.6444, Test: 0.5730,marco: 0.5861,micro: 0.5730\n",
            "ShadowModel Epoch: 039, Approx Train: 0.6444, Train: 0.6508, Test: 0.5794,marco: 0.5928,micro: 0.5794\n",
            "ShadowModel Epoch: 040, Approx Train: 0.6508, Train: 0.6571, Test: 0.5921,marco: 0.6051,micro: 0.5921\n",
            "ShadowModel Epoch: 041, Approx Train: 0.6571, Train: 0.6635, Test: 0.5952,marco: 0.6079,micro: 0.5952\n",
            "ShadowModel Epoch: 042, Approx Train: 0.6635, Train: 0.6683, Test: 0.6016,marco: 0.6138,micro: 0.6016\n",
            "ShadowModel Epoch: 043, Approx Train: 0.6683, Train: 0.6730, Test: 0.6095,marco: 0.6211,micro: 0.6095\n",
            "ShadowModel Epoch: 044, Approx Train: 0.6730, Train: 0.6762, Test: 0.6175,marco: 0.6290,micro: 0.6175\n",
            "ShadowModel Epoch: 045, Approx Train: 0.6762, Train: 0.6794, Test: 0.6222,marco: 0.6336,micro: 0.6222\n",
            "ShadowModel Epoch: 046, Approx Train: 0.6794, Train: 0.6794, Test: 0.6365,marco: 0.6451,micro: 0.6365\n",
            "ShadowModel Epoch: 047, Approx Train: 0.6794, Train: 0.6810, Test: 0.6476,marco: 0.6555,micro: 0.6476\n",
            "ShadowModel Epoch: 048, Approx Train: 0.6810, Train: 0.6825, Test: 0.6524,marco: 0.6592,micro: 0.6524\n",
            "ShadowModel Epoch: 049, Approx Train: 0.6825, Train: 0.6857, Test: 0.6556,marco: 0.6612,micro: 0.6556\n",
            "ShadowModel Epoch: 050, Approx Train: 0.6857, Train: 0.6889, Test: 0.6571,marco: 0.6620,micro: 0.6571\n",
            "ShadowModel Epoch: 051, Approx Train: 0.6889, Train: 0.6937, Test: 0.6698,marco: 0.6755,micro: 0.6698\n",
            "ShadowModel Epoch: 052, Approx Train: 0.6937, Train: 0.6952, Test: 0.6762,marco: 0.6801,micro: 0.6762\n",
            "ShadowModel Epoch: 053, Approx Train: 0.6952, Train: 0.6952, Test: 0.6810,marco: 0.6863,micro: 0.6810\n",
            "ShadowModel Epoch: 054, Approx Train: 0.6952, Train: 0.7000, Test: 0.6937,marco: 0.6968,micro: 0.6937\n",
            "ShadowModel Epoch: 055, Approx Train: 0.7000, Train: 0.7016, Test: 0.6968,marco: 0.6992,micro: 0.6968\n",
            "ShadowModel Epoch: 056, Approx Train: 0.7016, Train: 0.7048, Test: 0.7016,marco: 0.7038,micro: 0.7016\n",
            "ShadowModel Epoch: 057, Approx Train: 0.7048, Train: 0.7048, Test: 0.7079,marco: 0.7087,micro: 0.7079\n",
            "ShadowModel Epoch: 058, Approx Train: 0.7048, Train: 0.7048, Test: 0.7111,marco: 0.7127,micro: 0.7111\n",
            "ShadowModel Epoch: 059, Approx Train: 0.7048, Train: 0.7063, Test: 0.7143,marco: 0.7157,micro: 0.7143\n",
            "ShadowModel Epoch: 060, Approx Train: 0.7063, Train: 0.7079, Test: 0.7159,marco: 0.7169,micro: 0.7159\n",
            "ShadowModel Epoch: 061, Approx Train: 0.7079, Train: 0.7079, Test: 0.7222,marco: 0.7234,micro: 0.7222\n",
            "ShadowModel Epoch: 062, Approx Train: 0.7079, Train: 0.7095, Test: 0.7302,marco: 0.7306,micro: 0.7302\n",
            "ShadowModel Epoch: 063, Approx Train: 0.7095, Train: 0.7111, Test: 0.7333,marco: 0.7338,micro: 0.7333\n",
            "ShadowModel Epoch: 064, Approx Train: 0.7111, Train: 0.7095, Test: 0.7333,marco: 0.7336,micro: 0.7333\n",
            "ShadowModel Epoch: 065, Approx Train: 0.7095, Train: 0.7095, Test: 0.7333,marco: 0.7336,micro: 0.7333\n",
            "ShadowModel Epoch: 066, Approx Train: 0.7095, Train: 0.7127, Test: 0.7333,marco: 0.7336,micro: 0.7333\n",
            "ShadowModel Epoch: 067, Approx Train: 0.7127, Train: 0.7127, Test: 0.7349,marco: 0.7351,micro: 0.7349\n",
            "ShadowModel Epoch: 068, Approx Train: 0.7127, Train: 0.7127, Test: 0.7444,marco: 0.7438,micro: 0.7444\n",
            "ShadowModel Epoch: 069, Approx Train: 0.7127, Train: 0.7143, Test: 0.7476,marco: 0.7466,micro: 0.7476\n",
            "ShadowModel Epoch: 070, Approx Train: 0.7143, Train: 0.7127, Test: 0.7571,marco: 0.7541,micro: 0.7571\n",
            "ShadowModel Epoch: 071, Approx Train: 0.7127, Train: 0.7127, Test: 0.7571,marco: 0.7541,micro: 0.7571\n",
            "ShadowModel Epoch: 072, Approx Train: 0.7127, Train: 0.7127, Test: 0.7587,marco: 0.7547,micro: 0.7587\n",
            "ShadowModel Epoch: 073, Approx Train: 0.7127, Train: 0.7143, Test: 0.7619,marco: 0.7578,micro: 0.7619\n",
            "ShadowModel Epoch: 074, Approx Train: 0.7143, Train: 0.7143, Test: 0.7651,marco: 0.7604,micro: 0.7651\n",
            "ShadowModel Epoch: 075, Approx Train: 0.7143, Train: 0.7175, Test: 0.7667,marco: 0.7630,micro: 0.7667\n",
            "ShadowModel Epoch: 076, Approx Train: 0.7175, Train: 0.7175, Test: 0.7667,marco: 0.7630,micro: 0.7667\n",
            "ShadowModel Epoch: 077, Approx Train: 0.7175, Train: 0.7190, Test: 0.7683,marco: 0.7641,micro: 0.7683\n",
            "ShadowModel Epoch: 078, Approx Train: 0.7190, Train: 0.7190, Test: 0.7698,marco: 0.7668,micro: 0.7698\n",
            "ShadowModel Epoch: 079, Approx Train: 0.7190, Train: 0.7206, Test: 0.7698,marco: 0.7668,micro: 0.7698\n",
            "ShadowModel Epoch: 080, Approx Train: 0.7206, Train: 0.7238, Test: 0.7714,marco: 0.7684,micro: 0.7714\n",
            "ShadowModel Epoch: 081, Approx Train: 0.7238, Train: 0.7270, Test: 0.7746,marco: 0.7723,micro: 0.7746\n",
            "ShadowModel Epoch: 082, Approx Train: 0.7270, Train: 0.7286, Test: 0.7746,marco: 0.7723,micro: 0.7746\n",
            "ShadowModel Epoch: 083, Approx Train: 0.7286, Train: 0.7286, Test: 0.7762,marco: 0.7739,micro: 0.7762\n",
            "ShadowModel Epoch: 084, Approx Train: 0.7286, Train: 0.7302, Test: 0.7794,marco: 0.7770,micro: 0.7794\n",
            "ShadowModel Epoch: 085, Approx Train: 0.7302, Train: 0.7286, Test: 0.7810,marco: 0.7792,micro: 0.7810\n",
            "ShadowModel Epoch: 086, Approx Train: 0.7286, Train: 0.7286, Test: 0.7825,marco: 0.7802,micro: 0.7825\n",
            "ShadowModel Epoch: 087, Approx Train: 0.7286, Train: 0.7302, Test: 0.7825,marco: 0.7807,micro: 0.7825\n",
            "ShadowModel Epoch: 088, Approx Train: 0.7302, Train: 0.7302, Test: 0.7825,marco: 0.7807,micro: 0.7825\n",
            "ShadowModel Epoch: 089, Approx Train: 0.7302, Train: 0.7286, Test: 0.7841,marco: 0.7824,micro: 0.7841\n",
            "ShadowModel Epoch: 090, Approx Train: 0.7286, Train: 0.7302, Test: 0.7857,marco: 0.7839,micro: 0.7857\n",
            "ShadowModel Epoch: 091, Approx Train: 0.7302, Train: 0.7286, Test: 0.7857,marco: 0.7839,micro: 0.7857\n",
            "ShadowModel Epoch: 092, Approx Train: 0.7286, Train: 0.7286, Test: 0.7873,marco: 0.7855,micro: 0.7873\n",
            "ShadowModel Epoch: 093, Approx Train: 0.7286, Train: 0.7286, Test: 0.7873,marco: 0.7845,micro: 0.7873\n",
            "ShadowModel Epoch: 094, Approx Train: 0.7286, Train: 0.7302, Test: 0.7889,marco: 0.7866,micro: 0.7889\n",
            "ShadowModel Epoch: 095, Approx Train: 0.7302, Train: 0.7333, Test: 0.7889,marco: 0.7866,micro: 0.7889\n",
            "ShadowModel Epoch: 096, Approx Train: 0.7333, Train: 0.7333, Test: 0.7921,marco: 0.7900,micro: 0.7921\n",
            "ShadowModel Epoch: 097, Approx Train: 0.7333, Train: 0.7333, Test: 0.7905,marco: 0.7887,micro: 0.7905\n",
            "ShadowModel Epoch: 098, Approx Train: 0.7333, Train: 0.7365, Test: 0.7905,marco: 0.7891,micro: 0.7905\n",
            "ShadowModel Epoch: 099, Approx Train: 0.7365, Train: 0.7365, Test: 0.7937,marco: 0.7925,micro: 0.7937\n",
            "ShadowModel Epoch: 100, Approx Train: 0.7365, Train: 0.7349, Test: 0.7937,marco: 0.7925,micro: 0.7937\n",
            "ShadowModel Epoch: 101, Approx Train: 0.7349, Train: 0.7365, Test: 0.7968,marco: 0.7956,micro: 0.7968\n",
            "ShadowModel Epoch: 102, Approx Train: 0.7365, Train: 0.7365, Test: 0.7968,marco: 0.7956,micro: 0.7968\n",
            "ShadowModel Epoch: 103, Approx Train: 0.7365, Train: 0.7365, Test: 0.7968,marco: 0.7956,micro: 0.7968\n",
            "ShadowModel Epoch: 104, Approx Train: 0.7365, Train: 0.7365, Test: 0.7968,marco: 0.7956,micro: 0.7968\n",
            "ShadowModel Epoch: 105, Approx Train: 0.7365, Train: 0.7365, Test: 0.8000,marco: 0.7989,micro: 0.8000\n",
            "ShadowModel Epoch: 106, Approx Train: 0.7365, Train: 0.7365, Test: 0.8032,marco: 0.8013,micro: 0.8032\n",
            "ShadowModel Epoch: 107, Approx Train: 0.7365, Train: 0.7397, Test: 0.8032,marco: 0.8013,micro: 0.8032\n",
            "ShadowModel Epoch: 108, Approx Train: 0.7397, Train: 0.7397, Test: 0.8063,marco: 0.8048,micro: 0.8063\n",
            "ShadowModel Epoch: 109, Approx Train: 0.7397, Train: 0.7397, Test: 0.8063,marco: 0.8048,micro: 0.8063\n",
            "ShadowModel Epoch: 110, Approx Train: 0.7397, Train: 0.7413, Test: 0.8079,marco: 0.8059,micro: 0.8079\n",
            "ShadowModel Epoch: 111, Approx Train: 0.7413, Train: 0.7429, Test: 0.8079,marco: 0.8059,micro: 0.8079\n",
            "ShadowModel Epoch: 112, Approx Train: 0.7429, Train: 0.7429, Test: 0.8079,marco: 0.8059,micro: 0.8079\n",
            "ShadowModel Epoch: 113, Approx Train: 0.7429, Train: 0.7429, Test: 0.8095,marco: 0.8070,micro: 0.8095\n",
            "ShadowModel Epoch: 114, Approx Train: 0.7429, Train: 0.7429, Test: 0.8095,marco: 0.8070,micro: 0.8095\n",
            "ShadowModel Epoch: 115, Approx Train: 0.7429, Train: 0.7429, Test: 0.8095,marco: 0.8070,micro: 0.8095\n",
            "ShadowModel Epoch: 116, Approx Train: 0.7429, Train: 0.7429, Test: 0.8111,marco: 0.8085,micro: 0.8111\n",
            "ShadowModel Epoch: 117, Approx Train: 0.7429, Train: 0.7429, Test: 0.8111,marco: 0.8085,micro: 0.8111\n",
            "ShadowModel Epoch: 118, Approx Train: 0.7429, Train: 0.7429, Test: 0.8111,marco: 0.8085,micro: 0.8111\n",
            "ShadowModel Epoch: 119, Approx Train: 0.7429, Train: 0.7429, Test: 0.8159,marco: 0.8123,micro: 0.8159\n",
            "ShadowModel Epoch: 120, Approx Train: 0.7429, Train: 0.7444, Test: 0.8159,marco: 0.8123,micro: 0.8159\n",
            "ShadowModel Epoch: 121, Approx Train: 0.7444, Train: 0.7444, Test: 0.8206,marco: 0.8161,micro: 0.8206\n",
            "ShadowModel Epoch: 122, Approx Train: 0.7444, Train: 0.7429, Test: 0.8206,marco: 0.8161,micro: 0.8206\n",
            "ShadowModel Epoch: 123, Approx Train: 0.7429, Train: 0.7444, Test: 0.8206,marco: 0.8161,micro: 0.8206\n",
            "ShadowModel Epoch: 124, Approx Train: 0.7444, Train: 0.7460, Test: 0.8206,marco: 0.8156,micro: 0.8206\n",
            "ShadowModel Epoch: 125, Approx Train: 0.7460, Train: 0.7460, Test: 0.8190,marco: 0.8130,micro: 0.8190\n",
            "ShadowModel Epoch: 126, Approx Train: 0.7460, Train: 0.7476, Test: 0.8190,marco: 0.8130,micro: 0.8190\n",
            "ShadowModel Epoch: 127, Approx Train: 0.7476, Train: 0.7476, Test: 0.8190,marco: 0.8130,micro: 0.8190\n",
            "ShadowModel Epoch: 128, Approx Train: 0.7476, Train: 0.7476, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 129, Approx Train: 0.7476, Train: 0.7476, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 130, Approx Train: 0.7476, Train: 0.7460, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 131, Approx Train: 0.7460, Train: 0.7460, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 132, Approx Train: 0.7460, Train: 0.7476, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 133, Approx Train: 0.7476, Train: 0.7476, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 134, Approx Train: 0.7476, Train: 0.7476, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 135, Approx Train: 0.7476, Train: 0.7476, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 136, Approx Train: 0.7476, Train: 0.7476, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 137, Approx Train: 0.7476, Train: 0.7492, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 138, Approx Train: 0.7492, Train: 0.7492, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 139, Approx Train: 0.7492, Train: 0.7492, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 140, Approx Train: 0.7492, Train: 0.7492, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 141, Approx Train: 0.7492, Train: 0.7492, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 142, Approx Train: 0.7492, Train: 0.7492, Test: 0.8222,marco: 0.8153,micro: 0.8222\n",
            "ShadowModel Epoch: 143, Approx Train: 0.7492, Train: 0.7492, Test: 0.8238,marco: 0.8169,micro: 0.8238\n",
            "ShadowModel Epoch: 144, Approx Train: 0.7492, Train: 0.7492, Test: 0.8238,marco: 0.8169,micro: 0.8238\n",
            "ShadowModel Epoch: 145, Approx Train: 0.7492, Train: 0.7476, Test: 0.8238,marco: 0.8169,micro: 0.8238\n",
            "ShadowModel Epoch: 146, Approx Train: 0.7476, Train: 0.7460, Test: 0.8238,marco: 0.8169,micro: 0.8238\n",
            "ShadowModel Epoch: 147, Approx Train: 0.7460, Train: 0.7460, Test: 0.8254,marco: 0.8185,micro: 0.8254\n",
            "ShadowModel Epoch: 148, Approx Train: 0.7460, Train: 0.7476, Test: 0.8270,marco: 0.8196,micro: 0.8270\n",
            "ShadowModel Epoch: 149, Approx Train: 0.7476, Train: 0.7476, Test: 0.8270,marco: 0.8196,micro: 0.8270\n",
            "ShadowModel Epoch: 150, Approx Train: 0.7476, Train: 0.7492, Test: 0.8302,marco: 0.8219,micro: 0.8302\n",
            "ShadowModel Epoch: 151, Approx Train: 0.7492, Train: 0.7492, Test: 0.8302,marco: 0.8219,micro: 0.8302\n",
            "ShadowModel Epoch: 152, Approx Train: 0.7492, Train: 0.7492, Test: 0.8302,marco: 0.8219,micro: 0.8302\n",
            "ShadowModel Epoch: 153, Approx Train: 0.7492, Train: 0.7508, Test: 0.8302,marco: 0.8219,micro: 0.8302\n",
            "ShadowModel Epoch: 154, Approx Train: 0.7508, Train: 0.7508, Test: 0.8317,marco: 0.8231,micro: 0.8317\n",
            "ShadowModel Epoch: 155, Approx Train: 0.7508, Train: 0.7508, Test: 0.8317,marco: 0.8231,micro: 0.8317\n",
            "ShadowModel Epoch: 156, Approx Train: 0.7508, Train: 0.7508, Test: 0.8317,marco: 0.8231,micro: 0.8317\n",
            "ShadowModel Epoch: 157, Approx Train: 0.7508, Train: 0.7508, Test: 0.8317,marco: 0.8231,micro: 0.8317\n",
            "ShadowModel Epoch: 158, Approx Train: 0.7508, Train: 0.7508, Test: 0.8333,marco: 0.8242,micro: 0.8333\n",
            "ShadowModel Epoch: 159, Approx Train: 0.7508, Train: 0.7508, Test: 0.8333,marco: 0.8242,micro: 0.8333\n",
            "ShadowModel Epoch: 160, Approx Train: 0.7508, Train: 0.7508, Test: 0.8333,marco: 0.8242,micro: 0.8333\n",
            "ShadowModel Epoch: 161, Approx Train: 0.7508, Train: 0.7524, Test: 0.8333,marco: 0.8242,micro: 0.8333\n",
            "ShadowModel Epoch: 162, Approx Train: 0.7524, Train: 0.7524, Test: 0.8365,marco: 0.8270,micro: 0.8365\n",
            "ShadowModel Epoch: 163, Approx Train: 0.7524, Train: 0.7524, Test: 0.8365,marco: 0.8270,micro: 0.8365\n",
            "ShadowModel Epoch: 164, Approx Train: 0.7524, Train: 0.7540, Test: 0.8381,marco: 0.8281,micro: 0.8381\n",
            "ShadowModel Epoch: 165, Approx Train: 0.7540, Train: 0.7556, Test: 0.8381,marco: 0.8281,micro: 0.8381\n",
            "ShadowModel Epoch: 166, Approx Train: 0.7556, Train: 0.7556, Test: 0.8381,marco: 0.8281,micro: 0.8381\n",
            "ShadowModel Epoch: 167, Approx Train: 0.7556, Train: 0.7587, Test: 0.8381,marco: 0.8281,micro: 0.8381\n",
            "ShadowModel Epoch: 168, Approx Train: 0.7587, Train: 0.7587, Test: 0.8397,marco: 0.8292,micro: 0.8397\n",
            "ShadowModel Epoch: 169, Approx Train: 0.7587, Train: 0.7587, Test: 0.8397,marco: 0.8292,micro: 0.8397\n",
            "ShadowModel Epoch: 170, Approx Train: 0.7587, Train: 0.7603, Test: 0.8397,marco: 0.8292,micro: 0.8397\n",
            "ShadowModel Epoch: 171, Approx Train: 0.7603, Train: 0.7603, Test: 0.8413,marco: 0.8304,micro: 0.8413\n",
            "ShadowModel Epoch: 172, Approx Train: 0.7603, Train: 0.7603, Test: 0.8413,marco: 0.8300,micro: 0.8413\n",
            "ShadowModel Epoch: 173, Approx Train: 0.7603, Train: 0.7619, Test: 0.8413,marco: 0.8300,micro: 0.8413\n",
            "ShadowModel Epoch: 174, Approx Train: 0.7619, Train: 0.7619, Test: 0.8413,marco: 0.8300,micro: 0.8413\n",
            "ShadowModel Epoch: 175, Approx Train: 0.7619, Train: 0.7619, Test: 0.8413,marco: 0.8300,micro: 0.8413\n",
            "ShadowModel Epoch: 176, Approx Train: 0.7619, Train: 0.7635, Test: 0.8429,marco: 0.8315,micro: 0.8429\n",
            "ShadowModel Epoch: 177, Approx Train: 0.7635, Train: 0.7651, Test: 0.8429,marco: 0.8315,micro: 0.8429\n",
            "ShadowModel Epoch: 178, Approx Train: 0.7651, Train: 0.7683, Test: 0.8429,marco: 0.8315,micro: 0.8429\n",
            "ShadowModel Epoch: 179, Approx Train: 0.7683, Train: 0.7683, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 180, Approx Train: 0.7683, Train: 0.7683, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 181, Approx Train: 0.7683, Train: 0.7698, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 182, Approx Train: 0.7698, Train: 0.7698, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 183, Approx Train: 0.7698, Train: 0.7714, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 184, Approx Train: 0.7714, Train: 0.7714, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 185, Approx Train: 0.7714, Train: 0.7698, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 186, Approx Train: 0.7698, Train: 0.7698, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 187, Approx Train: 0.7698, Train: 0.7698, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 188, Approx Train: 0.7698, Train: 0.7698, Test: 0.8429,marco: 0.8314,micro: 0.8429\n",
            "ShadowModel Epoch: 189, Approx Train: 0.7698, Train: 0.7698, Test: 0.8429,marco: 0.8314,micro: 0.8429\n",
            "ShadowModel Epoch: 190, Approx Train: 0.7698, Train: 0.7698, Test: 0.8429,marco: 0.8314,micro: 0.8429\n",
            "ShadowModel Epoch: 191, Approx Train: 0.7698, Train: 0.7698, Test: 0.8429,marco: 0.8314,micro: 0.8429\n",
            "ShadowModel Epoch: 192, Approx Train: 0.7698, Train: 0.7698, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 193, Approx Train: 0.7698, Train: 0.7714, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 194, Approx Train: 0.7714, Train: 0.7714, Test: 0.8460,marco: 0.8349,micro: 0.8460\n",
            "ShadowModel Epoch: 195, Approx Train: 0.7714, Train: 0.7714, Test: 0.8460,marco: 0.8349,micro: 0.8460\n",
            "ShadowModel Epoch: 196, Approx Train: 0.7714, Train: 0.7698, Test: 0.8460,marco: 0.8349,micro: 0.8460\n",
            "ShadowModel Epoch: 197, Approx Train: 0.7698, Train: 0.7698, Test: 0.8460,marco: 0.8349,micro: 0.8460\n",
            "ShadowModel Epoch: 198, Approx Train: 0.7698, Train: 0.7698, Test: 0.8460,marco: 0.8349,micro: 0.8460\n",
            "ShadowModel Epoch: 199, Approx Train: 0.7698, Train: 0.7714, Test: 0.8460,marco: 0.8349,micro: 0.8460\n",
            "ShadowModel Epoch: 200, Approx Train: 0.7714, Train: 0.7714, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 201, Approx Train: 0.7714, Train: 0.7730, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 202, Approx Train: 0.7730, Train: 0.7730, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 203, Approx Train: 0.7730, Train: 0.7730, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 204, Approx Train: 0.7730, Train: 0.7746, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 205, Approx Train: 0.7746, Train: 0.7746, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 206, Approx Train: 0.7746, Train: 0.7746, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 207, Approx Train: 0.7746, Train: 0.7746, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 208, Approx Train: 0.7746, Train: 0.7746, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 209, Approx Train: 0.7746, Train: 0.7746, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 210, Approx Train: 0.7746, Train: 0.7778, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 211, Approx Train: 0.7778, Train: 0.7778, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 212, Approx Train: 0.7778, Train: 0.7778, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 213, Approx Train: 0.7778, Train: 0.7778, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 214, Approx Train: 0.7778, Train: 0.7810, Test: 0.8444,marco: 0.8335,micro: 0.8444\n",
            "ShadowModel Epoch: 215, Approx Train: 0.7810, Train: 0.7810, Test: 0.8460,marco: 0.8347,micro: 0.8460\n",
            "ShadowModel Epoch: 216, Approx Train: 0.7810, Train: 0.7810, Test: 0.8460,marco: 0.8347,micro: 0.8460\n",
            "ShadowModel Epoch: 217, Approx Train: 0.7810, Train: 0.7825, Test: 0.8460,marco: 0.8347,micro: 0.8460\n",
            "ShadowModel Epoch: 218, Approx Train: 0.7825, Train: 0.7841, Test: 0.8460,marco: 0.8347,micro: 0.8460\n",
            "ShadowModel Epoch: 219, Approx Train: 0.7841, Train: 0.7841, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 220, Approx Train: 0.7841, Train: 0.7841, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 221, Approx Train: 0.7841, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 222, Approx Train: 0.7889, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 223, Approx Train: 0.7889, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 224, Approx Train: 0.7889, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 225, Approx Train: 0.7889, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 226, Approx Train: 0.7889, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 227, Approx Train: 0.7889, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 228, Approx Train: 0.7889, Train: 0.7889, Test: 0.8460,marco: 0.8344,micro: 0.8460\n",
            "ShadowModel Epoch: 229, Approx Train: 0.7889, Train: 0.7889, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 230, Approx Train: 0.7889, Train: 0.7857, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 231, Approx Train: 0.7857, Train: 0.7873, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 232, Approx Train: 0.7873, Train: 0.7889, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 233, Approx Train: 0.7889, Train: 0.7905, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 234, Approx Train: 0.7905, Train: 0.7905, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 235, Approx Train: 0.7905, Train: 0.7921, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 236, Approx Train: 0.7921, Train: 0.7921, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 237, Approx Train: 0.7921, Train: 0.7921, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 238, Approx Train: 0.7921, Train: 0.7937, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 239, Approx Train: 0.7937, Train: 0.7968, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 240, Approx Train: 0.7968, Train: 0.7968, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 241, Approx Train: 0.7968, Train: 0.7968, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 242, Approx Train: 0.7968, Train: 0.7968, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 243, Approx Train: 0.7968, Train: 0.7968, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 244, Approx Train: 0.7968, Train: 0.7968, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 245, Approx Train: 0.7968, Train: 0.7984, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 246, Approx Train: 0.7984, Train: 0.7984, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 247, Approx Train: 0.7984, Train: 0.7984, Test: 0.8444,marco: 0.8329,micro: 0.8444\n",
            "ShadowModel Epoch: 248, Approx Train: 0.7984, Train: 0.7984, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 249, Approx Train: 0.7984, Train: 0.7984, Test: 0.8444,marco: 0.8325,micro: 0.8444\n",
            "ShadowModel Epoch: 250, Approx Train: 0.7984, Train: 0.7984, Test: 0.8444,marco: 0.8325,micro: 0.8444\n",
            "ShadowModel Epoch: 251, Approx Train: 0.7984, Train: 0.7984, Test: 0.8444,marco: 0.8325,micro: 0.8444\n",
            "ShadowModel Epoch: 252, Approx Train: 0.7984, Train: 0.7984, Test: 0.8444,marco: 0.8325,micro: 0.8444\n",
            "ShadowModel Epoch: 253, Approx Train: 0.7984, Train: 0.8000, Test: 0.8460,marco: 0.8339,micro: 0.8460\n",
            "ShadowModel Epoch: 254, Approx Train: 0.8000, Train: 0.8000, Test: 0.8460,marco: 0.8339,micro: 0.8460\n",
            "ShadowModel Epoch: 255, Approx Train: 0.8000, Train: 0.8000, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 256, Approx Train: 0.8000, Train: 0.8000, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 257, Approx Train: 0.8000, Train: 0.8000, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 258, Approx Train: 0.8000, Train: 0.8000, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 259, Approx Train: 0.8000, Train: 0.8000, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 260, Approx Train: 0.8000, Train: 0.8000, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 261, Approx Train: 0.8000, Train: 0.8032, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 262, Approx Train: 0.8032, Train: 0.8032, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 263, Approx Train: 0.8032, Train: 0.8032, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 264, Approx Train: 0.8032, Train: 0.8032, Test: 0.8444,marco: 0.8327,micro: 0.8444\n",
            "ShadowModel Epoch: 265, Approx Train: 0.8032, Train: 0.8032, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 266, Approx Train: 0.8032, Train: 0.8032, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 267, Approx Train: 0.8032, Train: 0.8048, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 268, Approx Train: 0.8048, Train: 0.8048, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 269, Approx Train: 0.8048, Train: 0.8063, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 270, Approx Train: 0.8063, Train: 0.8063, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 271, Approx Train: 0.8063, Train: 0.8063, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 272, Approx Train: 0.8063, Train: 0.8063, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 273, Approx Train: 0.8063, Train: 0.8079, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 274, Approx Train: 0.8079, Train: 0.8079, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 275, Approx Train: 0.8079, Train: 0.8079, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 276, Approx Train: 0.8079, Train: 0.8079, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 277, Approx Train: 0.8079, Train: 0.8079, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 278, Approx Train: 0.8079, Train: 0.8079, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 279, Approx Train: 0.8079, Train: 0.8079, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 280, Approx Train: 0.8079, Train: 0.8095, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 281, Approx Train: 0.8095, Train: 0.8095, Test: 0.8460,marco: 0.8340,micro: 0.8460\n",
            "ShadowModel Epoch: 282, Approx Train: 0.8095, Train: 0.8095, Test: 0.8476,marco: 0.8352,micro: 0.8476\n",
            "ShadowModel Epoch: 283, Approx Train: 0.8095, Train: 0.8095, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 284, Approx Train: 0.8095, Train: 0.8095, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 285, Approx Train: 0.8095, Train: 0.8095, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 286, Approx Train: 0.8095, Train: 0.8111, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 287, Approx Train: 0.8111, Train: 0.8111, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 288, Approx Train: 0.8111, Train: 0.8111, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 289, Approx Train: 0.8111, Train: 0.8111, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 290, Approx Train: 0.8111, Train: 0.8111, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 291, Approx Train: 0.8111, Train: 0.8111, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 292, Approx Train: 0.8111, Train: 0.8127, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 293, Approx Train: 0.8127, Train: 0.8127, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 294, Approx Train: 0.8127, Train: 0.8127, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 295, Approx Train: 0.8127, Train: 0.8127, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 296, Approx Train: 0.8127, Train: 0.8127, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 297, Approx Train: 0.8127, Train: 0.8143, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 298, Approx Train: 0.8143, Train: 0.8159, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 299, Approx Train: 0.8159, Train: 0.8190, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "ShadowModel Epoch: 300, Approx Train: 0.8190, Train: 0.8190, Test: 0.8492,marco: 0.8372,micro: 0.8492\n",
            "positive_attack_data.shape (630, 8)\n",
            "negative_attack_data.shape (630, 8)\n",
            "target_data_for_testing_InAndOutTrain (1260, 9)\n",
            "attack_data.shape (1260, 8)\n",
            "X_attack.shape (1260, 7)\n",
            "X_attack_InTrain (630, 7)\n",
            "X_attack_OutTrain (630, 7)\n",
            "X_InTrain.shape (1260, 7)\n",
            "baba\n",
            "Net(\n",
            "  (fc1): Linear(in_features=7, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "Epoch: 1/100.. Training loss: 0.67614.. Test Loss: 0.66074.. Train Accuracy: 0.588 Test Accuracy: 0.615\n",
            "Epoch: 2/100.. Training loss: 0.63556.. Test Loss: 0.63283.. Train Accuracy: 0.658 Test Accuracy: 0.655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1599: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3/100.. Training loss: 0.61287.. Test Loss: 0.66889.. Train Accuracy: 0.670 Test Accuracy: 0.568\n",
            "Epoch: 4/100.. Training loss: 0.60405.. Test Loss: 0.65711.. Train Accuracy: 0.684 Test Accuracy: 0.578\n",
            "Epoch: 5/100.. Training loss: 0.58737.. Test Loss: 0.60784.. Train Accuracy: 0.707 Test Accuracy: 0.689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6/100.. Training loss: 0.57739.. Test Loss: 0.66664.. Train Accuracy: 0.723 Test Accuracy: 0.538\n",
            "Epoch: 7/100.. Training loss: 0.56554.. Test Loss: 0.65909.. Train Accuracy: 0.736 Test Accuracy: 0.587\n",
            "Epoch: 8/100.. Training loss: 0.57162.. Test Loss: 0.63197.. Train Accuracy: 0.724 Test Accuracy: 0.634\n",
            "Epoch: 9/100.. Training loss: 0.54663.. Test Loss: 0.55318.. Train Accuracy: 0.765 Test Accuracy: 0.780\n",
            "Epoch: 10/100.. Training loss: 0.53225.. Test Loss: 0.52797.. Train Accuracy: 0.781 Test Accuracy: 0.816\n",
            "Epoch: 11/100.. Training loss: 0.53559.. Test Loss: 0.56320.. Train Accuracy: 0.773 Test Accuracy: 0.708\n",
            "Epoch: 12/100.. Training loss: 0.52258.. Test Loss: 0.53499.. Train Accuracy: 0.791 Test Accuracy: 0.780\n",
            "Epoch: 13/100.. Training loss: 0.52327.. Test Loss: 0.56417.. Train Accuracy: 0.783 Test Accuracy: 0.792\n",
            "Epoch: 14/100.. Training loss: 0.52110.. Test Loss: 0.53841.. Train Accuracy: 0.784 Test Accuracy: 0.804\n",
            "Epoch: 15/100.. Training loss: 0.51756.. Test Loss: 0.53748.. Train Accuracy: 0.792 Test Accuracy: 0.792\n",
            "Epoch: 16/100.. Training loss: 0.53298.. Test Loss: 0.57616.. Train Accuracy: 0.769 Test Accuracy: 0.743\n",
            "Epoch: 17/100.. Training loss: 0.51307.. Test Loss: 0.50722.. Train Accuracy: 0.794 Test Accuracy: 0.785\n",
            "Epoch: 18/100.. Training loss: 0.52236.. Test Loss: 0.53267.. Train Accuracy: 0.787 Test Accuracy: 0.771\n",
            "Epoch: 19/100.. Training loss: 0.51079.. Test Loss: 0.56171.. Train Accuracy: 0.798 Test Accuracy: 0.752\n",
            "Epoch: 20/100.. Training loss: 0.51423.. Test Loss: 0.60666.. Train Accuracy: 0.790 Test Accuracy: 0.642\n",
            "Epoch: 21/100.. Training loss: 0.51539.. Test Loss: 0.52569.. Train Accuracy: 0.793 Test Accuracy: 0.776\n",
            "Epoch: 22/100.. Training loss: 0.52325.. Test Loss: 0.49304.. Train Accuracy: 0.787 Test Accuracy: 0.823\n",
            "Epoch: 23/100.. Training loss: 0.51260.. Test Loss: 0.51537.. Train Accuracy: 0.798 Test Accuracy: 0.795\n",
            "Epoch: 24/100.. Training loss: 0.49952.. Test Loss: 0.46261.. Train Accuracy: 0.814 Test Accuracy: 0.851\n",
            "Epoch: 25/100.. Training loss: 0.49557.. Test Loss: 0.52715.. Train Accuracy: 0.815 Test Accuracy: 0.799\n",
            "Epoch: 26/100.. Training loss: 0.50894.. Test Loss: 0.43408.. Train Accuracy: 0.803 Test Accuracy: 0.863\n",
            "Epoch: 27/100.. Training loss: 0.49192.. Test Loss: 0.47805.. Train Accuracy: 0.818 Test Accuracy: 0.839\n",
            "Epoch: 28/100.. Training loss: 0.49668.. Test Loss: 0.51302.. Train Accuracy: 0.819 Test Accuracy: 0.811\n",
            "Epoch: 29/100.. Training loss: 0.49208.. Test Loss: 0.46516.. Train Accuracy: 0.817 Test Accuracy: 0.870\n",
            "Epoch: 30/100.. Training loss: 0.49554.. Test Loss: 0.46598.. Train Accuracy: 0.816 Test Accuracy: 0.847\n",
            "Epoch: 31/100.. Training loss: 0.48867.. Test Loss: 0.43755.. Train Accuracy: 0.822 Test Accuracy: 0.894\n",
            "Epoch: 32/100.. Training loss: 0.49456.. Test Loss: 0.44434.. Train Accuracy: 0.814 Test Accuracy: 0.866\n",
            "Epoch: 33/100.. Training loss: 0.48388.. Test Loss: 0.49640.. Train Accuracy: 0.830 Test Accuracy: 0.835\n",
            "Epoch: 34/100.. Training loss: 0.48420.. Test Loss: 0.45006.. Train Accuracy: 0.823 Test Accuracy: 0.882\n",
            "Epoch: 35/100.. Training loss: 0.49922.. Test Loss: 0.41871.. Train Accuracy: 0.809 Test Accuracy: 0.894\n",
            "Epoch: 36/100.. Training loss: 0.48395.. Test Loss: 0.45135.. Train Accuracy: 0.825 Test Accuracy: 0.885\n",
            "Epoch: 37/100.. Training loss: 0.48480.. Test Loss: 0.46485.. Train Accuracy: 0.825 Test Accuracy: 0.842\n",
            "Epoch: 38/100.. Training loss: 0.49101.. Test Loss: 0.54325.. Train Accuracy: 0.821 Test Accuracy: 0.764\n",
            "Epoch: 39/100.. Training loss: 0.50423.. Test Loss: 0.47977.. Train Accuracy: 0.801 Test Accuracy: 0.839\n",
            "Epoch: 40/100.. Training loss: 0.47825.. Test Loss: 0.42511.. Train Accuracy: 0.834 Test Accuracy: 0.906\n",
            "Epoch: 41/100.. Training loss: 0.47874.. Test Loss: 0.43963.. Train Accuracy: 0.827 Test Accuracy: 0.866\n",
            "Epoch: 42/100.. Training loss: 0.47660.. Test Loss: 0.44884.. Train Accuracy: 0.834 Test Accuracy: 0.854\n",
            "Epoch: 43/100.. Training loss: 0.47588.. Test Loss: 0.43250.. Train Accuracy: 0.835 Test Accuracy: 0.878\n",
            "Epoch: 44/100.. Training loss: 0.46957.. Test Loss: 0.43278.. Train Accuracy: 0.848 Test Accuracy: 0.878\n",
            "Epoch: 45/100.. Training loss: 0.47015.. Test Loss: 0.41606.. Train Accuracy: 0.845 Test Accuracy: 0.910\n",
            "Epoch: 46/100.. Training loss: 0.47549.. Test Loss: 0.50170.. Train Accuracy: 0.840 Test Accuracy: 0.783\n",
            "Epoch: 47/100.. Training loss: 0.48124.. Test Loss: 0.46726.. Train Accuracy: 0.822 Test Accuracy: 0.839\n",
            "Epoch: 48/100.. Training loss: 0.47626.. Test Loss: 0.45082.. Train Accuracy: 0.832 Test Accuracy: 0.866\n",
            "Epoch: 49/100.. Training loss: 0.46936.. Test Loss: 0.46488.. Train Accuracy: 0.843 Test Accuracy: 0.823\n",
            "Epoch: 50/100.. Training loss: 0.46624.. Test Loss: 0.46286.. Train Accuracy: 0.845 Test Accuracy: 0.823\n",
            "Epoch: 51/100.. Training loss: 0.47830.. Test Loss: 0.46934.. Train Accuracy: 0.833 Test Accuracy: 0.847\n",
            "Epoch: 52/100.. Training loss: 0.46819.. Test Loss: 0.50279.. Train Accuracy: 0.848 Test Accuracy: 0.814\n",
            "Epoch: 53/100.. Training loss: 0.47418.. Test Loss: 0.42122.. Train Accuracy: 0.832 Test Accuracy: 0.891\n",
            "Epoch: 54/100.. Training loss: 0.46702.. Test Loss: 0.43578.. Train Accuracy: 0.843 Test Accuracy: 0.875\n",
            "Epoch: 55/100.. Training loss: 0.47255.. Test Loss: 0.47070.. Train Accuracy: 0.846 Test Accuracy: 0.842\n",
            "Epoch: 56/100.. Training loss: 0.47439.. Test Loss: 0.47780.. Train Accuracy: 0.840 Test Accuracy: 0.826\n",
            "Epoch: 57/100.. Training loss: 0.47310.. Test Loss: 0.48652.. Train Accuracy: 0.837 Test Accuracy: 0.842\n",
            "Epoch: 58/100.. Training loss: 0.46502.. Test Loss: 0.48085.. Train Accuracy: 0.850 Test Accuracy: 0.823\n",
            "Epoch: 59/100.. Training loss: 0.47912.. Test Loss: 0.47841.. Train Accuracy: 0.829 Test Accuracy: 0.826\n",
            "Epoch: 60/100.. Training loss: 0.47672.. Test Loss: 0.48909.. Train Accuracy: 0.829 Test Accuracy: 0.851\n",
            "Epoch: 61/100.. Training loss: 0.46577.. Test Loss: 0.45473.. Train Accuracy: 0.852 Test Accuracy: 0.875\n",
            "Epoch: 62/100.. Training loss: 0.46465.. Test Loss: 0.50940.. Train Accuracy: 0.849 Test Accuracy: 0.823\n",
            "Epoch: 63/100.. Training loss: 0.46474.. Test Loss: 0.46463.. Train Accuracy: 0.849 Test Accuracy: 0.835\n",
            "Epoch: 64/100.. Training loss: 0.46565.. Test Loss: 0.50994.. Train Accuracy: 0.849 Test Accuracy: 0.783\n",
            "Epoch: 65/100.. Training loss: 0.46071.. Test Loss: 0.48048.. Train Accuracy: 0.853 Test Accuracy: 0.826\n",
            "Epoch: 66/100.. Training loss: 0.47589.. Test Loss: 0.49052.. Train Accuracy: 0.837 Test Accuracy: 0.783\n",
            "Epoch: 67/100.. Training loss: 0.46736.. Test Loss: 0.46262.. Train Accuracy: 0.847 Test Accuracy: 0.839\n",
            "Epoch: 68/100.. Training loss: 0.46574.. Test Loss: 0.45335.. Train Accuracy: 0.848 Test Accuracy: 0.863\n",
            "Epoch: 69/100.. Training loss: 0.45915.. Test Loss: 0.52097.. Train Accuracy: 0.855 Test Accuracy: 0.776\n",
            "Epoch: 70/100.. Training loss: 0.45430.. Test Loss: 0.49528.. Train Accuracy: 0.856 Test Accuracy: 0.807\n",
            "Epoch: 71/100.. Training loss: 0.47287.. Test Loss: 0.42096.. Train Accuracy: 0.841 Test Accuracy: 0.906\n",
            "Epoch: 72/100.. Training loss: 0.46430.. Test Loss: 0.47558.. Train Accuracy: 0.850 Test Accuracy: 0.839\n",
            "Epoch: 73/100.. Training loss: 0.46197.. Test Loss: 0.51840.. Train Accuracy: 0.852 Test Accuracy: 0.780\n",
            "Epoch: 74/100.. Training loss: 0.47510.. Test Loss: 0.43769.. Train Accuracy: 0.836 Test Accuracy: 0.854\n",
            "Epoch: 75/100.. Training loss: 0.46456.. Test Loss: 0.48191.. Train Accuracy: 0.844 Test Accuracy: 0.863\n",
            "Epoch: 76/100.. Training loss: 0.45551.. Test Loss: 0.52681.. Train Accuracy: 0.860 Test Accuracy: 0.774\n",
            "Epoch: 77/100.. Training loss: 0.45674.. Test Loss: 0.45364.. Train Accuracy: 0.856 Test Accuracy: 0.863\n",
            "Epoch: 78/100.. Training loss: 0.45150.. Test Loss: 0.49432.. Train Accuracy: 0.864 Test Accuracy: 0.783\n",
            "Epoch: 79/100.. Training loss: 0.45875.. Test Loss: 0.43817.. Train Accuracy: 0.850 Test Accuracy: 0.866\n",
            "Epoch: 80/100.. Training loss: 0.45751.. Test Loss: 0.43267.. Train Accuracy: 0.856 Test Accuracy: 0.866\n",
            "Epoch: 81/100.. Training loss: 0.45452.. Test Loss: 0.49452.. Train Accuracy: 0.859 Test Accuracy: 0.795\n",
            "Epoch: 82/100.. Training loss: 0.45575.. Test Loss: 0.47088.. Train Accuracy: 0.857 Test Accuracy: 0.823\n",
            "Epoch: 83/100.. Training loss: 0.46151.. Test Loss: 0.45581.. Train Accuracy: 0.847 Test Accuracy: 0.866\n",
            "Epoch: 84/100.. Training loss: 0.46004.. Test Loss: 0.47795.. Train Accuracy: 0.850 Test Accuracy: 0.811\n",
            "Epoch: 85/100.. Training loss: 0.45711.. Test Loss: 0.47174.. Train Accuracy: 0.859 Test Accuracy: 0.839\n",
            "Epoch: 86/100.. Training loss: 0.46716.. Test Loss: 0.46716.. Train Accuracy: 0.847 Test Accuracy: 0.835\n",
            "Epoch: 87/100.. Training loss: 0.46062.. Test Loss: 0.47536.. Train Accuracy: 0.851 Test Accuracy: 0.832\n",
            "Epoch: 88/100.. Training loss: 0.46339.. Test Loss: 0.45082.. Train Accuracy: 0.846 Test Accuracy: 0.866\n",
            "Epoch: 89/100.. Training loss: 0.45700.. Test Loss: 0.46091.. Train Accuracy: 0.856 Test Accuracy: 0.835\n",
            "Epoch: 90/100.. Training loss: 0.45812.. Test Loss: 0.47982.. Train Accuracy: 0.853 Test Accuracy: 0.819\n",
            "Epoch: 91/100.. Training loss: 0.46331.. Test Loss: 0.48985.. Train Accuracy: 0.850 Test Accuracy: 0.839\n",
            "Epoch: 92/100.. Training loss: 0.45939.. Test Loss: 0.45402.. Train Accuracy: 0.853 Test Accuracy: 0.851\n",
            "Epoch: 93/100.. Training loss: 0.45714.. Test Loss: 0.48605.. Train Accuracy: 0.852 Test Accuracy: 0.851\n",
            "Epoch: 94/100.. Training loss: 0.45938.. Test Loss: 0.51307.. Train Accuracy: 0.849 Test Accuracy: 0.807\n",
            "Epoch: 95/100.. Training loss: 0.45301.. Test Loss: 0.47231.. Train Accuracy: 0.861 Test Accuracy: 0.839\n",
            "Epoch: 96/100.. Training loss: 0.46508.. Test Loss: 0.50021.. Train Accuracy: 0.845 Test Accuracy: 0.783\n",
            "Epoch: 97/100.. Training loss: 0.47379.. Test Loss: 0.44796.. Train Accuracy: 0.836 Test Accuracy: 0.854\n",
            "Epoch: 98/100.. Training loss: 0.48017.. Test Loss: 0.51308.. Train Accuracy: 0.831 Test Accuracy: 0.804\n",
            "Epoch: 99/100.. Training loss: 0.46206.. Test Loss: 0.51267.. Train Accuracy: 0.852 Test Accuracy: 0.799\n",
            "Epoch: 100/100.. Training loss: 0.45547.. Test Loss: 0.47825.. Train Accuracy: 0.860 Test Accuracy: 0.839\n",
            "To confirm using attack_test_data_loader (50 test samples): 0.863 AUROC: 0.860 precision: 0.885 recall 0.863\n",
            "Test accuracy with Target Train InOut: 0.760 AUROC: 0.758 precision: 0.779 recall 0.760 F1 score 0.755 ===> Attack Performance!\n",
            "data_type Cora\n",
            "model_type GCN\n",
            "WhichRun 1  Total time 153.365\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}